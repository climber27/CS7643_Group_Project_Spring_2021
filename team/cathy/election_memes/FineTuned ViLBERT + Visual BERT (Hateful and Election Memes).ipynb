{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20746,
     "status": "ok",
     "timestamp": 1620084329034,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "jSPOfKQom8AF",
    "outputId": "66786b2d-1992-4d50-f5fd-a19f66f37a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5023,
     "status": "ok",
     "timestamp": 1620084329035,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "SlKoJSAknjuq",
    "outputId": "0a0c4807-2cef-4b3b-cf23-f03d6ba902c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/colab\n"
     ]
    }
   ],
   "source": [
    "%cd gdrive/MyDrive/colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5046,
     "status": "ok",
     "timestamp": 1620084329351,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "o-zreWCXnpGw",
    "outputId": "cd6a1a6d-8ad4-490d-92bb-4c3145acd1b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf\n"
     ]
    }
   ],
   "source": [
    "%cd mmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 238293,
     "status": "ok",
     "timestamp": 1620084562853,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "6hM3BET1nrBO",
    "outputId": "6dcf22b7-9282-4b44-e06b-cc317108c7de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting demjson==2.2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/67/6db789e2533158963d4af689f961b644ddd9200615b8ce92d6cad695c65a/demjson-2.2.4.tar.gz (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 13.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n",
      "Requirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.0.2)\n",
      "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n",
      "Collecting torchtext==0.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 6.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.0)\n",
      "Collecting torchvision<=0.9.1,>=0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/8a/82062a33b5eb7f696bf23f8ccf04bf6fc81d1a4972740fb21c2569ada0a6/torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4MB)\n",
      "\u001b[K     |████████████████████████████████| 17.4MB 355kB/s \n",
      "\u001b[?25hCollecting ftfy==5.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 11.4MB/s \n",
      "\u001b[?25hCollecting torch<=1.8.1,>=1.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n",
      "\u001b[K     |████████████████████████████████| 804.1MB 23kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.19.5)\n",
      "Collecting iopath==0.1.7\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/d5/1c70fea7632640e8a9fb5a176676e555238119b3e7ee8b6dc49980ec5769/iopath-0.1.7-py3-none-any.whl\n",
      "Collecting fasttext==0.9.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
      "\u001b[?25hCollecting omegaconf==2.0.6\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
      "Collecting transformers==3.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 47.6MB/s \n",
      "\u001b[?25hCollecting pytorch-lightning==1.2.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/13/fb401b8f9d9c5e2aa08769d230bb401bf11dee0bc93e069d7337a4201ec8/pytorch_lightning-1.2.7-py3-none-any.whl (830kB)\n",
      "\u001b[K     |████████████████████████████████| 839kB 49.4MB/s \n",
      "\u001b[?25hCollecting nltk==3.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 47.3MB/s \n",
      "\u001b[?25hCollecting datasets==1.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 53.7MB/s \n",
      "\u001b[?25hCollecting GitPython==3.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 63.3MB/s \n",
      "\u001b[?25hCollecting lmdb==0.98\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/5c/d56dbc2532ecf14fa004c543927500c0f645eaca8bd7ec39420c7546396a/lmdb-0.98.tar.gz (869kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 48.7MB/s \n",
      "\u001b[?25hCollecting matplotlib==3.3.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/3d/db9a6b3c83c9511301152dbb64a029c3a4313c86eaef12c237b13ecf91d6/matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6MB 48.0MB/s \n",
      "\u001b[?25hCollecting tqdm<4.50.0,>=4.43.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n",
      "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (0.29.22)\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (56.0.0)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 45.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->mmf==1.0.0rc12) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (0.22.2.post1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<=0.9.1,>=0.7.0->mmf==1.0.0rc12) (7.1.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.8.1,>=1.6.0->mmf==1.0.0rc12) (3.7.4.3)\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (2.6.2)\n",
      "Collecting PyYAML>=5.1.*\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 54.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.0.12)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 43.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (2019.12.20)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.12.4)\n",
      "Collecting tokenizers==0.9.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/e7/edf655ae34925aeaefb7b7fcc3dd0887d2a1203ee6b0df4d1170d1a19d4f/tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 46.8MB/s \n",
      "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/99/dc59248df9a50349d537ffb3403c1bdc1fa69077109d46feaa0843488001/torchmetrics-0.3.1-py3-none-any.whl (271kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 61.0MB/s \n",
      "\u001b[?25hCollecting fsspec[http]>=0.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 62.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.7->mmf==1.0.0rc12) (2.4.1)\n",
      "Collecting future>=0.17.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 50.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (1.1.5)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.70.11.1)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.0.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.10.1)\n",
      "Collecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 56.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.3.3)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (7.1.2)\n",
      "Collecting aiohttp; extra == \"http\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 45.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (3.3.4)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.12.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.32.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1->mmf==1.0.0rc12) (2018.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.2.1->mmf==1.0.0rc12) (3.4.1)\n",
      "Collecting smmap<5,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (20.3.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 61.0MB/s \n",
      "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting multidict<7.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 56.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (3.1.0)\n",
      "Building wheels for collected packages: demjson, ftfy, fasttext, nltk, lmdb, future\n",
      "  Building wheel for demjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for demjson: filename=demjson-2.2.4-cp37-none-any.whl size=73546 sha256=a636e9f01b612fbc1212eff00e4e7d5da8e152d6112b5db2fca50b1a12418a25\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/d2/ab/a54fb5ea53ac3badba098160e8452fa126a51febda80440ded\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-5.8-cp37-none-any.whl size=45613 sha256=2eae8351e95e6d684f7134a5218a9f16c13d4e2972baa7e5e2b0fc4706b70824\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2463095 sha256=b2d530775d45ed37d08c4aaf7d8bea93b6f3a113edf0e57c68ef1c3373efa8a3\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449910 sha256=3d1e83221c4ff167e703d9c9e18c6422400236215dd99b9c510304991d848b4e\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=219680 sha256=020fc7ef497de2e463311f11eb13043ee4d2b2f0fd611dcd19cc4996da6c2b0d\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/97/8c/7721e4b6b0ac723c6cc45ecca60599a80f75e2367330647390\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=3267c45d475c33b28ef60af029e453590ca9327d9ab62421c0ee375106b54455\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built demjson ftfy fasttext nltk lmdb future\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pytorch-lightning 1.2.7 has requirement PyYAML!=5.4.*,>=5.1, but you'll have pyyaml 5.4.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: demjson, torch, sentencepiece, tqdm, torchtext, torchvision, ftfy, portalocker, iopath, fasttext, PyYAML, omegaconf, sacremoses, tokenizers, transformers, torchmetrics, multidict, yarl, async-timeout, aiohttp, fsspec, future, pytorch-lightning, nltk, xxhash, datasets, smmap, gitdb, GitPython, lmdb, matplotlib, mmf\n",
      "  Found existing installation: torch 1.8.1+cu101\n",
      "    Uninstalling torch-1.8.1+cu101:\n",
      "      Successfully uninstalled torch-1.8.1+cu101\n",
      "  Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "  Found existing installation: torchtext 0.9.1\n",
      "    Uninstalling torchtext-0.9.1:\n",
      "      Successfully uninstalled torchtext-0.9.1\n",
      "  Found existing installation: torchvision 0.9.1+cu101\n",
      "    Uninstalling torchvision-0.9.1+cu101:\n",
      "      Successfully uninstalled torchvision-0.9.1+cu101\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "  Found existing installation: lmdb 0.99\n",
      "    Uninstalling lmdb-0.99:\n",
      "      Successfully uninstalled lmdb-0.99\n",
      "  Found existing installation: matplotlib 3.2.2\n",
      "    Uninstalling matplotlib-3.2.2:\n",
      "      Successfully uninstalled matplotlib-3.2.2\n",
      "  Running setup.py develop for mmf\n",
      "Successfully installed GitPython-3.1.0 PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.2.1 demjson-2.2.4 fasttext-0.9.1 fsspec-2021.4.0 ftfy-5.8 future-0.18.2 gitdb-4.0.7 iopath-0.1.7 lmdb-0.98 matplotlib-3.3.4 mmf multidict-5.1.0 nltk-3.4.5 omegaconf-2.0.6 portalocker-2.3.0 pytorch-lightning-1.2.7 sacremoses-0.0.45 sentencepiece-0.1.95 smmap-4.0.0 tokenizers-0.9.2 torch-1.8.1 torchmetrics-0.3.1 torchtext-0.5.0 torchvision-0.9.1 tqdm-4.49.0 transformers-3.4.0 xxhash-2.0.2 yarl-1.6.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --editable ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 242665,
     "status": "ok",
     "timestamp": 1620084568865,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "x4PQkXOxnsOA",
    "outputId": "357e4338-fe09-4b41-e38b-5d78c7ab55d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyYAML==5.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 13.9MB/s \n",
      "\u001b[?25hCollecting imgaug==0.2.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
      "\u001b[K     |████████████████████████████████| 634kB 27.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.4.1)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (0.16.2)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.15.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.5.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.4.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (7.1.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (3.3.4)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.6) (4.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (1.3.1)\n",
      "Building wheels for collected packages: PyYAML, imgaug\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=a1dc2945970a5268b64f99ed6778131fd575d36616c4a3acd426419374fdfd40\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for imgaug: filename=imgaug-0.2.6-cp37-none-any.whl size=654019 sha256=7959bba79ac456e8cced14f47dd7e6b900a9d67e20e9a39be6259081716fbfe8\n",
      "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
      "Successfully built PyYAML imgaug\n",
      "Installing collected packages: PyYAML, imgaug\n",
      "  Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Found existing installation: imgaug 0.2.9\n",
      "    Uninstalling imgaug-0.2.9:\n",
      "      Successfully uninstalled imgaug-0.2.9\n",
      "Successfully installed PyYAML-5.3.1 imgaug-0.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install PyYAML==5.3.1 imgaug==0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241422,
     "status": "ok",
     "timestamp": 1620084568865,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "TSrAzqf9nvUO",
    "outputId": "52f978a6-03f3-45d6-8866-51c96473a131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/colab\n"
     ]
    }
   ],
   "source": [
    "%cd /content/gdrive/MyDrive/colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 488547,
     "status": "ok",
     "timestamp": 1620084816873,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "4qRna4BrnxaZ",
    "outputId": "cbcabc21-bada-41ae-9db8-4b8533a1fd9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-03 23:29:42.900322: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Data folder is /root/.cache/torch/mmf/data\n",
      "Zip path is ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Starting checksum for XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Checksum successful\n",
      "Copying ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Unzipping ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Extracting the zip can take time. Sit back and relax.\n",
      "Moving train.jsonl\n",
      "Moving dev_seen.jsonl\n",
      "Moving test_seen.jsonl\n",
      "Moving dev_unseen.jsonl\n",
      "Moving test_unseen.jsonl\n",
      "Moving img\n"
     ]
    }
   ],
   "source": [
    "!mmf_convert_hm --zip_file=\"./XjiOc5ycDBRRNwbhRlgH.zip\" --password=REDACTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK6PzXyfiPdy"
   },
   "source": [
    "Finetune on finetuned Visual BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3315018,
     "status": "ok",
     "timestamp": 1620111162725,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "7tZFNasSiLie",
    "outputId": "b3185569-2941-4e9a-f3f9-fd7076b2b12b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-04 03:37:30.991281: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/direct.yaml\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 10000\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/gdrive/MyDrive/colab/finetuned_visualbert_election_memes/\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.direct\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to /content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_seen.jsonl\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.train[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.val[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.test[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf: \u001b[0mLogging to: /content/gdrive/MyDrive/colab/finetuned_visualbert_election_memes/train.log\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/direct.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.max_updates=10000', 'training.batch_size=32', 'env.save_dir=/content/gdrive/MyDrive/colab/finetuned_visualbert_election_memes/', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct', 'checkpoint.resume_pretrained=True', 'dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl', 'dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb'])\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf_cli.run: \u001b[0mUsing seed 48604284\n",
      "\u001b[32m2021-05-04T03:38:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:38:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:38:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:38:52 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T03:38:52 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T03:38:52 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T03:38:52 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-04T03:39:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-04T03:39:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:39:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:39:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-04T03:39:08 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.finetuned.hateful_memes_direct.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.finetuned.hateful_memes.direct/visual_bert.finetuned.hateful_memes_direct.tar.gz ]\n",
      "Downloading visual_bert.finetuned.hateful_memes_direct.tar.gz: 100% 415M/415M [01:01<00:00, 6.72MB/s]\n",
      "[ Starting checksum for visual_bert.finetuned.hateful_memes_direct.tar.gz]\n",
      "[ Checksum successful for visual_bert.finetuned.hateful_memes_direct.tar.gz]\n",
      "Unpacking visual_bert.finetuned.hateful_memes_direct.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:40:17 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:40:17 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:40:17 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:40:17 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-04T03:40:17 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-04T03:40:18 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2021-05-04T03:40:18 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2021-05-04T03:45:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/10000, train/hateful_memes/cross_entropy: 0.4937, train/hateful_memes/cross_entropy/avg: 0.4937, train/total_loss: 0.4937, train/total_loss/avg: 0.4937, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 10000, lr: 0., ups: 0.30, time: 05m 28s 028ms, time_since_start: 06m 37s 662ms, eta: 09h 09m 54s 430ms\n",
      "\u001b[32m2021-05-04T03:47:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/10000, train/hateful_memes/cross_entropy: 0.2400, train/hateful_memes/cross_entropy/avg: 0.3668, train/total_loss: 0.2400, train/total_loss/avg: 0.3668, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 01m 30s 379ms, time_since_start: 08m 08s 041ms, eta: 02h 29m 58s 927ms\n",
      "\u001b[32m2021-05-04T03:48:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/10000, train/hateful_memes/cross_entropy: 0.2400, train/hateful_memes/cross_entropy/avg: 0.2781, train/total_loss: 0.2400, train/total_loss/avg: 0.2781, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 10000, lr: 0.00001, ups: 1.05, time: 01m 35s 198ms, time_since_start: 09m 43s 240ms, eta: 02h 36m 21s 993ms\n",
      "\u001b[32m2021-05-04T03:50:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/10000, train/hateful_memes/cross_entropy: 0.2400, train/hateful_memes/cross_entropy/avg: 0.2717, train/total_loss: 0.2400, train/total_loss/avg: 0.2717, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 10000, lr: 0.00001, ups: 1.12, time: 01m 29s 229ms, time_since_start: 11m 12s 469ms, eta: 02h 25m 03s 074ms\n",
      "\u001b[32m2021-05-04T03:51:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/10000, train/hateful_memes/cross_entropy: 0.2526, train/hateful_memes/cross_entropy/avg: 0.3102, train/total_loss: 0.2526, train/total_loss/avg: 0.3102, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 10000, lr: 0.00001, ups: 1.10, time: 01m 31s 357ms, time_since_start: 12m 43s 827ms, eta: 02h 26m 57s 814ms\n",
      "\u001b[32m2021-05-04T03:53:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/10000, train/hateful_memes/cross_entropy: 0.2526, train/hateful_memes/cross_entropy/avg: 0.3133, train/total_loss: 0.2526, train/total_loss/avg: 0.3133, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 10000, lr: 0.00002, ups: 1.06, time: 01m 34s 002ms, time_since_start: 14m 17s 829ms, eta: 02h 29m 37s 621ms\n",
      "\u001b[32m2021-05-04T03:54:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/10000, train/hateful_memes/cross_entropy: 0.2679, train/hateful_memes/cross_entropy/avg: 0.3068, train/total_loss: 0.2679, train/total_loss/avg: 0.3068, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 10000, lr: 0.00002, ups: 1.11, time: 01m 30s 180ms, time_since_start: 15m 48s 010ms, eta: 02h 22m 992ms\n",
      "\u001b[32m2021-05-04T03:56:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/10000, train/hateful_memes/cross_entropy: 0.2526, train/hateful_memes/cross_entropy/avg: 0.2975, train/total_loss: 0.2526, train/total_loss/avg: 0.2975, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 10000, lr: 0.00002, ups: 1.11, time: 01m 30s 701ms, time_since_start: 17m 18s 712ms, eta: 02h 21m 18s 071ms\n",
      "\u001b[32m2021-05-04T03:58:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/10000, train/hateful_memes/cross_entropy: 0.2679, train/hateful_memes/cross_entropy/avg: 0.2946, train/total_loss: 0.2679, train/total_loss/avg: 0.2946, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 10000, lr: 0.00002, ups: 1.08, time: 01m 33s 907ms, time_since_start: 18m 52s 620ms, eta: 02h 24m 42s 350ms\n",
      "\u001b[32m2021-05-04T03:59:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T03:59:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:03:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:04:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:04:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/10000, train/hateful_memes/cross_entropy: 0.2679, train/hateful_memes/cross_entropy/avg: 0.2923, train/total_loss: 0.2679, train/total_loss/avg: 0.2923, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 10000, lr: 0.00003, ups: 0.28, time: 06m 02s 380ms, time_since_start: 24m 55s 000ms, eta: 09h 12m 16s 076ms\n",
      "\u001b[32m2021-05-04T04:04:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T04:04:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:04:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:04:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:04:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:04:30 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-04T04:04:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:04:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:04:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/10000, val/hateful_memes/cross_entropy: 1.2010, val/total_loss: 1.2010, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.4235, val/hateful_memes/roc_auc: 0.7197, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 10000, val_time: 55s 649ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.719736\n",
      "\u001b[32m2021-05-04T04:07:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/10000, train/hateful_memes/cross_entropy: 0.2679, train/hateful_memes/cross_entropy/avg: 0.2872, train/total_loss: 0.2679, train/total_loss/avg: 0.2872, max mem: 9226.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 10000, lr: 0.00003, ups: 0.81, time: 02m 04s 884ms, time_since_start: 27m 55s 541ms, eta: 03h 08m 12s 735ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:07:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:07:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:08:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/10000, train/hateful_memes/cross_entropy: 0.2526, train/hateful_memes/cross_entropy/avg: 0.2666, train/total_loss: 0.2526, train/total_loss/avg: 0.2666, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 10000, lr: 0.00003, ups: 1.08, time: 01m 33s 334ms, time_since_start: 29m 28s 876ms, eta: 02h 19m 04s 882ms\n",
      "\u001b[32m2021-05-04T04:10:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/10000, train/hateful_memes/cross_entropy: 0.2679, train/hateful_memes/cross_entropy/avg: 0.2733, train/total_loss: 0.2679, train/total_loss/avg: 0.2733, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 10000, lr: 0.00003, ups: 1.10, time: 01m 31s 601ms, time_since_start: 31m 478ms, eta: 02h 14m 56s 846ms\n",
      "\u001b[32m2021-05-04T04:11:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/10000, train/hateful_memes/cross_entropy: 0.2679, train/hateful_memes/cross_entropy/avg: 0.2741, train/total_loss: 0.2679, train/total_loss/avg: 0.2741, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 10000, lr: 0.00003, ups: 1.10, time: 01m 31s 167ms, time_since_start: 32m 31s 646ms, eta: 02h 12m 45s 891ms\n",
      "\u001b[32m2021-05-04T04:13:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/10000, train/hateful_memes/cross_entropy: 0.2679, train/hateful_memes/cross_entropy/avg: 0.2681, train/total_loss: 0.2679, train/total_loss/avg: 0.2681, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 10000, lr: 0.00004, ups: 1.06, time: 01m 34s 306ms, time_since_start: 34m 05s 952ms, eta: 02h 15m 44s 284ms\n",
      "\u001b[32m2021-05-04T04:14:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/10000, train/hateful_memes/cross_entropy: 0.2526, train/hateful_memes/cross_entropy/avg: 0.2537, train/total_loss: 0.2526, train/total_loss/avg: 0.2537, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 10000, lr: 0.00004, ups: 1.10, time: 01m 31s 560ms, time_since_start: 35m 37s 513ms, eta: 02h 10m 14s 163ms\n",
      "\u001b[32m2021-05-04T04:16:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/10000, train/hateful_memes/cross_entropy: 0.2679, train/hateful_memes/cross_entropy/avg: 0.2592, train/total_loss: 0.2679, train/total_loss/avg: 0.2592, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1700, iterations: 1700, max_updates: 10000, lr: 0.00004, ups: 1.09, time: 01m 32s 796ms, time_since_start: 37m 10s 309ms, eta: 02h 10m 25s 308ms\n",
      "\u001b[32m2021-05-04T04:17:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/10000, train/hateful_memes/cross_entropy: 0.2526, train/hateful_memes/cross_entropy/avg: 0.2570, train/total_loss: 0.2526, train/total_loss/avg: 0.2570, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 10000, lr: 0.00005, ups: 1.04, time: 01m 36s 232ms, time_since_start: 38m 46s 541ms, eta: 02h 13m 37s 297ms\n",
      "\u001b[32m2021-05-04T04:19:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/10000, train/hateful_memes/cross_entropy: 0.2526, train/hateful_memes/cross_entropy/avg: 0.2492, train/total_loss: 0.2526, train/total_loss/avg: 0.2492, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 10000, lr: 0.00005, ups: 1.09, time: 01m 32s 452ms, time_since_start: 40m 18s 994ms, eta: 02h 06m 48s 462ms\n",
      "\u001b[32m2021-05-04T04:20:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T04:20:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:21:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:21:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:21:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/10000, train/hateful_memes/cross_entropy: 0.2400, train/hateful_memes/cross_entropy/avg: 0.2406, train/total_loss: 0.2400, train/total_loss/avg: 0.2406, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 2000, iterations: 2000, max_updates: 10000, lr: 0.00005, ups: 0.87, time: 01m 55s 467ms, time_since_start: 42m 14s 461ms, eta: 02h 36m 25s 204ms\n",
      "\u001b[32m2021-05-04T04:21:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T04:21:22 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:21:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:21:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:21:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:21:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:22:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:22:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/10000, val/hateful_memes/cross_entropy: 1.6902, val/total_loss: 1.6902, val/hateful_memes/accuracy: 0.6140, val/hateful_memes/binary_f1: 0.4624, val/hateful_memes/roc_auc: 0.7033, num_updates: 2000, epoch: 7, iterations: 2000, max_updates: 10000, val_time: 38s 956ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.719736\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:22:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:22:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:23:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/10000, train/hateful_memes/cross_entropy: 0.2367, train/hateful_memes/cross_entropy/avg: 0.2297, train/total_loss: 0.2367, train/total_loss/avg: 0.2297, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 10000, lr: 0.00005, ups: 1.03, time: 01m 37s 766ms, time_since_start: 44m 31s 188ms, eta: 02h 10m 47s 148ms\n",
      "\u001b[32m2021-05-04T04:25:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/10000, train/hateful_memes/cross_entropy: 0.2322, train/hateful_memes/cross_entropy/avg: 0.2268, train/total_loss: 0.2322, train/total_loss/avg: 0.2268, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 10000, lr: 0.00005, ups: 1.10, time: 01m 31s 068ms, time_since_start: 46m 02s 257ms, eta: 02h 16s 983ms\n",
      "\u001b[32m2021-05-04T04:26:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/10000, train/hateful_memes/cross_entropy: 0.2322, train/hateful_memes/cross_entropy/avg: 0.2262, train/total_loss: 0.2322, train/total_loss/avg: 0.2262, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2300, iterations: 2300, max_updates: 10000, lr: 0.00005, ups: 1.10, time: 01m 31s 267ms, time_since_start: 47m 33s 525ms, eta: 01h 59m 054ms\n",
      "\u001b[32m2021-05-04T04:28:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/10000, train/hateful_memes/cross_entropy: 0.2193, train/hateful_memes/cross_entropy/avg: 0.2217, train/total_loss: 0.2193, train/total_loss/avg: 0.2217, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2400, iterations: 2400, max_updates: 10000, lr: 0.00005, ups: 1.09, time: 01m 32s 080ms, time_since_start: 49m 05s 605ms, eta: 01h 58m 30s 080ms\n",
      "\u001b[32m2021-05-04T04:29:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/10000, train/hateful_memes/cross_entropy: 0.2141, train/hateful_memes/cross_entropy/avg: 0.2160, train/total_loss: 0.2141, train/total_loss/avg: 0.2160, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 10000, lr: 0.00005, ups: 1.10, time: 01m 31s 114ms, time_since_start: 50m 36s 719ms, eta: 01h 55m 42s 911ms\n",
      "\u001b[32m2021-05-04T04:31:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/10000, train/hateful_memes/cross_entropy: 0.2141, train/hateful_memes/cross_entropy/avg: 0.2165, train/total_loss: 0.2141, train/total_loss/avg: 0.2165, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2600, iterations: 2600, max_updates: 10000, lr: 0.00005, ups: 1.11, time: 01m 30s 787ms, time_since_start: 52m 07s 507ms, eta: 01h 53m 45s 766ms\n",
      "\u001b[32m2021-05-04T04:32:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/10000, train/hateful_memes/cross_entropy: 0.1845, train/hateful_memes/cross_entropy/avg: 0.2090, train/total_loss: 0.1845, train/total_loss/avg: 0.2090, max mem: 9226.0, experiment: run, epoch: 10, num_updates: 2700, iterations: 2700, max_updates: 10000, lr: 0.00005, ups: 1.09, time: 01m 32s 870ms, time_since_start: 53m 40s 378ms, eta: 01h 54m 48s 049ms\n",
      "\u001b[32m2021-05-04T04:34:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/10000, train/hateful_memes/cross_entropy: 0.1660, train/hateful_memes/cross_entropy/avg: 0.2023, train/total_loss: 0.1660, train/total_loss/avg: 0.2023, max mem: 9226.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 10000, lr: 0.00005, ups: 1.11, time: 01m 30s 736ms, time_since_start: 55m 11s 115ms, eta: 01h 50m 37s 575ms\n",
      "\u001b[32m2021-05-04T04:35:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/10000, train/hateful_memes/cross_entropy: 0.1160, train/hateful_memes/cross_entropy/avg: 0.1969, train/total_loss: 0.1160, train/total_loss/avg: 0.1969, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 10000, lr: 0.00004, ups: 1.08, time: 01m 33s 297ms, time_since_start: 56m 44s 412ms, eta: 01h 52m 10s 084ms\n",
      "\u001b[32m2021-05-04T04:37:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T04:37:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:37:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:37:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:37:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/10000, train/hateful_memes/cross_entropy: 0.1096, train/hateful_memes/cross_entropy/avg: 0.1920, train/total_loss: 0.1096, train/total_loss/avg: 0.1920, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 3000, iterations: 3000, max_updates: 10000, lr: 0.00004, ups: 0.88, time: 01m 53s 100ms, time_since_start: 58m 37s 512ms, eta: 02h 14m 03s 716ms\n",
      "\u001b[32m2021-05-04T04:37:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T04:37:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:37:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:37:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:38:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:38:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:38:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:38:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/10000, val/hateful_memes/cross_entropy: 1.9390, val/total_loss: 1.9390, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.4463, val/hateful_memes/roc_auc: 0.6635, num_updates: 3000, epoch: 11, iterations: 3000, max_updates: 10000, val_time: 59s 241ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.719736\n",
      "\u001b[32m2021-05-04T04:40:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/10000, train/hateful_memes/cross_entropy: 0.0793, train/hateful_memes/cross_entropy/avg: 0.1859, train/total_loss: 0.0793, train/total_loss/avg: 0.1859, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 3100, iterations: 3100, max_updates: 10000, lr: 0.00004, ups: 0.87, time: 01m 55s 400ms, time_since_start: 01h 01m 32s 159ms, eta: 02h 14m 50s 064ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:41:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:41:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:42:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/10000, train/hateful_memes/cross_entropy: 0.0793, train/hateful_memes/cross_entropy/avg: 0.1808, train/total_loss: 0.0793, train/total_loss/avg: 0.1808, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3200, iterations: 3200, max_updates: 10000, lr: 0.00004, ups: 1.08, time: 01m 33s 329ms, time_since_start: 01h 03m 05s 489ms, eta: 01h 47m 27s 982ms\n",
      "\u001b[32m2021-05-04T04:43:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/10000, train/hateful_memes/cross_entropy: 0.0793, train/hateful_memes/cross_entropy/avg: 0.1836, train/total_loss: 0.0793, train/total_loss/avg: 0.1836, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3300, iterations: 3300, max_updates: 10000, lr: 0.00004, ups: 1.12, time: 01m 29s 653ms, time_since_start: 01h 04m 35s 142ms, eta: 01h 41m 42s 882ms\n",
      "\u001b[32m2021-05-04T04:45:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/10000, train/hateful_memes/cross_entropy: 0.0776, train/hateful_memes/cross_entropy/avg: 0.1787, train/total_loss: 0.0776, train/total_loss/avg: 0.1787, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3400, iterations: 3400, max_updates: 10000, lr: 0.00004, ups: 1.11, time: 01m 30s 720ms, time_since_start: 01h 06m 05s 863ms, eta: 01h 41m 23s 385ms\n",
      "\u001b[32m2021-05-04T04:46:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/10000, train/hateful_memes/cross_entropy: 0.0776, train/hateful_memes/cross_entropy/avg: 0.1763, train/total_loss: 0.0776, train/total_loss/avg: 0.1763, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3500, iterations: 3500, max_updates: 10000, lr: 0.00004, ups: 1.08, time: 01m 33s 119ms, time_since_start: 01h 07m 38s 983ms, eta: 01h 42m 29s 622ms\n",
      "\u001b[32m2021-05-04T04:48:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/10000, train/hateful_memes/cross_entropy: 0.0776, train/hateful_memes/cross_entropy/avg: 0.1716, train/total_loss: 0.0776, train/total_loss/avg: 0.1716, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3600, iterations: 3600, max_updates: 10000, lr: 0.00004, ups: 1.11, time: 01m 30s 455ms, time_since_start: 01h 09m 09s 439ms, eta: 01h 38m 01s 805ms\n",
      "\u001b[32m2021-05-04T04:49:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/10000, train/hateful_memes/cross_entropy: 0.0487, train/hateful_memes/cross_entropy/avg: 0.1681, train/total_loss: 0.0487, train/total_loss/avg: 0.1681, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3700, iterations: 3700, max_updates: 10000, lr: 0.00004, ups: 1.11, time: 01m 30s 291ms, time_since_start: 01h 10m 39s 731ms, eta: 01h 36m 19s 403ms\n",
      "\u001b[32m2021-05-04T04:51:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/10000, train/hateful_memes/cross_entropy: 0.0454, train/hateful_memes/cross_entropy/avg: 0.1638, train/total_loss: 0.0454, train/total_loss/avg: 0.1638, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 3800, iterations: 3800, max_updates: 10000, lr: 0.00004, ups: 1.08, time: 01m 33s 307ms, time_since_start: 01h 12m 13s 039ms, eta: 01h 37m 57s 655ms\n",
      "\u001b[32m2021-05-04T04:52:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/10000, train/hateful_memes/cross_entropy: 0.0423, train/hateful_memes/cross_entropy/avg: 0.1605, train/total_loss: 0.0423, train/total_loss/avg: 0.1605, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 3900, iterations: 3900, max_updates: 10000, lr: 0.00004, ups: 1.11, time: 01m 30s 249ms, time_since_start: 01h 13m 43s 288ms, eta: 01h 33m 13s 284ms\n",
      "\u001b[32m2021-05-04T04:54:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T04:54:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:54:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:54:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:54:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/10000, train/hateful_memes/cross_entropy: 0.0345, train/hateful_memes/cross_entropy/avg: 0.1566, train/total_loss: 0.0345, train/total_loss/avg: 0.1566, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 4000, iterations: 4000, max_updates: 10000, lr: 0.00004, ups: 0.88, time: 01m 54s 255ms, time_since_start: 01h 15m 37s 543ms, eta: 01h 56m 05s 017ms\n",
      "\u001b[32m2021-05-04T04:54:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T04:54:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:54:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:54:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:55:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:55:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:55:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:55:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/10000, val/hateful_memes/cross_entropy: 2.1904, val/total_loss: 2.1904, val/hateful_memes/accuracy: 0.6260, val/hateful_memes/binary_f1: 0.5266, val/hateful_memes/roc_auc: 0.7128, num_updates: 4000, epoch: 14, iterations: 4000, max_updates: 10000, val_time: 42s 708ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.719736\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:56:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:56:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:57:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/10000, train/hateful_memes/cross_entropy: 0.0345, train/hateful_memes/cross_entropy/avg: 0.1536, train/total_loss: 0.0345, train/total_loss/avg: 0.1536, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4100, iterations: 4100, max_updates: 10000, lr: 0.00004, ups: 1.05, time: 01m 35s 400ms, time_since_start: 01h 17m 55s 657ms, eta: 01h 35m 18s 685ms\n",
      "\u001b[32m2021-05-04T04:58:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/10000, train/hateful_memes/cross_entropy: 0.0331, train/hateful_memes/cross_entropy/avg: 0.1500, train/total_loss: 0.0331, train/total_loss/avg: 0.1500, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4200, iterations: 4200, max_updates: 10000, lr: 0.00004, ups: 1.12, time: 01m 29s 836ms, time_since_start: 01h 19m 25s 494ms, eta: 01h 28m 13s 913ms\n",
      "\u001b[32m2021-05-04T05:00:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/10000, train/hateful_memes/cross_entropy: 0.0234, train/hateful_memes/cross_entropy/avg: 0.1467, train/total_loss: 0.0234, train/total_loss/avg: 0.1467, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4300, iterations: 4300, max_updates: 10000, lr: 0.00004, ups: 1.12, time: 01m 29s 900ms, time_since_start: 01h 20m 55s 394ms, eta: 01h 26m 46s 302ms\n",
      "\u001b[32m2021-05-04T05:01:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/10000, train/hateful_memes/cross_entropy: 0.0217, train/hateful_memes/cross_entropy/avg: 0.1436, train/total_loss: 0.0217, train/total_loss/avg: 0.1436, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4400, iterations: 4400, max_updates: 10000, lr: 0.00003, ups: 1.09, time: 01m 32s 130ms, time_since_start: 01h 22m 27s 525ms, eta: 01h 27m 21s 884ms\n",
      "\u001b[32m2021-05-04T05:03:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/10000, train/hateful_memes/cross_entropy: 0.0177, train/hateful_memes/cross_entropy/avg: 0.1408, train/total_loss: 0.0177, train/total_loss/avg: 0.1408, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4500, iterations: 4500, max_updates: 10000, lr: 0.00003, ups: 1.11, time: 01m 30s 541ms, time_since_start: 01h 23m 58s 067ms, eta: 01h 24m 19s 464ms\n",
      "\u001b[32m2021-05-04T05:04:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/10000, train/hateful_memes/cross_entropy: 0.0177, train/hateful_memes/cross_entropy/avg: 0.1390, train/total_loss: 0.0177, train/total_loss/avg: 0.1390, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4600, iterations: 4600, max_updates: 10000, lr: 0.00003, ups: 1.11, time: 01m 30s 578ms, time_since_start: 01h 25m 28s 645ms, eta: 01h 22m 49s 499ms\n",
      "\u001b[32m2021-05-04T05:06:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/10000, train/hateful_memes/cross_entropy: 0.0217, train/hateful_memes/cross_entropy/avg: 0.1367, train/total_loss: 0.0217, train/total_loss/avg: 0.1367, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4700, iterations: 4700, max_updates: 10000, lr: 0.00003, ups: 1.08, time: 01m 33s 498ms, time_since_start: 01h 27m 02s 143ms, eta: 01h 23m 54s 686ms\n",
      "\u001b[32m2021-05-04T05:07:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/10000, train/hateful_memes/cross_entropy: 0.0177, train/hateful_memes/cross_entropy/avg: 0.1338, train/total_loss: 0.0177, train/total_loss/avg: 0.1338, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4800, iterations: 4800, max_updates: 10000, lr: 0.00003, ups: 1.11, time: 01m 30s 106ms, time_since_start: 01h 28m 32s 250ms, eta: 01h 19m 20s 499ms\n",
      "\u001b[32m2021-05-04T05:09:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/10000, train/hateful_memes/cross_entropy: 0.0174, train/hateful_memes/cross_entropy/avg: 0.1314, train/total_loss: 0.0174, train/total_loss/avg: 0.1314, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4900, iterations: 4900, max_updates: 10000, lr: 0.00003, ups: 1.11, time: 01m 30s 804ms, time_since_start: 01h 30m 03s 054ms, eta: 01h 18m 25s 133ms\n",
      "\u001b[32m2021-05-04T05:10:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T05:10:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T05:10:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T05:11:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T05:11:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/10000, train/hateful_memes/cross_entropy: 0.0145, train/hateful_memes/cross_entropy/avg: 0.1288, train/total_loss: 0.0145, train/total_loss/avg: 0.1288, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5000, iterations: 5000, max_updates: 10000, lr: 0.00003, ups: 0.87, time: 01m 55s 872ms, time_since_start: 01h 31m 58s 927ms, eta: 01h 38m 06s 303ms\n",
      "\u001b[32m2021-05-04T05:11:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T05:11:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:11:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:11:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:11:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T05:11:56 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-04T05:12:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T05:12:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T05:12:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/10000, val/hateful_memes/cross_entropy: 2.1707, val/total_loss: 2.1707, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5488, val/hateful_memes/roc_auc: 0.7393, num_updates: 5000, epoch: 18, iterations: 5000, max_updates: 10000, val_time: 01m 18s 686ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.739338\n",
      "\u001b[32m2021-05-04T05:14:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/10000, train/hateful_memes/cross_entropy: 0.0145, train/hateful_memes/cross_entropy/avg: 0.1263, train/total_loss: 0.0145, train/total_loss/avg: 0.1263, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5100, iterations: 5100, max_updates: 10000, lr: 0.00003, ups: 0.78, time: 02m 09s 000ms, time_since_start: 01h 35m 26s 631ms, eta: 01h 47m 02s 166ms\n",
      "\u001b[32m2021-05-04T05:16:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/10000, train/hateful_memes/cross_entropy: 0.0111, train/hateful_memes/cross_entropy/avg: 0.1240, train/total_loss: 0.0111, train/total_loss/avg: 0.1240, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5200, iterations: 5200, max_updates: 10000, lr: 0.00003, ups: 1.12, time: 01m 29s 395ms, time_since_start: 01h 36m 56s 026ms, eta: 01h 12m 39s 629ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:16:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:16:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:17:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/10000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1216, train/total_loss: 0.0067, train/total_loss/avg: 0.1216, max mem: 9226.0, experiment: run, epoch: 19, num_updates: 5300, iterations: 5300, max_updates: 10000, lr: 0.00003, ups: 1.08, time: 01m 33s 284ms, time_since_start: 01h 38m 29s 311ms, eta: 01h 14m 14s 526ms\n",
      "\u001b[32m2021-05-04T05:19:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/10000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1195, train/total_loss: 0.0067, train/total_loss/avg: 0.1195, max mem: 9226.0, experiment: run, epoch: 19, num_updates: 5400, iterations: 5400, max_updates: 10000, lr: 0.00003, ups: 1.11, time: 01m 30s 899ms, time_since_start: 01h 40m 211ms, eta: 01h 10m 48s 296ms\n",
      "\u001b[32m2021-05-04T05:24:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/10000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1174, train/total_loss: 0.0067, train/total_loss/avg: 0.1174, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5500, iterations: 5500, max_updates: 10000, lr: 0.00003, ups: 0.32, time: 05m 17s 837ms, time_since_start: 01h 45m 18s 049ms, eta: 04h 02m 11s 533ms\n",
      "\u001b[32m2021-05-04T05:26:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/10000, train/hateful_memes/cross_entropy: 0.0061, train/hateful_memes/cross_entropy/avg: 0.1153, train/total_loss: 0.0061, train/total_loss/avg: 0.1153, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5600, iterations: 5600, max_updates: 10000, lr: 0.00003, ups: 1.03, time: 01m 37s 883ms, time_since_start: 01h 46m 55s 932ms, eta: 01h 12m 55s 797ms\n",
      "\u001b[32m2021-05-04T05:27:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/10000, train/hateful_memes/cross_entropy: 0.0054, train/hateful_memes/cross_entropy/avg: 0.1133, train/total_loss: 0.0054, train/total_loss/avg: 0.1133, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5700, iterations: 5700, max_updates: 10000, lr: 0.00003, ups: 1.12, time: 01m 29s 132ms, time_since_start: 01h 48m 25s 065ms, eta: 01h 04m 54s 008ms\n",
      "\u001b[32m2021-05-04T05:29:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/10000, train/hateful_memes/cross_entropy: 0.0054, train/hateful_memes/cross_entropy/avg: 0.1125, train/total_loss: 0.0054, train/total_loss/avg: 0.1125, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 5800, iterations: 5800, max_updates: 10000, lr: 0.00003, ups: 1.10, time: 01m 31s 923ms, time_since_start: 01h 49m 56s 988ms, eta: 01h 05m 22s 555ms\n",
      "\u001b[32m2021-05-04T05:30:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/10000, train/hateful_memes/cross_entropy: 0.0054, train/hateful_memes/cross_entropy/avg: 0.1107, train/total_loss: 0.0054, train/total_loss/avg: 0.1107, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 5900, iterations: 5900, max_updates: 10000, lr: 0.00003, ups: 1.14, time: 01m 28s 627ms, time_since_start: 01h 51m 25s 616ms, eta: 01h 01m 31s 874ms\n",
      "\u001b[32m2021-05-04T05:32:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T05:32:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T05:36:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T05:37:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T05:37:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/10000, train/hateful_memes/cross_entropy: 0.0047, train/hateful_memes/cross_entropy/avg: 0.1089, train/total_loss: 0.0047, train/total_loss/avg: 0.1089, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 6000, iterations: 6000, max_updates: 10000, lr: 0.00003, ups: 0.26, time: 06m 28s 904ms, time_since_start: 01h 57m 54s 520ms, eta: 04h 23m 25s 070ms\n",
      "\u001b[32m2021-05-04T05:37:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T05:37:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:37:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:37:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:37:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T05:37:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T05:37:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T05:37:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/10000, val/hateful_memes/cross_entropy: 2.2438, val/total_loss: 2.2438, val/hateful_memes/accuracy: 0.6220, val/hateful_memes/binary_f1: 0.4765, val/hateful_memes/roc_auc: 0.7341, num_updates: 6000, epoch: 21, iterations: 6000, max_updates: 10000, val_time: 49s 873ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.739338\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:39:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:39:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:39:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/10000, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1071, train/total_loss: 0.0036, train/total_loss/avg: 0.1071, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6100, iterations: 6100, max_updates: 10000, lr: 0.00002, ups: 1.04, time: 01m 36s 862ms, time_since_start: 02h 21s 259ms, eta: 01h 03m 58s 061ms\n",
      "\u001b[32m2021-05-04T05:40:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/10000, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1054, train/total_loss: 0.0036, train/total_loss/avg: 0.1054, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6200, iterations: 6200, max_updates: 10000, lr: 0.00002, ups: 1.12, time: 01m 29s 599ms, time_since_start: 02h 01m 50s 858ms, eta: 57m 39s 258ms\n",
      "\u001b[32m2021-05-04T05:42:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/10000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.1038, train/total_loss: 0.0031, train/total_loss/avg: 0.1038, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6300, iterations: 6300, max_updates: 10000, lr: 0.00002, ups: 1.12, time: 01m 29s 775ms, time_since_start: 02h 03m 20s 633ms, eta: 56m 14s 824ms\n",
      "\u001b[32m2021-05-04T05:44:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/10000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.1025, train/total_loss: 0.0031, train/total_loss/avg: 0.1025, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6400, iterations: 6400, max_updates: 10000, lr: 0.00002, ups: 1.10, time: 01m 31s 884ms, time_since_start: 02h 04m 52s 518ms, eta: 56m 774ms\n",
      "\u001b[32m2021-05-04T05:45:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/10000, train/hateful_memes/cross_entropy: 0.0030, train/hateful_memes/cross_entropy/avg: 0.1009, train/total_loss: 0.0030, train/total_loss/avg: 0.1009, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6500, iterations: 6500, max_updates: 10000, lr: 0.00002, ups: 1.12, time: 01m 29s 120ms, time_since_start: 02h 06m 21s 638ms, eta: 52m 49s 113ms\n",
      "\u001b[32m2021-05-04T05:46:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/10000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.0994, train/total_loss: 0.0014, train/total_loss/avg: 0.0994, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6600, iterations: 6600, max_updates: 10000, lr: 0.00002, ups: 1.12, time: 01m 29s 286ms, time_since_start: 02h 07m 50s 925ms, eta: 51m 24s 328ms\n",
      "\u001b[32m2021-05-04T05:48:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/10000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0979, train/total_loss: 0.0013, train/total_loss/avg: 0.0979, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6700, iterations: 6700, max_updates: 10000, lr: 0.00002, ups: 1.08, time: 01m 33s 456ms, time_since_start: 02h 09m 24s 381ms, eta: 52m 13s 400ms\n",
      "\u001b[32m2021-05-04T05:50:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/10000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0965, train/total_loss: 0.0013, train/total_loss/avg: 0.0965, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6800, iterations: 6800, max_updates: 10000, lr: 0.00002, ups: 1.10, time: 01m 31s 045ms, time_since_start: 02h 10m 55s 427ms, eta: 49m 20s 083ms\n",
      "\u001b[32m2021-05-04T05:51:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/10000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.0951, train/total_loss: 0.0008, train/total_loss/avg: 0.0951, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6900, iterations: 6900, max_updates: 10000, lr: 0.00002, ups: 1.11, time: 01m 30s 698ms, time_since_start: 02h 12m 26s 126ms, eta: 47m 36s 636ms\n",
      "\u001b[32m2021-05-04T05:53:07 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T05:53:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T05:57:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T05:57:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T05:57:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/10000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0938, train/total_loss: 0.0013, train/total_loss/avg: 0.0938, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7000, iterations: 7000, max_updates: 10000, lr: 0.00002, ups: 0.27, time: 06m 06s 271ms, time_since_start: 02h 18m 32s 397ms, eta: 03h 06m 03s 944ms\n",
      "\u001b[32m2021-05-04T05:57:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T05:57:40 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:57:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:57:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:58:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T05:58:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T05:58:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T05:58:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/10000, val/hateful_memes/cross_entropy: 2.0652, val/total_loss: 2.0652, val/hateful_memes/accuracy: 0.6480, val/hateful_memes/binary_f1: 0.5440, val/hateful_memes/roc_auc: 0.7167, num_updates: 7000, epoch: 25, iterations: 7000, max_updates: 10000, val_time: 55s 262ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.739338\n",
      "\u001b[32m2021-05-04T06:00:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/10000, train/hateful_memes/cross_entropy: 0.0008, train/hateful_memes/cross_entropy/avg: 0.0925, train/total_loss: 0.0008, train/total_loss/avg: 0.0925, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7100, iterations: 7100, max_updates: 10000, lr: 0.00002, ups: 1.01, time: 01m 39s 236ms, time_since_start: 02h 21m 06s 898ms, eta: 48m 43s 890ms\n",
      "\u001b[32m2021-05-04T06:01:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/10000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0912, train/total_loss: 0.0007, train/total_loss/avg: 0.0912, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7200, iterations: 7200, max_updates: 10000, lr: 0.00002, ups: 1.11, time: 01m 30s 665ms, time_since_start: 02h 22m 37s 564ms, eta: 42m 59s 260ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:02:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:02:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T06:07:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/10000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0900, train/total_loss: 0.0007, train/total_loss/avg: 0.0900, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7300, iterations: 7300, max_updates: 10000, lr: 0.00002, ups: 0.31, time: 05m 24s 677ms, time_since_start: 02h 28m 02s 241ms, eta: 02h 28m 26s 541ms\n",
      "\u001b[32m2021-05-04T06:08:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/10000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0887, train/total_loss: 0.0007, train/total_loss/avg: 0.0887, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7400, iterations: 7400, max_updates: 10000, lr: 0.00002, ups: 1.11, time: 01m 30s 681ms, time_since_start: 02h 29m 32s 922ms, eta: 39m 55s 432ms\n",
      "\u001b[32m2021-05-04T06:10:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/10000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0886, train/total_loss: 0.0007, train/total_loss/avg: 0.0886, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7500, iterations: 7500, max_updates: 10000, lr: 0.00002, ups: 1.10, time: 01m 31s 164ms, time_since_start: 02h 31m 04s 086ms, eta: 38m 35s 567ms\n",
      "\u001b[32m2021-05-04T06:11:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/10000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0874, train/total_loss: 0.0007, train/total_loss/avg: 0.0874, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7600, iterations: 7600, max_updates: 10000, lr: 0.00002, ups: 1.08, time: 01m 33s 647ms, time_since_start: 02h 32m 37s 734ms, eta: 38m 03s 511ms\n",
      "\u001b[32m2021-05-04T06:13:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0863, train/total_loss: 0.0006, train/total_loss/avg: 0.0863, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7700, iterations: 7700, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 01m 30s 032ms, time_since_start: 02h 34m 07s 766ms, eta: 35m 03s 870ms\n",
      "\u001b[32m2021-05-04T06:14:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0852, train/total_loss: 0.0004, train/total_loss/avg: 0.0852, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7800, iterations: 7800, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 01m 30s 334ms, time_since_start: 02h 35m 38s 101ms, eta: 33m 39s 162ms\n",
      "\u001b[32m2021-05-04T06:16:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0841, train/total_loss: 0.0004, train/total_loss/avg: 0.0841, max mem: 9226.0, experiment: run, epoch: 28, num_updates: 7900, iterations: 7900, max_updates: 10000, lr: 0.00001, ups: 1.08, time: 01m 33s 729ms, time_since_start: 02h 37m 11s 830ms, eta: 33m 19s 807ms\n",
      "\u001b[32m2021-05-04T06:17:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T06:17:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T06:18:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T06:18:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T06:18:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0831, train/total_loss: 0.0003, train/total_loss/avg: 0.0831, max mem: 9226.0, experiment: run, epoch: 28, num_updates: 8000, iterations: 8000, max_updates: 10000, lr: 0.00001, ups: 0.87, time: 01m 55s 008ms, time_since_start: 02h 39m 06s 839ms, eta: 38m 56s 977ms\n",
      "\u001b[32m2021-05-04T06:18:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T06:18:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:18:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:18:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T06:18:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T06:18:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T06:18:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T06:18:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/10000, val/hateful_memes/cross_entropy: 2.9474, val/total_loss: 2.9474, val/hateful_memes/accuracy: 0.6260, val/hateful_memes/binary_f1: 0.4672, val/hateful_memes/roc_auc: 0.7301, num_updates: 8000, epoch: 28, iterations: 8000, max_updates: 10000, val_time: 42s 381ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.739338\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:20:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:20:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T06:20:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0821, train/total_loss: 0.0004, train/total_loss/avg: 0.0821, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8100, iterations: 8100, max_updates: 10000, lr: 0.00001, ups: 1.03, time: 01m 37s 193ms, time_since_start: 02h 41m 26s 417ms, eta: 31m 16s 226ms\n",
      "\u001b[32m2021-05-04T06:22:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0811, train/total_loss: 0.0004, train/total_loss/avg: 0.0811, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8200, iterations: 8200, max_updates: 10000, lr: 0.00001, ups: 1.12, time: 01m 29s 846ms, time_since_start: 02h 42m 56s 264ms, eta: 27m 23s 111ms\n",
      "\u001b[32m2021-05-04T06:23:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0801, train/total_loss: 0.0004, train/total_loss/avg: 0.0801, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8300, iterations: 8300, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 01m 30s 547ms, time_since_start: 02h 44m 26s 811ms, eta: 26m 03s 932ms\n",
      "\u001b[32m2021-05-04T06:25:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0792, train/total_loss: 0.0003, train/total_loss/avg: 0.0792, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8400, iterations: 8400, max_updates: 10000, lr: 0.00001, ups: 1.08, time: 01m 33s 643ms, time_since_start: 02h 46m 455ms, eta: 25m 22s 273ms\n",
      "\u001b[32m2021-05-04T06:26:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0782, train/total_loss: 0.0003, train/total_loss/avg: 0.0782, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8500, iterations: 8500, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 01m 30s 353ms, time_since_start: 02h 47m 30s 809ms, eta: 22m 56s 994ms\n",
      "\u001b[32m2021-05-04T06:28:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0773, train/total_loss: 0.0003, train/total_loss/avg: 0.0773, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8600, iterations: 8600, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 01m 30s 958ms, time_since_start: 02h 49m 01s 768ms, eta: 21m 33s 800ms\n",
      "\u001b[32m2021-05-04T06:29:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0764, train/total_loss: 0.0003, train/total_loss/avg: 0.0764, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8700, iterations: 8700, max_updates: 10000, lr: 0.00001, ups: 1.08, time: 01m 33s 484ms, time_since_start: 02h 50m 35s 252ms, eta: 20m 34s 739ms\n",
      "\u001b[32m2021-05-04T06:31:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0756, train/total_loss: 0.0003, train/total_loss/avg: 0.0756, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8800, iterations: 8800, max_updates: 10000, lr: 0.00001, ups: 1.12, time: 01m 29s 665ms, time_since_start: 02h 52m 04s 917ms, eta: 18m 13s 197ms\n",
      "\u001b[32m2021-05-04T06:32:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0747, train/total_loss: 0.0003, train/total_loss/avg: 0.0747, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8900, iterations: 8900, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 01m 30s 277ms, time_since_start: 02h 53m 35s 195ms, eta: 16m 48s 939ms\n",
      "\u001b[32m2021-05-04T06:34:16 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T06:34:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T06:34:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T06:34:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T06:34:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/10000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0739, train/total_loss: 0.0002, train/total_loss/avg: 0.0739, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9000, iterations: 9000, max_updates: 10000, lr: 0.00001, ups: 0.88, time: 01m 54s 029ms, time_since_start: 02h 55m 29s 224ms, eta: 19m 18s 539ms\n",
      "\u001b[32m2021-05-04T06:34:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T06:34:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:34:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:34:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T06:35:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T06:35:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T06:35:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T06:35:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/10000, val/hateful_memes/cross_entropy: 3.1167, val/total_loss: 3.1167, val/hateful_memes/accuracy: 0.6280, val/hateful_memes/binary_f1: 0.4862, val/hateful_memes/roc_auc: 0.7354, num_updates: 9000, epoch: 32, iterations: 9000, max_updates: 10000, val_time: 47s 751ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.739338\n",
      "\u001b[32m2021-05-04T06:37:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/10000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0731, train/total_loss: 0.0002, train/total_loss/avg: 0.0731, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9100, iterations: 9100, max_updates: 10000, lr: 0.00001, ups: 0.88, time: 01m 53s 807ms, time_since_start: 02h 58m 10s 792ms, eta: 17m 20s 717ms\n",
      "\u001b[32m2021-05-04T06:38:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0723, train/total_loss: 0.0001, train/total_loss/avg: 0.0723, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9200, iterations: 9200, max_updates: 10000, lr: 0.00001, ups: 1.11, time: 01m 30s 892ms, time_since_start: 02h 59m 41s 685ms, eta: 12m 18s 775ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:39:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:39:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T06:40:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0715, train/total_loss: 0.0001, train/total_loss/avg: 0.0715, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9300, iterations: 9300, max_updates: 10000, lr: 0., ups: 1.08, time: 01m 33s 586ms, time_since_start: 03h 01m 15s 271ms, eta: 11m 05s 585ms\n",
      "\u001b[32m2021-05-04T06:41:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0707, train/total_loss: 0.0001, train/total_loss/avg: 0.0707, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9400, iterations: 9400, max_updates: 10000, lr: 0., ups: 1.09, time: 01m 32s 028ms, time_since_start: 03h 02m 47s 300ms, eta: 09m 21s 008ms\n",
      "\u001b[32m2021-05-04T06:43:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0700, train/total_loss: 0.0001, train/total_loss/avg: 0.0700, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9500, iterations: 9500, max_updates: 10000, lr: 0., ups: 1.11, time: 01m 30s 319ms, time_since_start: 03h 04m 17s 619ms, eta: 07m 38s 821ms\n",
      "\u001b[32m2021-05-04T06:45:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0693, train/total_loss: 0.0001, train/total_loss/avg: 0.0693, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9600, iterations: 9600, max_updates: 10000, lr: 0., ups: 1.06, time: 01m 34s 173ms, time_since_start: 03h 05m 51s 793ms, eta: 06m 22s 722ms\n",
      "\u001b[32m2021-05-04T06:46:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/10000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0686, train/total_loss: 0.0001, train/total_loss/avg: 0.0686, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9700, iterations: 9700, max_updates: 10000, lr: 0., ups: 1.10, time: 01m 31s 371ms, time_since_start: 03h 07m 23s 164ms, eta: 04m 38s 498ms\n",
      "\u001b[32m2021-05-04T06:48:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/10000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0679, train/total_loss: 0.0000, train/total_loss/avg: 0.0679, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9800, iterations: 9800, max_updates: 10000, lr: 0., ups: 1.11, time: 01m 30s 690ms, time_since_start: 03h 08m 53s 854ms, eta: 03m 04s 282ms\n",
      "\u001b[32m2021-05-04T06:49:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/10000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0672, train/total_loss: 0.0000, train/total_loss/avg: 0.0672, max mem: 9226.0, experiment: run, epoch: 35, num_updates: 9900, iterations: 9900, max_updates: 10000, lr: 0., ups: 1.06, time: 01m 34s 652ms, time_since_start: 03h 10m 28s 507ms, eta: 01m 36s 166ms\n",
      "\u001b[32m2021-05-04T06:51:07 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T06:51:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T06:51:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T06:51:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T06:51:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/10000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0665, train/total_loss: 0.0000, train/total_loss/avg: 0.0665, max mem: 9226.0, experiment: run, epoch: 35, num_updates: 10000, iterations: 10000, max_updates: 10000, lr: 0., ups: 0.90, time: 01m 51s 026ms, time_since_start: 03h 12m 19s 534ms, eta: 0ms\n",
      "\u001b[32m2021-05-04T06:51:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T06:51:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:51:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:51:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T06:52:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/10000, val/hateful_memes/cross_entropy: 3.1188, val/total_loss: 3.1188, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5351, val/hateful_memes/roc_auc: 0.7361, num_updates: 10000, epoch: 35, iterations: 10000, max_updates: 10000, val_time: 34s 790ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.739338\n",
      "\u001b[32m2021-05-04T06:52:03 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2021-05-04T06:52:03 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2021-05-04T06:52:03 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2021-05-04T06:52:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-04T06:52:15 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 5000\n",
      "\u001b[32m2021-05-04T06:52:15 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 5000\n",
      "\u001b[32m2021-05-04T06:52:15 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 18\n",
      "\u001b[32m2021-05-04T06:52:17 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-04T06:52:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:52:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:52:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "100% 16/16 [00:20<00:00,  1.29s/it]\n",
      "\u001b[32m2021-05-04T06:52:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/10000, val/hateful_memes/cross_entropy: 2.1707, val/total_loss: 2.1707, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5488, val/hateful_memes/roc_auc: 0.7393\n",
      "\u001b[32m2021-05-04T06:52:38 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 03h 13m 29s 924ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/direct.yaml \\\n",
    "  model=visual_bert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=train_val \\\n",
    "  training.max_updates=10000 \\\n",
    "  training.batch_size=32 \\\n",
    "  env.save_dir=/content/gdrive/MyDrive/colab/finetuned_visualbert_election_memes/ \\\n",
    "  checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct \\\n",
    "  checkpoint.resume_pretrained=True \\\n",
    "  dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mid_-AZ9i6W1"
   },
   "source": [
    "Finetune on finetuned ViLBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13979220,
     "status": "ok",
     "timestamp": 1620099428373,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "jnl2u0LhisK7",
    "outputId": "8bb9ae15-1a5d-4b16-c277-72bc10d1f68c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-03 23:44:11.388874: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/defaults.yaml\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 10000\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/gdrive/MyDrive/colab/finetuned_vilbert_election_memes/\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.direct\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to /content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_seen.jsonl\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.train[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.val[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.test[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf: \u001b[0mLogging to: /content/gdrive/MyDrive/colab/finetuned_vilbert_election_memes/train.log\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=train_val', 'training.max_updates=10000', 'training.batch_size=32', 'env.save_dir=/content/gdrive/MyDrive/colab/finetuned_vilbert_election_memes/', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct', 'checkpoint.resume_pretrained=True', 'dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl', 'dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb'])\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf_cli.run: \u001b[0mUsing seed 15902294\n",
      "\u001b[32m2021-05-03T23:44:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100% 10.3G/10.3G [07:52<00:00, 21.8MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n",
      "Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 153kB/s] \n",
      "[ Starting checksum for extras.tar.gz]\n",
      "[ Checksum successful for extras.tar.gz]\n",
      "Unpacking extras.tar.gz\n",
      "\u001b[32m2021-05-03T23:58:04 | filelock: \u001b[0mLock 139736650212944 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "Downloading: 100% 433/433 [00:00<00:00, 391kB/s]\n",
      "\u001b[32m2021-05-03T23:58:04 | filelock: \u001b[0mLock 139736650212944 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "\u001b[32m2021-05-03T23:58:05 | filelock: \u001b[0mLock 139736685953296 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 675kB/s]\n",
      "\u001b[32m2021-05-03T23:58:05 | filelock: \u001b[0mLock 139736685953296 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:58:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:58:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:58:06 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T23:58:06 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T23:58:06 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T23:58:06 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2021-05-03T23:58:06 | filelock: \u001b[0mLock 139736644725520 acquired on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Downloading: 100% 440M/440M [00:06<00:00, 68.2MB/s]\n",
      "\u001b[32m2021-05-03T23:58:12 | filelock: \u001b[0mLock 139736644725520 released on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-03T23:58:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-03T23:58:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:58:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:58:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-03T23:58:22 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_direct.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.finetuned.hateful_memes.direct/vilbert.finetuned.hateful_memes_direct.tar.gz ]\n",
      "Downloading vilbert.finetuned.hateful_memes_direct.tar.gz: 100% 918M/918M [01:20<00:00, 11.4MB/s]\n",
      "[ Starting checksum for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
      "[ Checksum successful for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
      "Unpacking vilbert.finetuned.hateful_memes_direct.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:59:57 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:59:57 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:59:57 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:59:57 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
      "  (model): ViLBERTForClassification(\n",
      "    (bert): ViLBERTBase(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (v_embeddings): BertImageFeatureEmbeddings(\n",
      "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (v_layer): ModuleList(\n",
      "          (0): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (c_layer): ModuleList(\n",
      "          (0): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (t_pooler): BertTextPooler(\n",
      "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (v_pooler): BertImagePooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
      "\u001b[32m2021-05-03T23:59:57 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2021-05-04T00:05:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/10000, train/hateful_memes/cross_entropy: 0.5531, train/hateful_memes/cross_entropy/avg: 0.5531, train/total_loss: 0.5531, train/total_loss/avg: 0.5531, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 10000, lr: 0., ups: 0.30, time: 05m 35s 278ms, time_since_start: 07m 10s 401ms, eta: 09h 22m 03s 687ms\n",
      "\u001b[32m2021-05-04T00:07:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/10000, train/hateful_memes/cross_entropy: 0.3921, train/hateful_memes/cross_entropy/avg: 0.4726, train/total_loss: 0.3921, train/total_loss/avg: 0.4726, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 10000, lr: 0., ups: 0.97, time: 01m 43s 359ms, time_since_start: 08m 53s 760ms, eta: 02h 51m 31s 281ms\n",
      "\u001b[32m2021-05-04T00:09:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/10000, train/hateful_memes/cross_entropy: 0.5016, train/hateful_memes/cross_entropy/avg: 0.4822, train/total_loss: 0.5016, train/total_loss/avg: 0.4822, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 10000, lr: 0., ups: 0.93, time: 01m 47s 259ms, time_since_start: 10m 41s 020ms, eta: 02h 56m 10s 607ms\n",
      "\u001b[32m2021-05-04T00:10:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/10000, train/hateful_memes/cross_entropy: 0.3984, train/hateful_memes/cross_entropy/avg: 0.4613, train/total_loss: 0.3984, train/total_loss/avg: 0.4613, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 10000, lr: 0., ups: 0.99, time: 01m 41s 953ms, time_since_start: 12m 22s 973ms, eta: 02h 45m 44s 113ms\n",
      "\u001b[32m2021-05-04T00:12:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/10000, train/hateful_memes/cross_entropy: 0.3984, train/hateful_memes/cross_entropy/avg: 0.4359, train/total_loss: 0.3984, train/total_loss/avg: 0.4359, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 10000, lr: 0., ups: 0.97, time: 01m 43s 653ms, time_since_start: 14m 06s 626ms, eta: 02h 46m 44s 595ms\n",
      "\u001b[32m2021-05-04T00:14:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/10000, train/hateful_memes/cross_entropy: 0.3921, train/hateful_memes/cross_entropy/avg: 0.3952, train/total_loss: 0.3921, train/total_loss/avg: 0.3952, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 10000, lr: 0., ups: 0.95, time: 01m 45s 457ms, time_since_start: 15m 52s 084ms, eta: 02h 47m 51s 626ms\n",
      "\u001b[32m2021-05-04T00:15:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/10000, train/hateful_memes/cross_entropy: 0.3921, train/hateful_memes/cross_entropy/avg: 0.3788, train/total_loss: 0.3921, train/total_loss/avg: 0.3788, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 10000, lr: 0., ups: 0.98, time: 01m 42s 490ms, time_since_start: 17m 34s 575ms, eta: 02h 41m 24s 174ms\n",
      "\u001b[32m2021-05-04T00:17:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/10000, train/hateful_memes/cross_entropy: 0.3415, train/hateful_memes/cross_entropy/avg: 0.3741, train/total_loss: 0.3415, train/total_loss/avg: 0.3741, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 10000, lr: 0., ups: 0.97, time: 01m 43s 800ms, time_since_start: 19m 18s 376ms, eta: 02h 41m 42s 460ms\n",
      "\u001b[32m2021-05-04T00:19:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/10000, train/hateful_memes/cross_entropy: 0.3415, train/hateful_memes/cross_entropy/avg: 0.3615, train/total_loss: 0.3415, train/total_loss/avg: 0.3615, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 10000, lr: 0., ups: 0.96, time: 01m 44s 385ms, time_since_start: 21m 02s 761ms, eta: 02h 40m 51s 054ms\n",
      "\u001b[32m2021-05-04T00:21:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T00:21:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:25:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:26:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:26:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/10000, train/hateful_memes/cross_entropy: 0.3364, train/hateful_memes/cross_entropy/avg: 0.3590, train/total_loss: 0.3364, train/total_loss/avg: 0.3590, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 10000, lr: 0.00001, ups: 0.25, time: 06m 47s 834ms, time_since_start: 27m 50s 595ms, eta: 10h 21m 32s 369ms\n",
      "\u001b[32m2021-05-04T00:26:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T00:26:13 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:26:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:26:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:27:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:27:57 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-04T00:28:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:29:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:29:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/10000, val/hateful_memes/cross_entropy: 1.3164, val/total_loss: 1.3164, val/hateful_memes/accuracy: 0.6380, val/hateful_memes/binary_f1: 0.5224, val/hateful_memes/roc_auc: 0.6944, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 10000, val_time: 03m 05s 708ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.694388\n",
      "\u001b[32m2021-05-04T00:31:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/10000, train/hateful_memes/cross_entropy: 0.3364, train/hateful_memes/cross_entropy/avg: 0.3451, train/total_loss: 0.3364, train/total_loss/avg: 0.3451, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 10000, lr: 0.00001, ups: 0.65, time: 02m 34s 303ms, time_since_start: 33m 30s 609ms, eta: 03h 52m 32s 747ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:32:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:32:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:33:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/10000, train/hateful_memes/cross_entropy: 0.3342, train/hateful_memes/cross_entropy/avg: 0.3413, train/total_loss: 0.3342, train/total_loss/avg: 0.3413, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 10000, lr: 0.00001, ups: 0.95, time: 01m 45s 417ms, time_since_start: 35m 16s 026ms, eta: 02h 37m 05s 135ms\n",
      "\u001b[32m2021-05-04T00:35:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/10000, train/hateful_memes/cross_entropy: 0.3342, train/hateful_memes/cross_entropy/avg: 0.3334, train/total_loss: 0.3342, train/total_loss/avg: 0.3334, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 10000, lr: 0.00001, ups: 0.96, time: 01m 44s 381ms, time_since_start: 37m 408ms, eta: 02h 33m 46s 479ms\n",
      "\u001b[32m2021-05-04T00:37:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/10000, train/hateful_memes/cross_entropy: 0.3003, train/hateful_memes/cross_entropy/avg: 0.3195, train/total_loss: 0.3003, train/total_loss/avg: 0.3195, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 320ms, time_since_start: 38m 43s 729ms, eta: 02h 30m 27s 767ms\n",
      "\u001b[32m2021-05-04T00:38:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/10000, train/hateful_memes/cross_entropy: 0.3003, train/hateful_memes/cross_entropy/avg: 0.3059, train/total_loss: 0.3003, train/total_loss/avg: 0.3059, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 10000, lr: 0.00001, ups: 0.95, time: 01m 45s 578ms, time_since_start: 40m 29s 307ms, eta: 02h 31m 57s 775ms\n",
      "\u001b[32m2021-05-04T00:40:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/10000, train/hateful_memes/cross_entropy: 0.3003, train/hateful_memes/cross_entropy/avg: 0.3109, train/total_loss: 0.3003, train/total_loss/avg: 0.3109, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 575ms, time_since_start: 42m 12s 882ms, eta: 02h 27m 19s 514ms\n",
      "\u001b[32m2021-05-04T00:42:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/10000, train/hateful_memes/cross_entropy: 0.3003, train/hateful_memes/cross_entropy/avg: 0.3055, train/total_loss: 0.3003, train/total_loss/avg: 0.3055, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1700, iterations: 1700, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 038ms, time_since_start: 43m 55s 921ms, eta: 02h 24m 49s 061ms\n",
      "\u001b[32m2021-05-04T00:44:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/10000, train/hateful_memes/cross_entropy: 0.2800, train/hateful_memes/cross_entropy/avg: 0.2926, train/total_loss: 0.2800, train/total_loss/avg: 0.2926, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 10000, lr: 0.00001, ups: 0.95, time: 01m 45s 026ms, time_since_start: 45m 40s 948ms, eta: 02h 25m 49s 982ms\n",
      "\u001b[32m2021-05-04T00:45:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/10000, train/hateful_memes/cross_entropy: 0.2800, train/hateful_memes/cross_entropy/avg: 0.2851, train/total_loss: 0.2800, train/total_loss/avg: 0.2851, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 683ms, time_since_start: 47m 24s 631ms, eta: 02h 22m 12s 714ms\n",
      "\u001b[32m2021-05-04T00:47:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T00:47:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:47:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:48:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:48:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/10000, train/hateful_memes/cross_entropy: 0.2610, train/hateful_memes/cross_entropy/avg: 0.2736, train/total_loss: 0.2610, train/total_loss/avg: 0.2736, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 2000, iterations: 2000, max_updates: 10000, lr: 0.00001, ups: 0.61, time: 02m 45s 386ms, time_since_start: 50m 10s 018ms, eta: 03h 44m 02s 649ms\n",
      "\u001b[32m2021-05-04T00:48:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T00:48:32 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:48:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:48:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:49:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:49:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:50:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:50:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/10000, val/hateful_memes/cross_entropy: 1.6406, val/total_loss: 1.6406, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5491, val/hateful_memes/roc_auc: 0.6737, num_updates: 2000, epoch: 7, iterations: 2000, max_updates: 10000, val_time: 01m 49s 463ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.694388\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:51:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:51:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:52:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/10000, train/hateful_memes/cross_entropy: 0.2377, train/hateful_memes/cross_entropy/avg: 0.2617, train/total_loss: 0.2377, train/total_loss/avg: 0.2617, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 10000, lr: 0.00001, ups: 0.65, time: 02m 34s 534ms, time_since_start: 54m 34s 024ms, eta: 03h 26m 43s 560ms\n",
      "\u001b[32m2021-05-04T00:54:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/10000, train/hateful_memes/cross_entropy: 0.2179, train/hateful_memes/cross_entropy/avg: 0.2540, train/total_loss: 0.2179, train/total_loss/avg: 0.2540, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 571ms, time_since_start: 56m 17s 595ms, eta: 02h 16m 47s 843ms\n",
      "\u001b[32m2021-05-04T00:56:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/10000, train/hateful_memes/cross_entropy: 0.2055, train/hateful_memes/cross_entropy/avg: 0.2451, train/total_loss: 0.2055, train/total_loss/avg: 0.2451, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2300, iterations: 2300, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 559ms, time_since_start: 58m 01s 155ms, eta: 02h 15m 01s 644ms\n",
      "\u001b[32m2021-05-04T00:58:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/10000, train/hateful_memes/cross_entropy: 0.1921, train/hateful_memes/cross_entropy/avg: 0.2379, train/total_loss: 0.1921, train/total_loss/avg: 0.2379, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2400, iterations: 2400, max_updates: 10000, lr: 0.00001, ups: 0.95, time: 01m 45s 706ms, time_since_start: 59m 46s 861ms, eta: 02h 16m 02s 213ms\n",
      "\u001b[32m2021-05-04T00:59:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/10000, train/hateful_memes/cross_entropy: 0.1500, train/hateful_memes/cross_entropy/avg: 0.2314, train/total_loss: 0.1500, train/total_loss/avg: 0.2314, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 10000, lr: 0.00001, ups: 0.96, time: 01m 44s 180ms, time_since_start: 01h 01m 31s 041ms, eta: 02h 12m 18s 522ms\n",
      "\u001b[32m2021-05-04T01:01:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/10000, train/hateful_memes/cross_entropy: 0.1386, train/hateful_memes/cross_entropy/avg: 0.2265, train/total_loss: 0.1386, train/total_loss/avg: 0.2265, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2600, iterations: 2600, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 406ms, time_since_start: 01h 03m 14s 448ms, eta: 02h 09m 34s 509ms\n",
      "\u001b[32m2021-05-04T01:03:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/10000, train/hateful_memes/cross_entropy: 0.1154, train/hateful_memes/cross_entropy/avg: 0.2198, train/total_loss: 0.1154, train/total_loss/avg: 0.2198, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2700, iterations: 2700, max_updates: 10000, lr: 0.00001, ups: 0.94, time: 01m 46s 405ms, time_since_start: 01h 05m 853ms, eta: 02h 11m 31s 859ms\n",
      "\u001b[32m2021-05-04T01:05:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/10000, train/hateful_memes/cross_entropy: 0.1035, train/hateful_memes/cross_entropy/avg: 0.2123, train/total_loss: 0.1035, train/total_loss/avg: 0.2123, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 995ms, time_since_start: 01h 06m 44s 848ms, eta: 02h 06m 47s 490ms\n",
      "\u001b[32m2021-05-04T01:06:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/10000, train/hateful_memes/cross_entropy: 0.0932, train/hateful_memes/cross_entropy/avg: 0.2051, train/total_loss: 0.0932, train/total_loss/avg: 0.2051, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 10000, lr: 0.00001, ups: 0.94, time: 01m 46s 694ms, time_since_start: 01h 08m 31s 543ms, eta: 02h 08m 16s 514ms\n",
      "\u001b[32m2021-05-04T01:08:35 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T01:08:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:09:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:09:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:09:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/10000, train/hateful_memes/cross_entropy: 0.0763, train/hateful_memes/cross_entropy/avg: 0.1987, train/total_loss: 0.0763, train/total_loss/avg: 0.1987, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 3000, iterations: 3000, max_updates: 10000, lr: 0.00001, ups: 0.62, time: 02m 42s 707ms, time_since_start: 01h 11m 14s 250ms, eta: 03h 12m 51s 744ms\n",
      "\u001b[32m2021-05-04T01:09:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T01:09:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:09:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:09:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:10:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:10:59 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-04T01:11:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:12:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:12:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/10000, val/hateful_memes/cross_entropy: 2.3711, val/total_loss: 2.3711, val/hateful_memes/accuracy: 0.6440, val/hateful_memes/binary_f1: 0.5505, val/hateful_memes/roc_auc: 0.6976, num_updates: 3000, epoch: 11, iterations: 3000, max_updates: 10000, val_time: 02m 34s 243ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.697580\n",
      "\u001b[32m2021-05-04T01:15:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/10000, train/hateful_memes/cross_entropy: 0.0745, train/hateful_memes/cross_entropy/avg: 0.1927, train/total_loss: 0.0745, train/total_loss/avg: 0.1927, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 3100, iterations: 3100, max_updates: 10000, lr: 0.00001, ups: 0.58, time: 02m 53s 437ms, time_since_start: 01h 16m 41s 939ms, eta: 03h 22m 38s 688ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:16:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:16:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:16:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/10000, train/hateful_memes/cross_entropy: 0.0718, train/hateful_memes/cross_entropy/avg: 0.1868, train/total_loss: 0.0718, train/total_loss/avg: 0.1868, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3200, iterations: 3200, max_updates: 10000, lr: 0.00001, ups: 0.96, time: 01m 44s 269ms, time_since_start: 01h 18m 26s 208ms, eta: 02h 03s 758ms\n",
      "\u001b[32m2021-05-04T01:18:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/10000, train/hateful_memes/cross_entropy: 0.0540, train/hateful_memes/cross_entropy/avg: 0.1813, train/total_loss: 0.0540, train/total_loss/avg: 0.1813, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3300, iterations: 3300, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 387ms, time_since_start: 01h 20m 09s 595ms, eta: 01h 57m 17s 778ms\n",
      "\u001b[32m2021-05-04T01:20:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/10000, train/hateful_memes/cross_entropy: 0.0488, train/hateful_memes/cross_entropy/avg: 0.1766, train/total_loss: 0.0488, train/total_loss/avg: 0.1766, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3400, iterations: 3400, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 812ms, time_since_start: 01h 21m 53s 408ms, eta: 01h 56m 01s 244ms\n",
      "\u001b[32m2021-05-04T01:22:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/10000, train/hateful_memes/cross_entropy: 0.0442, train/hateful_memes/cross_entropy/avg: 0.1716, train/total_loss: 0.0442, train/total_loss/avg: 0.1716, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3500, iterations: 3500, max_updates: 10000, lr: 0.00001, ups: 0.95, time: 01m 45s 691ms, time_since_start: 01h 23m 39s 100ms, eta: 01h 56m 19s 893ms\n",
      "\u001b[32m2021-05-04T01:23:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/10000, train/hateful_memes/cross_entropy: 0.0242, train/hateful_memes/cross_entropy/avg: 0.1673, train/total_loss: 0.0242, train/total_loss/avg: 0.1673, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3600, iterations: 3600, max_updates: 10000, lr: 0.00001, ups: 0.96, time: 01m 44s 461ms, time_since_start: 01h 25m 23s 562ms, eta: 01h 53m 12s 532ms\n",
      "\u001b[32m2021-05-04T01:25:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/10000, train/hateful_memes/cross_entropy: 0.0191, train/hateful_memes/cross_entropy/avg: 0.1629, train/total_loss: 0.0191, train/total_loss/avg: 0.1629, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3700, iterations: 3700, max_updates: 10000, lr: 0.00001, ups: 0.96, time: 01m 44s 222ms, time_since_start: 01h 27m 07s 784ms, eta: 01h 51m 11s 082ms\n",
      "\u001b[32m2021-05-04T01:27:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/10000, train/hateful_memes/cross_entropy: 0.0169, train/hateful_memes/cross_entropy/avg: 0.1587, train/total_loss: 0.0169, train/total_loss/avg: 0.1587, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3800, iterations: 3800, max_updates: 10000, lr: 0.00001, ups: 0.96, time: 01m 44s 546ms, time_since_start: 01h 28m 52s 330ms, eta: 01h 49m 45s 566ms\n",
      "\u001b[32m2021-05-04T01:28:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/10000, train/hateful_memes/cross_entropy: 0.0169, train/hateful_memes/cross_entropy/avg: 0.1560, train/total_loss: 0.0169, train/total_loss/avg: 0.1560, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3900, iterations: 3900, max_updates: 10000, lr: 0.00001, ups: 1.02, time: 01m 38s 189ms, time_since_start: 01h 30m 30s 520ms, eta: 01h 41m 25s 414ms\n",
      "\u001b[32m2021-05-04T01:30:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T01:30:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:30:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:31:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:31:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/10000, train/hateful_memes/cross_entropy: 0.0157, train/hateful_memes/cross_entropy/avg: 0.1522, train/total_loss: 0.0157, train/total_loss/avg: 0.1522, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 4000, iterations: 4000, max_updates: 10000, lr: 0.00001, ups: 0.62, time: 02m 40s 677ms, time_since_start: 01h 33m 11s 198ms, eta: 02h 43m 14s 906ms\n",
      "\u001b[32m2021-05-04T01:31:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T01:31:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:31:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:31:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:32:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:32:40 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-04T01:33:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:33:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:33:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/10000, val/hateful_memes/cross_entropy: 2.2471, val/total_loss: 2.2471, val/hateful_memes/accuracy: 0.6600, val/hateful_memes/binary_f1: 0.5833, val/hateful_memes/roc_auc: 0.7138, num_updates: 4000, epoch: 14, iterations: 4000, max_updates: 10000, val_time: 02m 17s 660ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.713755\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:36:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:36:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:36:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/10000, train/hateful_memes/cross_entropy: 0.0115, train/hateful_memes/cross_entropy/avg: 0.1486, train/total_loss: 0.0115, train/total_loss/avg: 0.1486, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4100, iterations: 4100, max_updates: 10000, lr: 0.00001, ups: 0.54, time: 03m 05s 025ms, time_since_start: 01h 38m 33s 888ms, eta: 03h 04m 51s 195ms\n",
      "\u001b[32m2021-05-04T01:38:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/10000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.1452, train/total_loss: 0.0103, train/total_loss/avg: 0.1452, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4200, iterations: 4200, max_updates: 10000, lr: 0.00001, ups: 1.03, time: 01m 37s 384ms, time_since_start: 01h 40m 11s 272ms, eta: 01h 35m 38s 682ms\n",
      "\u001b[32m2021-05-04T01:40:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/10000, train/hateful_memes/cross_entropy: 0.0064, train/hateful_memes/cross_entropy/avg: 0.1419, train/total_loss: 0.0064, train/total_loss/avg: 0.1419, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4300, iterations: 4300, max_updates: 10000, lr: 0.00001, ups: 1.03, time: 01m 37s 420ms, time_since_start: 01h 41m 48s 693ms, eta: 01h 34m 01s 808ms\n",
      "\u001b[32m2021-05-04T01:41:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/10000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.1387, train/total_loss: 0.0058, train/total_loss/avg: 0.1387, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4400, iterations: 4400, max_updates: 10000, lr: 0.00001, ups: 0.98, time: 01m 42s 206ms, time_since_start: 01h 43m 30s 900ms, eta: 01h 36m 55s 159ms\n",
      "\u001b[32m2021-05-04T01:43:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/10000, train/hateful_memes/cross_entropy: 0.0058, train/hateful_memes/cross_entropy/avg: 0.1357, train/total_loss: 0.0058, train/total_loss/avg: 0.1357, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4500, iterations: 4500, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 875ms, time_since_start: 01h 45m 14s 775ms, eta: 01h 36m 44s 564ms\n",
      "\u001b[32m2021-05-04T01:45:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/10000, train/hateful_memes/cross_entropy: 0.0053, train/hateful_memes/cross_entropy/avg: 0.1327, train/total_loss: 0.0053, train/total_loss/avg: 0.1327, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4600, iterations: 4600, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 857ms, time_since_start: 01h 46m 58s 632ms, eta: 01h 34m 58s 020ms\n",
      "\u001b[32m2021-05-04T01:47:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/10000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1300, train/total_loss: 0.0050, train/total_loss/avg: 0.1300, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4700, iterations: 4700, max_updates: 10000, lr: 0.00001, ups: 0.98, time: 01m 42s 341ms, time_since_start: 01h 48m 40s 974ms, eta: 01h 31m 50s 902ms\n",
      "\u001b[32m2021-05-04T01:48:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/10000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1275, train/total_loss: 0.0050, train/total_loss/avg: 0.1275, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4800, iterations: 4800, max_updates: 10000, lr: 0.00001, ups: 1.02, time: 01m 38s 516ms, time_since_start: 01h 50m 19s 491ms, eta: 01h 26m 44s 828ms\n",
      "\u001b[32m2021-05-04T01:50:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/10000, train/hateful_memes/cross_entropy: 0.0053, train/hateful_memes/cross_entropy/avg: 0.1251, train/total_loss: 0.0053, train/total_loss/avg: 0.1251, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4900, iterations: 4900, max_updates: 10000, lr: 0.00001, ups: 1.03, time: 01m 37s 891ms, time_since_start: 01h 51m 57s 383ms, eta: 01h 24m 32s 367ms\n",
      "\u001b[32m2021-05-04T01:52:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T01:52:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:52:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:53:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:53:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/10000, train/hateful_memes/cross_entropy: 0.0053, train/hateful_memes/cross_entropy/avg: 0.1237, train/total_loss: 0.0053, train/total_loss/avg: 0.1237, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5000, iterations: 5000, max_updates: 10000, lr: 0.00001, ups: 0.60, time: 02m 46s 765ms, time_since_start: 01h 54m 44s 148ms, eta: 02h 21m 11s 679ms\n",
      "\u001b[32m2021-05-04T01:53:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T01:53:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:53:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:53:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:53:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:54:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:54:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:54:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/10000, val/hateful_memes/cross_entropy: 2.3442, val/total_loss: 2.3442, val/hateful_memes/accuracy: 0.6480, val/hateful_memes/binary_f1: 0.5417, val/hateful_memes/roc_auc: 0.7132, num_updates: 5000, epoch: 18, iterations: 5000, max_updates: 10000, val_time: 01m 37s 095ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.713755\n",
      "\u001b[32m2021-05-04T01:57:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/10000, train/hateful_memes/cross_entropy: 0.0053, train/hateful_memes/cross_entropy/avg: 0.1215, train/total_loss: 0.0053, train/total_loss/avg: 0.1215, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5100, iterations: 5100, max_updates: 10000, lr: 0.00001, ups: 0.67, time: 02m 29s 597ms, time_since_start: 01h 58m 50s 845ms, eta: 02h 04m 07s 582ms\n",
      "\u001b[32m2021-05-04T01:58:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/10000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1192, train/total_loss: 0.0050, train/total_loss/avg: 0.1192, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5200, iterations: 5200, max_updates: 10000, lr: 0.00001, ups: 1.04, time: 01m 36s 536ms, time_since_start: 02h 27s 382ms, eta: 01h 18m 27s 891ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:58:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:58:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:00:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/10000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1173, train/total_loss: 0.0050, train/total_loss/avg: 0.1173, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 5300, iterations: 5300, max_updates: 10000, lr: 0.00001, ups: 0.94, time: 01m 46s 116ms, time_since_start: 02h 02m 13s 498ms, eta: 01h 24m 27s 289ms\n",
      "\u001b[32m2021-05-04T02:02:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/10000, train/hateful_memes/cross_entropy: 0.0044, train/hateful_memes/cross_entropy/avg: 0.1152, train/total_loss: 0.0044, train/total_loss/avg: 0.1152, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 5400, iterations: 5400, max_updates: 10000, lr: 0.00001, ups: 0.97, time: 01m 43s 778ms, time_since_start: 02h 03m 57s 276ms, eta: 01h 20m 50s 175ms\n",
      "\u001b[32m2021-05-04T02:04:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/10000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1132, train/total_loss: 0.0043, train/total_loss/avg: 0.1132, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5500, iterations: 5500, max_updates: 10000, lr: 0.00001, ups: 0.93, time: 01m 47s 664ms, time_since_start: 02h 05m 44s 941ms, eta: 01h 22m 02s 413ms\n",
      "\u001b[32m2021-05-04T02:05:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/10000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1112, train/total_loss: 0.0043, train/total_loss/avg: 0.1112, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5600, iterations: 5600, max_updates: 10000, lr: 0.00001, ups: 1.04, time: 01m 36s 989ms, time_since_start: 02h 07m 21s 931ms, eta: 01h 12m 15s 836ms\n",
      "\u001b[32m2021-05-04T02:07:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/10000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1094, train/total_loss: 0.0050, train/total_loss/avg: 0.1094, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5700, iterations: 5700, max_updates: 10000, lr: 0.00001, ups: 1.03, time: 01m 37s 845ms, time_since_start: 02h 08m 59s 776ms, eta: 01h 11m 14s 676ms\n",
      "\u001b[32m2021-05-04T02:09:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/10000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1075, train/total_loss: 0.0050, train/total_loss/avg: 0.1075, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5800, iterations: 5800, max_updates: 10000, lr: 0.00001, ups: 0.98, time: 01m 42s 181ms, time_since_start: 02h 10m 41s 958ms, eta: 01h 12m 40s 274ms\n",
      "\u001b[32m2021-05-04T02:10:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/10000, train/hateful_memes/cross_entropy: 0.0050, train/hateful_memes/cross_entropy/avg: 0.1060, train/total_loss: 0.0050, train/total_loss/avg: 0.1060, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5900, iterations: 5900, max_updates: 10000, lr: 0.00001, ups: 1.03, time: 01m 37s 186ms, time_since_start: 02h 12m 19s 145ms, eta: 01h 07m 28s 417ms\n",
      "\u001b[32m2021-05-04T02:12:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T02:12:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:12:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:13:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:13:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/10000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1042, train/total_loss: 0.0029, train/total_loss/avg: 0.1042, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 6000, iterations: 6000, max_updates: 10000, lr: 0.00001, ups: 0.62, time: 02m 41s 050ms, time_since_start: 02h 15m 195ms, eta: 01h 49m 05s 095ms\n",
      "\u001b[32m2021-05-04T02:13:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T02:13:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:13:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:13:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:14:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:14:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:15:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:15:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/10000, val/hateful_memes/cross_entropy: 2.8983, val/total_loss: 2.8983, val/hateful_memes/accuracy: 0.6480, val/hateful_memes/binary_f1: 0.5217, val/hateful_memes/roc_auc: 0.7099, num_updates: 6000, epoch: 21, iterations: 6000, max_updates: 10000, val_time: 01m 47s 752ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.713755\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:17:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:17:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:17:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/10000, train/hateful_memes/cross_entropy: 0.0029, train/hateful_memes/cross_entropy/avg: 0.1026, train/total_loss: 0.0029, train/total_loss/avg: 0.1026, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6100, iterations: 6100, max_updates: 10000, lr: 0., ups: 0.65, time: 02m 34s 364ms, time_since_start: 02h 19m 22s 314ms, eta: 01h 41m 56s 523ms\n",
      "\u001b[32m2021-05-04T02:19:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/10000, train/hateful_memes/cross_entropy: 0.0040, train/hateful_memes/cross_entropy/avg: 0.1011, train/total_loss: 0.0040, train/total_loss/avg: 0.1011, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6200, iterations: 6200, max_updates: 10000, lr: 0., ups: 0.97, time: 01m 43s 061ms, time_since_start: 02h 21m 05s 375ms, eta: 01h 06m 18s 985ms\n",
      "\u001b[32m2021-05-04T02:21:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/10000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.0995, train/total_loss: 0.0026, train/total_loss/avg: 0.0995, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6300, iterations: 6300, max_updates: 10000, lr: 0., ups: 0.96, time: 01m 44s 270ms, time_since_start: 02h 22m 49s 646ms, eta: 01h 05m 19s 730ms\n",
      "\u001b[32m2021-05-04T02:22:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/10000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.0979, train/total_loss: 0.0026, train/total_loss/avg: 0.0979, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6400, iterations: 6400, max_updates: 10000, lr: 0., ups: 0.95, time: 01m 45s 097ms, time_since_start: 02h 24m 34s 743ms, eta: 01h 04m 04s 056ms\n",
      "\u001b[32m2021-05-04T02:24:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.0964, train/total_loss: 0.0028, train/total_loss/avg: 0.0964, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6500, iterations: 6500, max_updates: 10000, lr: 0., ups: 0.97, time: 01m 43s 639ms, time_since_start: 02h 26m 18s 383ms, eta: 01h 01m 25s 437ms\n",
      "\u001b[32m2021-05-04T02:26:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.0950, train/total_loss: 0.0028, train/total_loss/avg: 0.0950, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6600, iterations: 6600, max_updates: 10000, lr: 0., ups: 0.96, time: 01m 44s 245ms, time_since_start: 02h 28m 02s 629ms, eta: 01h 01s 046ms\n",
      "\u001b[32m2021-05-04T02:28:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/10000, train/hateful_memes/cross_entropy: 0.0040, train/hateful_memes/cross_entropy/avg: 0.0938, train/total_loss: 0.0040, train/total_loss/avg: 0.0938, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6700, iterations: 6700, max_updates: 10000, lr: 0., ups: 0.97, time: 01m 43s 687ms, time_since_start: 02h 29m 46s 316ms, eta: 57m 56s 424ms\n",
      "\u001b[32m2021-05-04T02:29:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/10000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.0925, train/total_loss: 0.0032, train/total_loss/avg: 0.0925, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6800, iterations: 6800, max_updates: 10000, lr: 0., ups: 0.96, time: 01m 44s 523ms, time_since_start: 02h 31m 30s 839ms, eta: 56m 38s 256ms\n",
      "\u001b[32m2021-05-04T02:31:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.0911, train/total_loss: 0.0028, train/total_loss/avg: 0.0911, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6900, iterations: 6900, max_updates: 10000, lr: 0., ups: 0.97, time: 01m 43s 963ms, time_since_start: 02h 33m 14s 802ms, eta: 54m 34s 432ms\n",
      "\u001b[32m2021-05-04T02:33:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T02:33:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:33:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:34:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:34:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/10000, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.0901, train/total_loss: 0.0028, train/total_loss/avg: 0.0901, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7000, iterations: 7000, max_updates: 10000, lr: 0., ups: 0.60, time: 02m 47s 094ms, time_since_start: 02h 36m 01s 897ms, eta: 01h 24m 53s 037ms\n",
      "\u001b[32m2021-05-04T02:34:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T02:34:24 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:34:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:34:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:35:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:35:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:36:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:36:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/10000, val/hateful_memes/cross_entropy: 3.2270, val/total_loss: 3.2270, val/hateful_memes/accuracy: 0.6120, val/hateful_memes/binary_f1: 0.4489, val/hateful_memes/roc_auc: 0.6997, num_updates: 7000, epoch: 25, iterations: 7000, max_updates: 10000, val_time: 01m 49s 691ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.713755\n",
      "\u001b[32m2021-05-04T02:39:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/10000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.0888, train/total_loss: 0.0026, train/total_loss/avg: 0.0888, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7100, iterations: 7100, max_updates: 10000, lr: 0., ups: 0.53, time: 03m 10s 111ms, time_since_start: 02h 41m 01s 703ms, eta: 01h 33m 21s 451ms\n",
      "\u001b[32m2021-05-04T02:41:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/10000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.0876, train/total_loss: 0.0026, train/total_loss/avg: 0.0876, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7200, iterations: 7200, max_updates: 10000, lr: 0., ups: 1.03, time: 01m 37s 564ms, time_since_start: 02h 42m 39s 268ms, eta: 46m 15s 526ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:41:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:41:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:42:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/10000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.0865, train/total_loss: 0.0026, train/total_loss/avg: 0.0865, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7300, iterations: 7300, max_updates: 10000, lr: 0., ups: 0.99, time: 01m 41s 496ms, time_since_start: 02h 44m 20s 764ms, eta: 46m 24s 250ms\n",
      "\u001b[32m2021-05-04T02:44:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/10000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.0854, train/total_loss: 0.0023, train/total_loss/avg: 0.0854, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7400, iterations: 7400, max_updates: 10000, lr: 0., ups: 1.03, time: 01m 37s 553ms, time_since_start: 02h 45m 58s 318ms, eta: 42m 56s 984ms\n",
      "\u001b[32m2021-05-04T02:45:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/10000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.0842, train/total_loss: 0.0023, train/total_loss/avg: 0.0842, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7500, iterations: 7500, max_updates: 10000, lr: 0., ups: 1.02, time: 01m 38s 479ms, time_since_start: 02h 47m 36s 798ms, eta: 41m 41s 374ms\n",
      "\u001b[32m2021-05-04T02:47:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/10000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0831, train/total_loss: 0.0013, train/total_loss/avg: 0.0831, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7600, iterations: 7600, max_updates: 10000, lr: 0., ups: 0.99, time: 01m 41s 926ms, time_since_start: 02h 49m 18s 724ms, eta: 41m 25s 377ms\n",
      "\u001b[32m2021-05-04T02:49:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/10000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.0821, train/total_loss: 0.0011, train/total_loss/avg: 0.0821, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7700, iterations: 7700, max_updates: 10000, lr: 0., ups: 1.02, time: 01m 38s 081ms, time_since_start: 02h 50m 56s 806ms, eta: 38m 11s 974ms\n",
      "\u001b[32m2021-05-04T02:50:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/10000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.0811, train/total_loss: 0.0011, train/total_loss/avg: 0.0811, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7800, iterations: 7800, max_updates: 10000, lr: 0., ups: 1.02, time: 01m 38s 812ms, time_since_start: 02h 52m 35s 619ms, eta: 36m 48s 660ms\n",
      "\u001b[32m2021-05-04T02:52:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/10000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0801, train/total_loss: 0.0007, train/total_loss/avg: 0.0801, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7900, iterations: 7900, max_updates: 10000, lr: 0., ups: 0.98, time: 01m 42s 098ms, time_since_start: 02h 54m 17s 717ms, eta: 36m 18s 364ms\n",
      "\u001b[32m2021-05-04T02:54:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T02:54:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:54:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:55:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:55:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/10000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0791, train/total_loss: 0.0007, train/total_loss/avg: 0.0791, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 8000, iterations: 8000, max_updates: 10000, lr: 0., ups: 0.62, time: 02m 41s 652ms, time_since_start: 02h 56m 59s 369ms, eta: 54m 44s 775ms\n",
      "\u001b[32m2021-05-04T02:55:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T02:55:22 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:55:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:55:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:56:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:56:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:57:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:57:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/10000, val/hateful_memes/cross_entropy: 2.9997, val/total_loss: 2.9997, val/hateful_memes/accuracy: 0.6340, val/hateful_memes/binary_f1: 0.5197, val/hateful_memes/roc_auc: 0.7028, num_updates: 8000, epoch: 28, iterations: 8000, max_updates: 10000, val_time: 02m 02s 489ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.713755\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:59:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:59:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:00:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/10000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0781, train/total_loss: 0.0005, train/total_loss/avg: 0.0781, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8100, iterations: 8100, max_updates: 10000, lr: 0., ups: 0.64, time: 02m 36s 310ms, time_since_start: 03h 01m 38s 175ms, eta: 50m 17s 421ms\n",
      "\u001b[32m2021-05-04T03:01:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/10000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0772, train/total_loss: 0.0005, train/total_loss/avg: 0.0772, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8200, iterations: 8200, max_updates: 10000, lr: 0., ups: 1.02, time: 01m 38s 996ms, time_since_start: 03h 03m 17s 172ms, eta: 30m 10s 447ms\n",
      "\u001b[32m2021-05-04T03:03:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0763, train/total_loss: 0.0006, train/total_loss/avg: 0.0763, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8300, iterations: 8300, max_updates: 10000, lr: 0., ups: 1.01, time: 01m 39s 960ms, time_since_start: 03h 04m 57s 133ms, eta: 28m 46s 522ms\n",
      "\u001b[32m2021-05-04T03:05:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/10000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0754, train/total_loss: 0.0007, train/total_loss/avg: 0.0754, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8400, iterations: 8400, max_updates: 10000, lr: 0., ups: 0.96, time: 01m 44s 727ms, time_since_start: 03h 06m 41s 860ms, eta: 28m 22s 452ms\n",
      "\u001b[32m2021-05-04T03:06:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0745, train/total_loss: 0.0006, train/total_loss/avg: 0.0745, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8500, iterations: 8500, max_updates: 10000, lr: 0., ups: 1.02, time: 01m 38s 991ms, time_since_start: 03h 08m 20s 852ms, eta: 25m 08s 632ms\n",
      "\u001b[32m2021-05-04T03:08:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0736, train/total_loss: 0.0006, train/total_loss/avg: 0.0736, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8600, iterations: 8600, max_updates: 10000, lr: 0., ups: 1.00, time: 01m 40s 169ms, time_since_start: 03h 10m 01s 022ms, eta: 23m 44s 816ms\n",
      "\u001b[32m2021-05-04T03:10:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0728, train/total_loss: 0.0006, train/total_loss/avg: 0.0728, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8700, iterations: 8700, max_updates: 10000, lr: 0., ups: 0.97, time: 01m 43s 210ms, time_since_start: 03h 11m 44s 232ms, eta: 22m 43s 201ms\n",
      "\u001b[32m2021-05-04T03:11:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0723, train/total_loss: 0.0006, train/total_loss/avg: 0.0723, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8800, iterations: 8800, max_updates: 10000, lr: 0., ups: 1.02, time: 01m 38s 002ms, time_since_start: 03h 13m 22s 235ms, eta: 19m 54s 848ms\n",
      "\u001b[32m2021-05-04T03:13:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/10000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0715, train/total_loss: 0.0006, train/total_loss/avg: 0.0715, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8900, iterations: 8900, max_updates: 10000, lr: 0., ups: 1.03, time: 01m 37s 662ms, time_since_start: 03h 14m 59s 898ms, eta: 18m 11s 479ms\n",
      "\u001b[32m2021-05-04T03:15:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T03:15:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:15:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:16:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:16:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/10000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0707, train/total_loss: 0.0005, train/total_loss/avg: 0.0707, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9000, iterations: 9000, max_updates: 10000, lr: 0., ups: 0.61, time: 02m 44s 843ms, time_since_start: 03h 17m 44s 741ms, eta: 27m 54s 812ms\n",
      "\u001b[32m2021-05-04T03:16:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T03:16:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:16:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:16:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:16:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/10000, val/hateful_memes/cross_entropy: 3.2785, val/total_loss: 3.2785, val/hateful_memes/accuracy: 0.6360, val/hateful_memes/binary_f1: 0.5134, val/hateful_memes/roc_auc: 0.7002, num_updates: 9000, epoch: 32, iterations: 9000, max_updates: 10000, val_time: 47s 653ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.713755\n",
      "\u001b[32m2021-05-04T03:18:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/10000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0699, train/total_loss: 0.0005, train/total_loss/avg: 0.0699, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9100, iterations: 9100, max_updates: 10000, lr: 0., ups: 0.85, time: 01m 57s 058ms, time_since_start: 03h 20m 29s 457ms, eta: 17m 50s 383ms\n",
      "\u001b[32m2021-05-04T03:20:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/10000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0692, train/total_loss: 0.0005, train/total_loss/avg: 0.0692, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9200, iterations: 9200, max_updates: 10000, lr: 0., ups: 1.02, time: 01m 38s 022ms, time_since_start: 03h 22m 07s 480ms, eta: 13m 16s 726ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:21:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:21:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:22:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/10000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0685, train/total_loss: 0.0005, train/total_loss/avg: 0.0685, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9300, iterations: 9300, max_updates: 10000, lr: 0., ups: 0.99, time: 01m 41s 869ms, time_since_start: 03h 23m 49s 349ms, eta: 12m 04s 497ms\n",
      "\u001b[32m2021-05-04T03:23:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/10000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0677, train/total_loss: 0.0005, train/total_loss/avg: 0.0677, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9400, iterations: 9400, max_updates: 10000, lr: 0., ups: 1.00, time: 01m 40s 806ms, time_since_start: 03h 25m 30s 156ms, eta: 10m 14s 518ms\n",
      "\u001b[32m2021-05-04T03:25:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0670, train/total_loss: 0.0004, train/total_loss/avg: 0.0670, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9500, iterations: 9500, max_updates: 10000, lr: 0., ups: 0.96, time: 01m 44s 407ms, time_since_start: 03h 27m 14s 564ms, eta: 08m 50s 390ms\n",
      "\u001b[32m2021-05-04T03:27:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0663, train/total_loss: 0.0004, train/total_loss/avg: 0.0663, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9600, iterations: 9600, max_updates: 10000, lr: 0., ups: 0.97, time: 01m 43s 312ms, time_since_start: 03h 28m 57s 876ms, eta: 06m 59s 862ms\n",
      "\u001b[32m2021-05-04T03:28:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/10000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0656, train/total_loss: 0.0004, train/total_loss/avg: 0.0656, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9700, iterations: 9700, max_updates: 10000, lr: 0., ups: 1.02, time: 01m 38s 831ms, time_since_start: 03h 30m 36s 708ms, eta: 05m 01s 237ms\n",
      "\u001b[32m2021-05-04T03:30:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0650, train/total_loss: 0.0003, train/total_loss/avg: 0.0650, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9800, iterations: 9800, max_updates: 10000, lr: 0., ups: 1.02, time: 01m 38s 138ms, time_since_start: 03h 32m 14s 846ms, eta: 03m 19s 416ms\n",
      "\u001b[32m2021-05-04T03:32:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0643, train/total_loss: 0.0003, train/total_loss/avg: 0.0643, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9900, iterations: 9900, max_updates: 10000, lr: 0., ups: 0.99, time: 01m 41s 918ms, time_since_start: 03h 33m 56s 764ms, eta: 01m 43s 548ms\n",
      "\u001b[32m2021-05-04T03:34:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T03:34:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:34:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:35:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:35:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/10000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0637, train/total_loss: 0.0003, train/total_loss/avg: 0.0637, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 10000, iterations: 10000, max_updates: 10000, lr: 0., ups: 0.61, time: 02m 44s 224ms, time_since_start: 03h 36m 40s 989ms, eta: 0ms\n",
      "\u001b[32m2021-05-04T03:35:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T03:35:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:35:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:35:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:35:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/10000, val/hateful_memes/cross_entropy: 3.3316, val/total_loss: 3.3316, val/hateful_memes/accuracy: 0.6400, val/hateful_memes/binary_f1: 0.5187, val/hateful_memes/roc_auc: 0.7013, num_updates: 10000, epoch: 35, iterations: 10000, max_updates: 10000, val_time: 44s 536ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.713755\n",
      "\u001b[32m2021-05-04T03:35:49 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2021-05-04T03:35:49 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2021-05-04T03:35:49 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2021-05-04T03:36:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-04T03:36:39 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 4000\n",
      "\u001b[32m2021-05-04T03:36:39 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 4000\n",
      "\u001b[32m2021-05-04T03:36:39 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 14\n",
      "\u001b[32m2021-05-04T03:36:46 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-04T03:36:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:36:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:36:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "100% 16/16 [00:17<00:00,  1.10s/it]\n",
      "\u001b[32m2021-05-04T03:37:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/10000, val/hateful_memes/cross_entropy: 2.2471, val/total_loss: 2.2471, val/hateful_memes/accuracy: 0.6600, val/hateful_memes/binary_f1: 0.5833, val/hateful_memes/roc_auc: 0.7138\n",
      "\u001b[32m2021-05-04T03:37:03 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 03h 38m 41s 163ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/vilbert/defaults.yaml \\\n",
    "  model=vilbert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=train_val \\\n",
    "  training.max_updates=10000 \\\n",
    "  training.batch_size=32 \\\n",
    "  env.save_dir=/content/gdrive/MyDrive/colab/finetuned_vilbert_election_memes/ \\\n",
    "  checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct \\\n",
    "  checkpoint.resume_pretrained=True \\\n",
    "  dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO49snAPRxX1xWsTAkXMufK",
   "collapsed_sections": [],
   "name": "FineTuned ViLBERT + Visual BERT (Hateful and Election Memes).ipynb",
   "provenance": [
    {
     "file_id": "1rvPLI3JN03BX7roziY0_n0-nn0WoLSfq",
     "timestamp": 1620084280821
    },
    {
     "file_id": "1ZZHsoTlNErUrhFCbmS6pTz7GLvHYw_Wp",
     "timestamp": 1620011798791
    },
    {
     "file_id": "1zJ_PgMxUEzdqlZoJPeoOv0T0SYkbYsM7",
     "timestamp": 1620005734396
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
