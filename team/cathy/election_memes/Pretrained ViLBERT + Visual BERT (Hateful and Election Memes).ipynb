{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20591,
     "status": "ok",
     "timestamp": 1620078794897,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "jSPOfKQom8AF",
    "outputId": "77ea8aa9-9dd5-43a3-9353-b9acabe0b386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8844,
     "status": "ok",
     "timestamp": 1620078794897,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "SlKoJSAknjuq",
    "outputId": "c40df349-96c7-4ad4-9660-76ff23eed565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/colab\n"
     ]
    }
   ],
   "source": [
    "%cd gdrive/MyDrive/colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9685,
     "status": "ok",
     "timestamp": 1620078796155,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "o-zreWCXnpGw",
    "outputId": "ea03738a-6df9-4eea-c9bd-2f4bebef168b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf\n"
     ]
    }
   ],
   "source": [
    "%cd mmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 242763,
     "status": "ok",
     "timestamp": 1620079038924,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "6hM3BET1nrBO",
    "outputId": "05454393-c589-43b7-9e60-0e52de71051f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pytorch-lightning==1.2.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/13/fb401b8f9d9c5e2aa08769d230bb401bf11dee0bc93e069d7337a4201ec8/pytorch_lightning-1.2.7-py3-none-any.whl (830kB)\n",
      "\u001b[K     |████████████████████████████████| 839kB 16.0MB/s \n",
      "\u001b[?25hCollecting transformers==3.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 33.6MB/s \n",
      "\u001b[?25hCollecting omegaconf==2.0.6\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
      "Collecting ftfy==5.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 12.2MB/s \n",
      "\u001b[?25hCollecting GitPython==3.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 52.3MB/s \n",
      "\u001b[?25hCollecting nltk==3.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 58.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.0)\n",
      "Collecting fasttext==0.9.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.6MB/s \n",
      "\u001b[?25hCollecting tqdm<4.50.0,>=4.43.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
      "\u001b[?25hCollecting demjson==2.2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/67/6db789e2533158963d4af689f961b644ddd9200615b8ce92d6cad695c65a/demjson-2.2.4.tar.gz (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 61.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.19.5)\n",
      "Collecting iopath==0.1.7\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/d5/1c70fea7632640e8a9fb5a176676e555238119b3e7ee8b6dc49980ec5769/iopath-0.1.7-py3-none-any.whl\n",
      "Collecting datasets==1.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 51.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n",
      "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n",
      "Requirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.0.2)\n",
      "Collecting torchvision<=0.9.1,>=0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/8a/82062a33b5eb7f696bf23f8ccf04bf6fc81d1a4972740fb21c2569ada0a6/torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4MB)\n",
      "\u001b[K     |████████████████████████████████| 17.4MB 329kB/s \n",
      "\u001b[?25hCollecting lmdb==0.98\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/5c/d56dbc2532ecf14fa004c543927500c0f645eaca8bd7ec39420c7546396a/lmdb-0.98.tar.gz (869kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 29.7MB/s \n",
      "\u001b[?25hCollecting matplotlib==3.3.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/3d/db9a6b3c83c9511301152dbb64a029c3a4313c86eaef12c237b13ecf91d6/matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6MB 48.7MB/s \n",
      "\u001b[?25hCollecting torchtext==0.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 10.0MB/s \n",
      "\u001b[?25hCollecting torch<=1.8.1,>=1.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n",
      "\u001b[K     |████████████████████████████████| 804.1MB 23kB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.7->mmf==1.0.0rc12) (2.4.1)\n",
      "Collecting PyYAML!=5.4.*,>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 54.4MB/s \n",
      "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/99/dc59248df9a50349d537ffb3403c1bdc1fa69077109d46feaa0843488001/torchmetrics-0.3.1-py3-none-any.whl (271kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 55.7MB/s \n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 49.8MB/s \n",
      "\u001b[?25hCollecting fsspec[http]>=0.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 55.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (2019.12.20)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 46.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.0.12)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 48.3MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.9.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/e7/edf655ae34925aeaefb7b7fcc3dd0887d2a1203ee6b0df4d1170d1a19d4f/tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 44.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.12.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (20.9)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.0.6->mmf==1.0.0rc12) (3.7.4.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.5)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 12.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.4.5->mmf==1.0.0rc12) (1.15.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (0.22.2.post1)\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (2.6.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (56.0.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2020.12.5)\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (1.1.5)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.3.3)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.70.11.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.10.1)\n",
      "Collecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 58.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (0.29.22)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<=0.9.1,>=0.7.0->mmf==1.0.0rc12) (7.1.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.12.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.32.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.28.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.0.1)\n",
      "Collecting aiohttp; extra == \"http\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 47.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (1.0.1)\n",
      "Collecting smmap<5,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1->mmf==1.0.0rc12) (2018.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.2.1->mmf==1.0.0rc12) (3.4.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (4.2.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.2.8)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 57.4MB/s \n",
      "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting multidict<7.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 60.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (20.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.4.8)\n",
      "Building wheels for collected packages: ftfy, nltk, fasttext, demjson, lmdb, PyYAML, future\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-5.8-cp37-none-any.whl size=45613 sha256=f70f7d66646baa483b2ab7570b850f13bfabb32f928a91620b39d8d5b73a19a4\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449905 sha256=e2a312c8ca73281cbca242d70422a2ce10dc3737592fdd4ac5d59390cf8a5466\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2463266 sha256=177a63956fe5b79e1cc7906da9072bc907aa352dab9345a3a5eb6b00f7e08570\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
      "  Building wheel for demjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for demjson: filename=demjson-2.2.4-cp37-none-any.whl size=73546 sha256=67affc1fcebab7f5cad2c53bdb8df385cc3d853721d6d504adbfec1b431c11e7\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/d2/ab/a54fb5ea53ac3badba098160e8452fa126a51febda80440ded\n",
      "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=219683 sha256=b7bac6d0e5e98483df32dc088f28100b92b23fab33679ca541264c820f144a46\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/97/8c/7721e4b6b0ac723c6cc45ecca60599a80f75e2367330647390\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=c64c127b4b5aae442b55753c1ffb6dd803cc6340937a484c439ecb509bee76f5\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=009b3d6356736a196c4b31669bcf731b261aa55e39a9e5386c5dff2ee0a0c8a3\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built ftfy nltk fasttext demjson lmdb PyYAML future\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: PyYAML, torch, torchmetrics, tqdm, future, multidict, yarl, async-timeout, aiohttp, fsspec, pytorch-lightning, sentencepiece, sacremoses, tokenizers, transformers, omegaconf, ftfy, smmap, gitdb, GitPython, nltk, fasttext, demjson, portalocker, iopath, xxhash, datasets, torchvision, lmdb, matplotlib, torchtext, mmf\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Found existing installation: torch 1.8.1+cu101\n",
      "    Uninstalling torch-1.8.1+cu101:\n",
      "      Successfully uninstalled torch-1.8.1+cu101\n",
      "  Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "  Found existing installation: torchvision 0.9.1+cu101\n",
      "    Uninstalling torchvision-0.9.1+cu101:\n",
      "      Successfully uninstalled torchvision-0.9.1+cu101\n",
      "  Found existing installation: lmdb 0.99\n",
      "    Uninstalling lmdb-0.99:\n",
      "      Successfully uninstalled lmdb-0.99\n",
      "  Found existing installation: matplotlib 3.2.2\n",
      "    Uninstalling matplotlib-3.2.2:\n",
      "      Successfully uninstalled matplotlib-3.2.2\n",
      "  Found existing installation: torchtext 0.9.1\n",
      "    Uninstalling torchtext-0.9.1:\n",
      "      Successfully uninstalled torchtext-0.9.1\n",
      "  Running setup.py develop for mmf\n",
      "Successfully installed GitPython-3.1.0 PyYAML-5.3.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.2.1 demjson-2.2.4 fasttext-0.9.1 fsspec-2021.4.0 ftfy-5.8 future-0.18.2 gitdb-4.0.7 iopath-0.1.7 lmdb-0.98 matplotlib-3.3.4 mmf multidict-5.1.0 nltk-3.4.5 omegaconf-2.0.6 portalocker-2.3.0 pytorch-lightning-1.2.7 sacremoses-0.0.45 sentencepiece-0.1.95 smmap-4.0.0 tokenizers-0.9.2 torch-1.8.1 torchmetrics-0.3.1 torchtext-0.5.0 torchvision-0.9.1 tqdm-4.49.0 transformers-3.4.0 xxhash-2.0.2 yarl-1.6.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --editable ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245501,
     "status": "ok",
     "timestamp": 1620079043064,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "x4PQkXOxnsOA",
    "outputId": "53f5e0ae-ef7d-4839-c242-5afbc43afce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyYAML==5.3.1 in /usr/local/lib/python3.7/dist-packages (5.3.1)\n",
      "Collecting imgaug==0.2.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
      "\r",
      "\u001b[K     |▌                               | 10kB 22.4MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 20kB 16.0MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 30kB 14.5MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 40kB 13.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 51kB 12.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 61kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 71kB 11.6MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 81kB 12.7MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 92kB 11.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 102kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 112kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 122kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 133kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 143kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 153kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 163kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 174kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 184kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 194kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 204kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 215kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 225kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 235kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 245kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 256kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 266kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 276kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 286kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 296kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 307kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 317kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 327kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 337kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 348kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 358kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 368kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 378kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 389kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 399kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 409kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 419kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 430kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 440kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 450kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 460kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 471kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 481kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 491kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 501kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 512kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 522kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 532kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 542kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 552kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 563kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 573kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 583kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 593kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 604kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 614kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 624kB 11.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 634kB 11.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.4.1)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (0.16.2)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.15.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.5.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.4.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (7.1.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (3.3.4)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.6) (4.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (1.3.1)\n",
      "Building wheels for collected packages: imgaug\n",
      "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for imgaug: filename=imgaug-0.2.6-cp37-none-any.whl size=654019 sha256=3436f772d7a5b74583a5b22bcdc65f1a3c7d18636a6bad20afab0cee8e5df51e\n",
      "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
      "Successfully built imgaug\n",
      "Installing collected packages: imgaug\n",
      "  Found existing installation: imgaug 0.2.9\n",
      "    Uninstalling imgaug-0.2.9:\n",
      "      Successfully uninstalled imgaug-0.2.9\n",
      "Successfully installed imgaug-0.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install PyYAML==5.3.1 imgaug==0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243147,
     "status": "ok",
     "timestamp": 1620079043065,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "TSrAzqf9nvUO",
    "outputId": "06a58fba-4adf-4130-f1c7-08d4f6b555ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/colab\n"
     ]
    }
   ],
   "source": [
    "%cd /content/gdrive/MyDrive/colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 512693,
     "status": "ok",
     "timestamp": 1620079312899,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "4qRna4BrnxaZ",
    "outputId": "f53e3e70-2477-40f6-dd3e-b261f6e61757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-03 21:57:40.075545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Data folder is /root/.cache/torch/mmf/data\n",
      "Zip path is ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Starting checksum for XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Checksum successful\n",
      "Copying ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Unzipping ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Extracting the zip can take time. Sit back and relax.\n",
      "Moving train.jsonl\n",
      "Moving dev_seen.jsonl\n",
      "Moving test_seen.jsonl\n",
      "Moving dev_unseen.jsonl\n",
      "Moving test_unseen.jsonl\n",
      "Moving img\n"
     ]
    }
   ],
   "source": [
    "!mmf_convert_hm --zip_file=\"./XjiOc5ycDBRRNwbhRlgH.zip\" --password=REDACTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1427899,
     "status": "ok",
     "timestamp": 1620013798203,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "UlF9T15Wt92g",
    "outputId": "37949494-1ea1-4042-c474-31611f86c5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-03 03:26:16.388369: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-03T03:27:28 | matplotlib.font_manager: \u001b[0mGenerating new fontManager, this may take some time...\n",
      "\u001b[32m2021-05-03T03:28:04 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/defaults.yaml\n",
      "\u001b[32m2021-05-03T03:28:04 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
      "\u001b[32m2021-05-03T03:28:04 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-03T03:28:04 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2021-05-03T03:28:04 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.direct\n",
      "\u001b[32m2021-05-03T03:28:04 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
      "\u001b[32m2021-05-03T03:28:05 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2021-05-03T03:28:05 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct', 'checkpoint.resume_pretrained=False'])\n",
      "\u001b[32m2021-05-03T03:28:05 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-03T03:28:05 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-03T03:28:05 | mmf_cli.run: \u001b[0mUsing seed 4716965\n",
      "\u001b[32m2021-05-03T03:28:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100% 10.3G/10.3G [13:17<00:00, 12.9MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n",
      "Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 168kB/s] \n",
      "[ Starting checksum for extras.tar.gz]\n",
      "[ Checksum successful for extras.tar.gz]\n",
      "Unpacking extras.tar.gz\n",
      "\u001b[32m2021-05-03T03:47:19 | filelock: \u001b[0mLock 139626297186832 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "Downloading: 100% 433/433 [00:00<00:00, 375kB/s]\n",
      "\u001b[32m2021-05-03T03:47:20 | filelock: \u001b[0mLock 139626297186832 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "\u001b[32m2021-05-03T03:47:21 | filelock: \u001b[0mLock 139626326232400 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 318kB/s]\n",
      "\u001b[32m2021-05-03T03:47:22 | filelock: \u001b[0mLock 139626326232400 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:47:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:47:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T03:47:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T03:47:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T03:47:22 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T03:47:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2021-05-03T03:47:23 | filelock: \u001b[0mLock 139626290919952 acquired on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Downloading: 100% 440M/440M [00:06<00:00, 67.7MB/s]\n",
      "\u001b[32m2021-05-03T03:47:29 | filelock: \u001b[0mLock 139626290919952 released on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-03T03:47:39 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-03T03:47:39 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:47:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:47:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-03T03:47:40 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_direct.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.finetuned.hateful_memes.direct/vilbert.finetuned.hateful_memes_direct.tar.gz ]\n",
      "Downloading vilbert.finetuned.hateful_memes_direct.tar.gz: 100% 918M/918M [01:15<00:00, 12.1MB/s]\n",
      "[ Starting checksum for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
      "[ Checksum successful for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
      "Unpacking vilbert.finetuned.hateful_memes_direct.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:49:10 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:49:10 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:49:10 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:49:10 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:49:11 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
      "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
      "Unexpected keys if any: []\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:49:12 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:49:12 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:49:12 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:49:12 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2021-05-03T03:49:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-03T03:49:12 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-03T03:49:12 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-03T03:49:12 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-03T03:49:12 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-03T03:49:12 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
      "  (model): ViLBERTForClassification(\n",
      "    (bert): ViLBERTBase(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (v_embeddings): BertImageFeatureEmbeddings(\n",
      "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (v_layer): ModuleList(\n",
      "          (0): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (c_layer): ModuleList(\n",
      "          (0): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (t_pooler): BertTextPooler(\n",
      "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (v_pooler): BertImagePooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-03T03:49:12 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
      "\u001b[32m2021-05-03T03:49:12 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-03T03:49:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100% 17/17 [00:43<00:00,  2.56s/it]\n",
      "\u001b[32m2021-05-03T03:49:55 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 2.2624, val/total_loss: 2.2624, val/hateful_memes/accuracy: 0.6778, val/hateful_memes/binary_f1: 0.4000, val/hateful_memes/roc_auc: 0.6566\n",
      "\u001b[32m2021-05-03T03:49:55 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 02m 15s 427ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/vilbert/defaults.yaml \\\n",
    "  model=vilbert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=val \\\n",
    "  checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct \\\n",
    "  checkpoint.resume_pretrained=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwAfY1esqz6N"
   },
   "source": [
    "Using pretrained ViLBERT on unimodal VQA2. Fine-tuning on all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19597506,
     "status": "ok",
     "timestamp": 1620042762962,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "j2Lhcy3fn7M0",
    "outputId": "c259c74a-2d66-4e87-9cb9-5cc3a64afacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-03 03:50:03.250607: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/defaults.yaml\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/gdrive/MyDrive/colab/pretrained_vilbert_election_memes/\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.pretrained.vqa2\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to /content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_seen.jsonl\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.train[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.val[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.test[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf: \u001b[0mLogging to: /content/gdrive/MyDrive/colab/pretrained_vilbert_election_memes/train.log\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=train_val', 'training.batch_size=32', 'env.save_dir=/content/gdrive/MyDrive/colab/pretrained_vilbert_election_memes/', 'checkpoint.resume_zoo=vilbert.pretrained.vqa2', 'checkpoint.resume_pretrained=True', 'dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl', 'dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb'])\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf_cli.run: \u001b[0mUsing seed 9341216\n",
      "\u001b[32m2021-05-03T03:50:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:50:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:50:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T03:50:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T03:50:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T03:50:13 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T03:50:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-03T03:50:26 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-03T03:50:26 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:50:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:50:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-03T03:50:26 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.pretrained.vqa2.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.pretrained.vqa2/vilbert.pretrained.vqa2.tar.gz ]\n",
      "Downloading vilbert.pretrained.vqa2.tar.gz: 100% 1.01G/1.01G [01:22<00:00, 12.2MB/s]\n",
      "[ Starting checksum for vilbert.pretrained.vqa2.tar.gz]\n",
      "[ Checksum successful for vilbert.pretrained.vqa2.tar.gz]\n",
      "Unpacking vilbert.pretrained.vqa2.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:52:03 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:52:03 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:52:03 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:52:03 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:03 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
      "  (model): ViLBERTForClassification(\n",
      "    (bert): ViLBERTBase(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (v_embeddings): BertImageFeatureEmbeddings(\n",
      "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (v_layer): ModuleList(\n",
      "          (0): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (c_layer): ModuleList(\n",
      "          (0): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (t_pooler): BertTextPooler(\n",
      "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (v_pooler): BertImagePooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
      "\u001b[32m2021-05-03T03:52:04 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2021-05-03T03:57:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.6295, train/hateful_memes/cross_entropy/avg: 0.6295, train/total_loss: 0.6295, train/total_loss/avg: 0.6295, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 0.28, time: 05m 53s 053ms, time_since_start: 07m 31s 703ms, eta: 21h 49m 15s 826ms\n",
      "\u001b[32m2021-05-03T03:59:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6295, train/hateful_memes/cross_entropy/avg: 0.7118, train/total_loss: 0.6295, train/total_loss/avg: 0.7118, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 690ms, time_since_start: 09m 14s 394ms, eta: 06h 19m 04s 659ms\n",
      "\u001b[32m2021-05-03T04:01:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6446, train/hateful_memes/cross_entropy/avg: 0.6894, train/total_loss: 0.6446, train/total_loss/avg: 0.6894, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 818ms, time_since_start: 11m 213ms, eta: 06h 28m 50s 123ms\n",
      "\u001b[32m2021-05-03T04:03:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6295, train/hateful_memes/cross_entropy/avg: 0.6521, train/total_loss: 0.6295, train/total_loss/avg: 0.6521, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 111ms, time_since_start: 12m 42s 324ms, eta: 06h 13m 29s 020ms\n",
      "\u001b[32m2021-05-03T04:04:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6446, train/hateful_memes/cross_entropy/avg: 0.6661, train/total_loss: 0.6446, train/total_loss/avg: 0.6661, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 687ms, time_since_start: 14m 26s 012ms, eta: 06h 17m 29s 491ms\n",
      "\u001b[32m2021-05-03T04:06:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6295, train/hateful_memes/cross_entropy/avg: 0.6362, train/total_loss: 0.6295, train/total_loss/avg: 0.6362, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 107ms, time_since_start: 16m 12s 120ms, eta: 06h 24m 30s 368ms\n",
      "\u001b[32m2021-05-03T04:08:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6446, train/hateful_memes/cross_entropy/avg: 0.6437, train/total_loss: 0.6446, train/total_loss/avg: 0.6437, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 222ms, time_since_start: 17m 55s 342ms, eta: 06h 12m 18s 164ms\n",
      "\u001b[32m2021-05-03T04:10:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.6295, train/hateful_memes/cross_entropy/avg: 0.6385, train/total_loss: 0.6295, train/total_loss/avg: 0.6385, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 782ms, time_since_start: 19m 39s 125ms, eta: 06h 12m 33s 931ms\n",
      "\u001b[32m2021-05-03T04:11:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6295, train/hateful_memes/cross_entropy/avg: 0.6273, train/total_loss: 0.6295, train/total_loss/avg: 0.6273, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 681ms, time_since_start: 21m 24s 806ms, eta: 06h 17m 35s 569ms\n",
      "\u001b[32m2021-05-03T04:13:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T04:13:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T04:21:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T04:21:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T04:21:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.6022, train/hateful_memes/cross_entropy/avg: 0.6152, train/total_loss: 0.6022, train/total_loss/avg: 0.6152, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 0.17, time: 09m 57s 993ms, time_since_start: 31m 22s 799ms, eta: 35h 26m 27s 817ms\n",
      "\u001b[32m2021-05-03T04:21:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T04:21:49 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:21:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:21:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T04:23:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T04:23:27 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T04:24:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T04:24:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T04:24:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.8057, val/total_loss: 0.8057, val/hateful_memes/accuracy: 0.5460, val/hateful_memes/binary_f1: 0.3264, val/hateful_memes/roc_auc: 0.5896, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 02m 57s 942ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.589605\n",
      "\u001b[32m2021-05-03T04:27:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.6295, train/hateful_memes/cross_entropy/avg: 0.6176, train/total_loss: 0.6295, train/total_loss/avg: 0.6176, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 0.65, time: 02m 35s 990ms, time_since_start: 36m 56s 736ms, eta: 09h 12m 03s 640ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:28:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:28:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T04:29:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.6022, train/hateful_memes/cross_entropy/avg: 0.6006, train/total_loss: 0.6022, train/total_loss/avg: 0.6006, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 648ms, time_since_start: 38m 41s 384ms, eta: 06h 08m 35s 078ms\n",
      "\u001b[32m2021-05-03T04:30:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.6022, train/hateful_memes/cross_entropy/avg: 0.5909, train/total_loss: 0.6022, train/total_loss/avg: 0.5909, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 781ms, time_since_start: 40m 25s 165ms, eta: 06h 03m 46s 428ms\n",
      "\u001b[32m2021-05-03T04:32:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.5403, train/hateful_memes/cross_entropy/avg: 0.5767, train/total_loss: 0.5403, train/total_loss/avg: 0.5767, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 542ms, time_since_start: 42m 08s 708ms, eta: 06h 01m 11s 082ms\n",
      "\u001b[32m2021-05-03T04:34:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.5403, train/hateful_memes/cross_entropy/avg: 0.5704, train/total_loss: 0.5403, train/total_loss/avg: 0.5704, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 973ms, time_since_start: 43m 53s 682ms, eta: 06h 04m 23s 918ms\n",
      "\u001b[32m2021-05-03T04:36:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.5403, train/hateful_memes/cross_entropy/avg: 0.5688, train/total_loss: 0.5403, train/total_loss/avg: 0.5688, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 765ms, time_since_start: 45m 37s 447ms, eta: 05h 58m 26s 766ms\n",
      "\u001b[32m2021-05-03T04:37:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.5403, train/hateful_memes/cross_entropy/avg: 0.5566, train/total_loss: 0.5403, train/total_loss/avg: 0.5566, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 535ms, time_since_start: 47m 20s 983ms, eta: 05h 55m 54s 030ms\n",
      "\u001b[32m2021-05-03T04:39:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.5374, train/hateful_memes/cross_entropy/avg: 0.5461, train/total_loss: 0.5374, train/total_loss/avg: 0.5461, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 985ms, time_since_start: 49m 06s 968ms, eta: 06h 02m 31s 640ms\n",
      "\u001b[32m2021-05-03T04:41:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.5374, train/hateful_memes/cross_entropy/avg: 0.5243, train/total_loss: 0.5374, train/total_loss/avg: 0.5243, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 276ms, time_since_start: 50m 50s 245ms, eta: 05h 51m 30s 669ms\n",
      "\u001b[32m2021-05-03T04:43:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T04:43:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T04:43:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T04:44:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T04:44:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.5070, train/hateful_memes/cross_entropy/avg: 0.5194, train/total_loss: 0.5070, train/total_loss/avg: 0.5194, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 44s 806ms, time_since_start: 53m 35s 051ms, eta: 09h 18m 08s 631ms\n",
      "\u001b[32m2021-05-03T04:44:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T04:44:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:44:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:44:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T04:44:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T04:44:59 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T04:45:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T04:46:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T04:46:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.0035, val/total_loss: 1.0035, val/hateful_memes/accuracy: 0.6020, val/hateful_memes/binary_f1: 0.4665, val/hateful_memes/roc_auc: 0.6425, num_updates: 2000, epoch: 7, iterations: 2000, max_updates: 22000, val_time: 02m 10s 976ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.642525\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:46:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:46:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T04:49:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.4867, train/hateful_memes/cross_entropy/avg: 0.5072, train/total_loss: 0.4867, train/total_loss/avg: 0.5072, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 0.46, time: 03m 38s 562ms, time_since_start: 59m 24s 592ms, eta: 12h 16m 29s 945ms\n",
      "\u001b[32m2021-05-03T04:51:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.4812, train/hateful_memes/cross_entropy/avg: 0.4905, train/total_loss: 0.4812, train/total_loss/avg: 0.4905, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 1.00, time: 01m 40s 822ms, time_since_start: 01h 01m 05s 415ms, eta: 05h 38m 02s 309ms\n",
      "\u001b[32m2021-05-03T04:53:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.4750, train/hateful_memes/cross_entropy/avg: 0.4765, train/total_loss: 0.4750, train/total_loss/avg: 0.4765, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 671ms, time_since_start: 01h 02m 49s 086ms, eta: 05h 45m 50s 032ms\n",
      "\u001b[32m2021-05-03T04:55:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.4277, train/hateful_memes/cross_entropy/avg: 0.4624, train/total_loss: 0.4277, train/total_loss/avg: 0.4624, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 559ms, time_since_start: 01h 04m 34s 646ms, eta: 05h 50m 20s 729ms\n",
      "\u001b[32m2021-05-03T04:56:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.4137, train/hateful_memes/cross_entropy/avg: 0.4525, train/total_loss: 0.4137, train/total_loss/avg: 0.4525, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 888ms, time_since_start: 01h 06m 18s 535ms, eta: 05h 43m 02s 364ms\n",
      "\u001b[32m2021-05-03T04:58:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.3924, train/hateful_memes/cross_entropy/avg: 0.4389, train/total_loss: 0.3924, train/total_loss/avg: 0.4389, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 920ms, time_since_start: 01h 08m 01s 455ms, eta: 05h 38m 06s 072ms\n",
      "\u001b[32m2021-05-03T05:00:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.3671, train/hateful_memes/cross_entropy/avg: 0.4269, train/total_loss: 0.3671, train/total_loss/avg: 0.4269, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 728ms, time_since_start: 01h 09m 48s 184ms, eta: 05h 48m 48s 162ms\n",
      "\u001b[32m2021-05-03T05:01:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.3609, train/hateful_memes/cross_entropy/avg: 0.4135, train/total_loss: 0.3609, train/total_loss/avg: 0.4135, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 735ms, time_since_start: 01h 11m 31s 920ms, eta: 05h 37m 15s 957ms\n",
      "\u001b[32m2021-05-03T05:03:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.2617, train/hateful_memes/cross_entropy/avg: 0.4001, train/total_loss: 0.2617, train/total_loss/avg: 0.4001, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 874ms, time_since_start: 01h 13m 18s 794ms, eta: 05h 45m 39s 599ms\n",
      "\u001b[32m2021-05-03T05:05:28 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T05:05:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:05:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:06:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:06:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.2155, train/hateful_memes/cross_entropy/avg: 0.3874, train/total_loss: 0.2155, train/total_loss/avg: 0.3874, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 0.62, time: 02m 42s 850ms, time_since_start: 01h 16m 01s 644ms, eta: 08h 43m 56s 593ms\n",
      "\u001b[32m2021-05-03T05:06:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T05:06:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:06:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:06:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:07:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:07:25 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T05:08:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:08:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:08:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.6068, val/total_loss: 1.6068, val/hateful_memes/accuracy: 0.5880, val/hateful_memes/binary_f1: 0.4718, val/hateful_memes/roc_auc: 0.6510, num_updates: 3000, epoch: 11, iterations: 3000, max_updates: 22000, val_time: 02m 08s 741ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.650954\n",
      "\u001b[32m2021-05-03T05:11:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1722, train/hateful_memes/cross_entropy/avg: 0.3805, train/total_loss: 0.1722, train/total_loss/avg: 0.3805, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 0.51, time: 03m 15s 375ms, time_since_start: 01h 21m 25s 763ms, eta: 10h 25m 16s 776ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:13:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:13:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:13:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.1694, train/hateful_memes/cross_entropy/avg: 0.3704, train/total_loss: 0.1694, train/total_loss/avg: 0.3704, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 0.85, time: 01m 58s 048ms, time_since_start: 01h 23m 23s 812ms, eta: 06h 15m 48s 239ms\n",
      "\u001b[32m2021-05-03T05:15:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.1399, train/hateful_memes/cross_entropy/avg: 0.3593, train/total_loss: 0.1399, train/total_loss/avg: 0.3593, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 133ms, time_since_start: 01h 25m 06s 946ms, eta: 05h 26m 34s 636ms\n",
      "\u001b[32m2021-05-03T05:17:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.1372, train/hateful_memes/cross_entropy/avg: 0.3494, train/total_loss: 0.1372, train/total_loss/avg: 0.3494, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 741ms, time_since_start: 01h 26m 50s 688ms, eta: 05h 26m 44s 743ms\n",
      "\u001b[32m2021-05-03T05:19:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.1319, train/hateful_memes/cross_entropy/avg: 0.3398, train/total_loss: 0.1319, train/total_loss/avg: 0.3398, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 695ms, time_since_start: 01h 28m 36s 383ms, eta: 05h 31m 06s 470ms\n",
      "\u001b[32m2021-05-03T05:20:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.1140, train/hateful_memes/cross_entropy/avg: 0.3312, train/total_loss: 0.1140, train/total_loss/avg: 0.3312, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 150ms, time_since_start: 01h 30m 20s 534ms, eta: 05h 24m 30s 349ms\n",
      "\u001b[32m2021-05-03T05:22:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0998, train/hateful_memes/cross_entropy/avg: 0.3224, train/total_loss: 0.0998, train/total_loss/avg: 0.3224, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 027ms, time_since_start: 01h 32m 04s 562ms, eta: 05h 22m 21s 697ms\n",
      "\u001b[32m2021-05-03T05:24:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0564, train/hateful_memes/cross_entropy/avg: 0.3146, train/total_loss: 0.0564, train/total_loss/avg: 0.3146, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 926ms, time_since_start: 01h 33m 50s 488ms, eta: 05h 26m 27s 133ms\n",
      "\u001b[32m2021-05-03T05:26:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0564, train/hateful_memes/cross_entropy/avg: 0.3119, train/total_loss: 0.0564, train/total_loss/avg: 0.3119, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 777ms, time_since_start: 01h 35m 34s 266ms, eta: 05h 18m 04s 292ms\n",
      "\u001b[32m2021-05-03T05:27:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T05:27:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:28:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:28:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:28:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0520, train/hateful_memes/cross_entropy/avg: 0.3044, train/total_loss: 0.0520, train/total_loss/avg: 0.3044, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 45s 610ms, time_since_start: 01h 38m 19s 876ms, eta: 08h 24m 46s 770ms\n",
      "\u001b[32m2021-05-03T05:28:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T05:28:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:28:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:28:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:29:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:29:54 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T05:30:31 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:31:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:31:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 2.2604, val/total_loss: 2.2604, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.4024, val/hateful_memes/roc_auc: 0.6586, num_updates: 4000, epoch: 14, iterations: 4000, max_updates: 22000, val_time: 02m 21s 034ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.658599\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:32:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:32:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:34:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0313, train/hateful_memes/cross_entropy/avg: 0.2975, train/total_loss: 0.0313, train/total_loss/avg: 0.2975, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 0.55, time: 03m 03s 808ms, time_since_start: 01h 43m 44s 721ms, eta: 09h 17m 08s 081ms\n",
      "\u001b[32m2021-05-03T05:35:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0258, train/hateful_memes/cross_entropy/avg: 0.2905, train/total_loss: 0.0258, train/total_loss/avg: 0.2905, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 665ms, time_since_start: 01h 45m 27s 387ms, eta: 05h 09m 26s 937ms\n",
      "\u001b[32m2021-05-03T05:37:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0258, train/hateful_memes/cross_entropy/avg: 0.2852, train/total_loss: 0.0258, train/total_loss/avg: 0.2852, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 952ms, time_since_start: 01h 47m 10s 340ms, eta: 05h 08m 34s 225ms\n",
      "\u001b[32m2021-05-03T05:39:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0258, train/hateful_memes/cross_entropy/avg: 0.2794, train/total_loss: 0.0258, train/total_loss/avg: 0.2794, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 706ms, time_since_start: 01h 48m 56s 047ms, eta: 05h 15m 01s 988ms\n",
      "\u001b[32m2021-05-03T05:41:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0256, train/hateful_memes/cross_entropy/avg: 0.2736, train/total_loss: 0.0256, train/total_loss/avg: 0.2736, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 475ms, time_since_start: 01h 50m 39s 523ms, eta: 05h 06m 38s 018ms\n",
      "\u001b[32m2021-05-03T05:42:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0256, train/hateful_memes/cross_entropy/avg: 0.2685, train/total_loss: 0.0256, train/total_loss/avg: 0.2685, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 487ms, time_since_start: 01h 52m 23s 010ms, eta: 05h 04m 54s 891ms\n",
      "\u001b[32m2021-05-03T05:44:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0256, train/hateful_memes/cross_entropy/avg: 0.2635, train/total_loss: 0.0256, train/total_loss/avg: 0.2635, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 196ms, time_since_start: 01h 54m 09s 207ms, eta: 05h 11m 05s 964ms\n",
      "\u001b[32m2021-05-03T05:46:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0229, train/hateful_memes/cross_entropy/avg: 0.2582, train/total_loss: 0.0229, train/total_loss/avg: 0.2582, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 970ms, time_since_start: 01h 55m 53s 177ms, eta: 05h 02m 48s 997ms\n",
      "\u001b[32m2021-05-03T05:48:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0229, train/hateful_memes/cross_entropy/avg: 0.2536, train/total_loss: 0.0229, train/total_loss/avg: 0.2536, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 373ms, time_since_start: 01h 57m 36s 550ms, eta: 04h 59m 19s 650ms\n",
      "\u001b[32m2021-05-03T05:49:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T05:49:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:50:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:50:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:50:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0229, train/hateful_memes/cross_entropy/avg: 0.2488, train/total_loss: 0.0229, train/total_loss/avg: 0.2488, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 47s 494ms, time_since_start: 02h 24s 045ms, eta: 08h 02m 09s 639ms\n",
      "\u001b[32m2021-05-03T05:50:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T05:50:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:50:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:50:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:51:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:51:50 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T05:52:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:53:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:53:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 2.5634, val/total_loss: 2.5634, val/hateful_memes/accuracy: 0.5880, val/hateful_memes/binary_f1: 0.3905, val/hateful_memes/roc_auc: 0.6605, num_updates: 5000, epoch: 18, iterations: 5000, max_updates: 22000, val_time: 02m 12s 081ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.660455\n",
      "\u001b[32m2021-05-03T05:56:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0221, train/hateful_memes/cross_entropy/avg: 0.2440, train/total_loss: 0.0221, train/total_loss/avg: 0.2440, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 0.55, time: 03m 02s 490ms, time_since_start: 02h 05m 38s 619ms, eta: 08h 42m 14s 308ms\n",
      "\u001b[32m2021-05-03T05:57:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0221, train/hateful_memes/cross_entropy/avg: 0.2402, train/total_loss: 0.0221, train/total_loss/avg: 0.2402, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 1.00, time: 01m 40s 239ms, time_since_start: 02h 07m 18s 858ms, eta: 04h 45m 09s 666ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:57:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:57:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:59:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0221, train/hateful_memes/cross_entropy/avg: 0.2357, train/total_loss: 0.0221, train/total_loss/avg: 0.2357, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 509ms, time_since_start: 02h 09m 05s 368ms, eta: 05h 01m 11s 718ms\n",
      "\u001b[32m2021-05-03T06:01:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0208, train/hateful_memes/cross_entropy/avg: 0.2314, train/total_loss: 0.0208, train/total_loss/avg: 0.2314, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 659ms, time_since_start: 02h 10m 49s 028ms, eta: 04h 51m 22s 817ms\n",
      "\u001b[32m2021-05-03T06:03:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0208, train/hateful_memes/cross_entropy/avg: 0.2273, train/total_loss: 0.0208, train/total_loss/avg: 0.2273, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 0.93, time: 01m 47s 421ms, time_since_start: 02h 12m 36s 450ms, eta: 05h 08s 221ms\n",
      "\u001b[32m2021-05-03T06:04:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0156, train/hateful_memes/cross_entropy/avg: 0.2233, train/total_loss: 0.0156, train/total_loss/avg: 0.2233, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 952ms, time_since_start: 02h 14m 19s 403ms, eta: 04h 45m 54s 417ms\n",
      "\u001b[32m2021-05-03T06:06:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0156, train/hateful_memes/cross_entropy/avg: 0.2194, train/total_loss: 0.0156, train/total_loss/avg: 0.2194, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 873ms, time_since_start: 02h 16m 04s 276ms, eta: 04h 49m 27s 930ms\n",
      "\u001b[32m2021-05-03T06:08:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0138, train/hateful_memes/cross_entropy/avg: 0.2158, train/total_loss: 0.0138, train/total_loss/avg: 0.2158, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 117ms, time_since_start: 02h 17m 50s 394ms, eta: 04h 51m 06s 067ms\n",
      "\u001b[32m2021-05-03T06:10:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0138, train/hateful_memes/cross_entropy/avg: 0.2134, train/total_loss: 0.0138, train/total_loss/avg: 0.2134, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 036ms, time_since_start: 02h 19m 34s 431ms, eta: 04h 43m 37s 928ms\n",
      "\u001b[32m2021-05-03T06:11:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T06:11:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:12:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:12:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:12:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0129, train/hateful_memes/cross_entropy/avg: 0.2100, train/total_loss: 0.0129, train/total_loss/avg: 0.2100, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 45s 751ms, time_since_start: 02h 22m 20s 182ms, eta: 07h 29m 04s 582ms\n",
      "\u001b[32m2021-05-03T06:12:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T06:12:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:12:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:12:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T06:13:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:13:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:14:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:14:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.5534, val/total_loss: 2.5534, val/hateful_memes/accuracy: 0.5860, val/hateful_memes/binary_f1: 0.4103, val/hateful_memes/roc_auc: 0.6569, num_updates: 6000, epoch: 21, iterations: 6000, max_updates: 22000, val_time: 01m 32s 375ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.660455\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:16:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:16:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T06:16:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0129, train/hateful_memes/cross_entropy/avg: 0.2071, train/total_loss: 0.0129, train/total_loss/avg: 0.2071, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 0.65, time: 02m 33s 218ms, time_since_start: 02h 26m 25s 778ms, eta: 06h 52m 31s 473ms\n",
      "\u001b[32m2021-05-03T06:18:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0129, train/hateful_memes/cross_entropy/avg: 0.2038, train/total_loss: 0.0129, train/total_loss/avg: 0.2038, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 155ms, time_since_start: 02h 28m 08s 933ms, eta: 04h 35m 59s 369ms\n",
      "\u001b[32m2021-05-03T06:20:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0129, train/hateful_memes/cross_entropy/avg: 0.2010, train/total_loss: 0.0129, train/total_loss/avg: 0.2010, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 101ms, time_since_start: 02h 29m 53s 035ms, eta: 04h 36m 45s 381ms\n",
      "\u001b[32m2021-05-03T06:22:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0105, train/hateful_memes/cross_entropy/avg: 0.1978, train/total_loss: 0.0105, train/total_loss/avg: 0.1978, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 191ms, time_since_start: 02h 31m 39s 227ms, eta: 04h 40m 30s 987ms\n",
      "\u001b[32m2021-05-03T06:23:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0074, train/hateful_memes/cross_entropy/avg: 0.1949, train/total_loss: 0.0074, train/total_loss/avg: 0.1949, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 931ms, time_since_start: 02h 33m 23s 158ms, eta: 04h 32m 47s 115ms\n",
      "\u001b[32m2021-05-03T06:25:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0074, train/hateful_memes/cross_entropy/avg: 0.1922, train/total_loss: 0.0074, train/total_loss/avg: 0.1922, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 185ms, time_since_start: 02h 35m 07s 343ms, eta: 04h 31m 41s 265ms\n",
      "\u001b[32m2021-05-03T06:27:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1894, train/total_loss: 0.0068, train/total_loss/avg: 0.1894, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 486ms, time_since_start: 02h 36m 53s 830ms, eta: 04h 35m 53s 064ms\n",
      "\u001b[32m2021-05-03T06:29:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1867, train/total_loss: 0.0068, train/total_loss/avg: 0.1867, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 607ms, time_since_start: 02h 38m 38s 437ms, eta: 04h 29m 14s 772ms\n",
      "\u001b[32m2021-05-03T06:30:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0068, train/hateful_memes/cross_entropy/avg: 0.1844, train/total_loss: 0.0068, train/total_loss/avg: 0.1844, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 607ms, time_since_start: 02h 40m 23s 045ms, eta: 04h 27m 28s 433ms\n",
      "\u001b[32m2021-05-03T06:32:35 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T06:32:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:32:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:33:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:33:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0060, train/hateful_memes/cross_entropy/avg: 0.1817, train/total_loss: 0.0060, train/total_loss/avg: 0.1817, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 45s 506ms, time_since_start: 02h 43m 08s 551ms, eta: 07h 23s 229ms\n",
      "\u001b[32m2021-05-03T06:33:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T06:33:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:33:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:33:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T06:34:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:34:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:35:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:35:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.6146, val/total_loss: 2.6146, val/hateful_memes/accuracy: 0.5940, val/hateful_memes/binary_f1: 0.4408, val/hateful_memes/roc_auc: 0.6493, num_updates: 7000, epoch: 25, iterations: 7000, max_updates: 22000, val_time: 01m 32s 821ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.660455\n",
      "\u001b[32m2021-05-03T06:38:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0060, train/hateful_memes/cross_entropy/avg: 0.1792, train/total_loss: 0.0060, train/total_loss/avg: 0.1792, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 0.54, time: 03m 04s 434ms, time_since_start: 02h 47m 45s 814ms, eta: 07h 45m 20s 505ms\n",
      "\u001b[32m2021-05-03T06:39:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1767, train/total_loss: 0.0041, train/total_loss/avg: 0.1767, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 1.01, time: 01m 39s 814ms, time_since_start: 02h 49m 25s 628ms, eta: 04h 10m 08s 910ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:40:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:40:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T06:41:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1743, train/total_loss: 0.0041, train/total_loss/avg: 0.1743, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 536ms, time_since_start: 02h 51m 11s 165ms, eta: 04h 22m 42s 142ms\n",
      "\u001b[32m2021-05-03T06:43:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0060, train/hateful_memes/cross_entropy/avg: 0.1728, train/total_loss: 0.0060, train/total_loss/avg: 0.1728, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 831ms, time_since_start: 02h 52m 54s 997ms, eta: 04h 16m 41s 915ms\n",
      "\u001b[32m2021-05-03T06:45:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0041, train/hateful_memes/cross_entropy/avg: 0.1706, train/total_loss: 0.0041, train/total_loss/avg: 0.1706, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 872ms, time_since_start: 02h 54m 38s 870ms, eta: 04h 15m 02s 554ms\n",
      "\u001b[32m2021-05-03T06:46:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0039, train/hateful_memes/cross_entropy/avg: 0.1684, train/total_loss: 0.0039, train/total_loss/avg: 0.1684, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 043ms, time_since_start: 02h 56m 24s 914ms, eta: 04h 18m 34s 658ms\n",
      "\u001b[32m2021-05-03T06:48:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0039, train/hateful_memes/cross_entropy/avg: 0.1662, train/total_loss: 0.0039, train/total_loss/avg: 0.1662, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 672ms, time_since_start: 02h 58m 09s 586ms, eta: 04h 13m 27s 684ms\n",
      "\u001b[32m2021-05-03T06:50:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0039, train/hateful_memes/cross_entropy/avg: 0.1643, train/total_loss: 0.0039, train/total_loss/avg: 0.1643, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 175ms, time_since_start: 02h 59m 53s 762ms, eta: 04h 10m 29s 630ms\n",
      "\u001b[32m2021-05-03T06:52:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0039, train/hateful_memes/cross_entropy/avg: 0.1625, train/total_loss: 0.0039, train/total_loss/avg: 0.1625, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 0.93, time: 01m 47s 023ms, time_since_start: 03h 01m 40s 786ms, eta: 04h 15m 31s 821ms\n",
      "\u001b[32m2021-05-03T06:53:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T06:53:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:54:15 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:54:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:54:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0039, train/hateful_memes/cross_entropy/avg: 0.1605, train/total_loss: 0.0039, train/total_loss/avg: 0.1605, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 45s 981ms, time_since_start: 03h 04m 26s 767ms, eta: 06h 33m 29s 163ms\n",
      "\u001b[32m2021-05-03T06:54:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T06:54:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:54:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:54:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T06:55:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:55:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:56:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:56:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.8913, val/total_loss: 2.8913, val/hateful_memes/accuracy: 0.6100, val/hateful_memes/binary_f1: 0.4538, val/hateful_memes/roc_auc: 0.6542, num_updates: 8000, epoch: 28, iterations: 8000, max_updates: 22000, val_time: 01m 31s 454ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.660455\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:58:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:58:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T06:58:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0039, train/hateful_memes/cross_entropy/avg: 0.1586, train/total_loss: 0.0039, train/total_loss/avg: 0.1586, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00001, ups: 0.65, time: 02m 34s 520ms, time_since_start: 03h 08m 32s 754ms, eta: 06h 03m 41s 939ms\n",
      "\u001b[32m2021-05-03T07:00:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0039, train/hateful_memes/cross_entropy/avg: 0.1567, train/total_loss: 0.0039, train/total_loss/avg: 0.1567, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 994ms, time_since_start: 03h 10m 15s 749ms, eta: 04h 40s 722ms\n",
      "\u001b[32m2021-05-03T07:02:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1548, train/total_loss: 0.0020, train/total_loss/avg: 0.1548, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 735ms, time_since_start: 03h 11m 59s 485ms, eta: 04h 39s 220ms\n",
      "\u001b[32m2021-05-03T07:04:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1530, train/total_loss: 0.0020, train/total_loss/avg: 0.1530, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 421ms, time_since_start: 03h 13m 45s 907ms, eta: 04h 05m 04s 915ms\n",
      "\u001b[32m2021-05-03T07:05:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1512, train/total_loss: 0.0019, train/total_loss/avg: 0.1512, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 134ms, time_since_start: 03h 15m 30s 042ms, eta: 03h 58m 03s 148ms\n",
      "\u001b[32m2021-05-03T07:07:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1495, train/total_loss: 0.0019, train/total_loss/avg: 0.1495, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 737ms, time_since_start: 03h 17m 14s 779ms, eta: 03h 57m 39s 335ms\n",
      "\u001b[32m2021-05-03T07:09:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1478, train/total_loss: 0.0020, train/total_loss/avg: 0.1478, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 819ms, time_since_start: 03h 19m 599ms, eta: 03h 58m 19s 243ms\n",
      "\u001b[32m2021-05-03T07:11:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1463, train/total_loss: 0.0020, train/total_loss/avg: 0.1463, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 152ms, time_since_start: 03h 20m 44s 752ms, eta: 03h 52m 48s 098ms\n",
      "\u001b[32m2021-05-03T07:12:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1447, train/total_loss: 0.0020, train/total_loss/avg: 0.1447, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 760ms, time_since_start: 03h 22m 28s 512ms, eta: 03h 50m 10s 100ms\n",
      "\u001b[32m2021-05-03T07:14:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T07:14:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T07:15:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T07:15:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T07:15:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1431, train/total_loss: 0.0020, train/total_loss/avg: 0.1431, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 47s 078ms, time_since_start: 03h 25m 15s 591ms, eta: 06h 07m 47s 781ms\n",
      "\u001b[32m2021-05-03T07:15:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T07:15:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:15:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:15:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T07:16:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T07:16:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T07:17:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T07:17:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.8650, val/total_loss: 2.8650, val/hateful_memes/accuracy: 0.5980, val/hateful_memes/binary_f1: 0.4463, val/hateful_memes/roc_auc: 0.6456, num_updates: 9000, epoch: 32, iterations: 9000, max_updates: 22000, val_time: 01m 36s 157ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.660455\n",
      "\u001b[32m2021-05-03T07:19:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1416, train/total_loss: 0.0024, train/total_loss/avg: 0.1416, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00001, ups: 0.66, time: 02m 31s 404ms, time_since_start: 03h 29m 23s 164ms, eta: 05h 30m 43s 632ms\n",
      "\u001b[32m2021-05-03T07:21:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1401, train/total_loss: 0.0024, train/total_loss/avg: 0.1401, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00001, ups: 1.00, time: 01m 40s 295ms, time_since_start: 03h 31m 03s 459ms, eta: 03h 37m 23s 261ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:22:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:22:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T07:23:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1386, train/total_loss: 0.0024, train/total_loss/avg: 0.1386, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 100ms, time_since_start: 03h 32m 49s 560ms, eta: 03h 48m 10s 315ms\n",
      "\u001b[32m2021-05-03T07:25:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1371, train/total_loss: 0.0024, train/total_loss/avg: 0.1371, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 254ms, time_since_start: 03h 34m 33s 814ms, eta: 03h 42m 26s 203ms\n",
      "\u001b[32m2021-05-03T07:26:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1357, train/total_loss: 0.0019, train/total_loss/avg: 0.1357, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 514ms, time_since_start: 03h 36m 18s 328ms, eta: 03h 41m 13s 324ms\n",
      "\u001b[32m2021-05-03T07:28:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.1343, train/total_loss: 0.0011, train/total_loss/avg: 0.1343, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 450ms, time_since_start: 03h 38m 03s 779ms, eta: 03h 41m 25s 116ms\n",
      "\u001b[32m2021-05-03T07:30:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1329, train/total_loss: 0.0016, train/total_loss/avg: 0.1329, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 902ms, time_since_start: 03h 39m 47s 682ms, eta: 03h 36m 24s 495ms\n",
      "\u001b[32m2021-05-03T07:31:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1316, train/total_loss: 0.0016, train/total_loss/avg: 0.1316, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 553ms, time_since_start: 03h 41m 32s 235ms, eta: 03h 35m 59s 634ms\n",
      "\u001b[32m2021-05-03T07:33:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1303, train/total_loss: 0.0012, train/total_loss/avg: 0.1303, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 663ms, time_since_start: 03h 43m 17s 899ms, eta: 03h 36m 29s 875ms\n",
      "\u001b[32m2021-05-03T07:35:28 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T07:35:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T07:35:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T07:36:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T07:36:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.1290, train/total_loss: 0.0011, train/total_loss/avg: 0.1290, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 46s 018ms, time_since_start: 03h 46m 03s 918ms, eta: 05h 37m 20s 995ms\n",
      "\u001b[32m2021-05-03T07:36:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T07:36:30 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:36:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:36:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T07:37:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 3.0804, val/total_loss: 3.0804, val/hateful_memes/accuracy: 0.6040, val/hateful_memes/binary_f1: 0.4706, val/hateful_memes/roc_auc: 0.6456, num_updates: 10000, epoch: 35, iterations: 10000, max_updates: 22000, val_time: 35s 444ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.660455\n",
      "\u001b[32m2021-05-03T07:39:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1277, train/total_loss: 0.0007, train/total_loss/avg: 0.1277, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00001, ups: 0.76, time: 02m 12s 456ms, time_since_start: 03h 48m 51s 821ms, eta: 04h 26m 54s 486ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:39:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:39:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T07:41:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.1264, train/total_loss: 0.0011, train/total_loss/avg: 0.1264, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 490ms, time_since_start: 03h 50m 36s 312ms, eta: 03h 28m 47s 216ms\n",
      "\u001b[32m2021-05-03T07:42:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1252, train/total_loss: 0.0012, train/total_loss/avg: 0.1252, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 957ms, time_since_start: 03h 52m 20s 269ms, eta: 03h 25m 57s 607ms\n",
      "\u001b[32m2021-05-03T07:44:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1240, train/total_loss: 0.0012, train/total_loss/avg: 0.1240, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 045ms, time_since_start: 03h 54m 04s 315ms, eta: 03h 24m 22s 390ms\n",
      "\u001b[32m2021-05-03T07:46:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1240, train/total_loss: 0.0015, train/total_loss/avg: 0.1240, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 801ms, time_since_start: 03h 55m 51s 116ms, eta: 03h 27m 58s 665ms\n",
      "\u001b[32m2021-05-03T07:48:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1228, train/total_loss: 0.0015, train/total_loss/avg: 0.1228, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 040ms, time_since_start: 03h 57m 35s 157ms, eta: 03h 20m 50s 399ms\n",
      "\u001b[32m2021-05-03T07:49:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1217, train/total_loss: 0.0012, train/total_loss/avg: 0.1217, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00001, ups: 0.93, time: 01m 47s 183ms, time_since_start: 03h 59m 22s 341ms, eta: 03h 25m 05s 582ms\n",
      "\u001b[32m2021-05-03T07:51:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1206, train/total_loss: 0.0012, train/total_loss/avg: 0.1206, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 965ms, time_since_start: 04h 01m 05s 306ms, eta: 03h 15m 16s 666ms\n",
      "\u001b[32m2021-05-03T07:53:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1196, train/total_loss: 0.0012, train/total_loss/avg: 0.1196, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 328ms, time_since_start: 04h 02m 49s 635ms, eta: 03h 16m 05s 766ms\n",
      "\u001b[32m2021-05-03T07:55:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T07:55:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T07:55:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T07:56:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T07:56:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1185, train/total_loss: 0.0012, train/total_loss/avg: 0.1185, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00001, ups: 0.59, time: 02m 49s 511ms, time_since_start: 04h 05m 39s 147ms, eta: 05h 15m 44s 631ms\n",
      "\u001b[32m2021-05-03T07:56:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T07:56:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:56:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:56:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T07:56:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.7717, val/total_loss: 2.7717, val/hateful_memes/accuracy: 0.6240, val/hateful_memes/binary_f1: 0.5253, val/hateful_memes/roc_auc: 0.6504, num_updates: 11000, epoch: 39, iterations: 11000, max_updates: 22000, val_time: 41s 295ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.660455\n",
      "\u001b[32m2021-05-03T07:58:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1175, train/total_loss: 0.0007, train/total_loss/avg: 0.1175, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00001, ups: 0.79, time: 02m 06s 489ms, time_since_start: 04h 08m 26s 935ms, eta: 03h 53m 27s 964ms\n",
      "\u001b[32m2021-05-03T08:00:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1164, train/total_loss: 0.0007, train/total_loss/avg: 0.1164, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 271ms, time_since_start: 04h 10m 10s 207ms, eta: 03h 08m 51s 779ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:01:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:01:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:02:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1154, train/total_loss: 0.0006, train/total_loss/avg: 0.1154, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 097ms, time_since_start: 04h 11m 56s 304ms, eta: 03h 12m 14s 057ms\n",
      "\u001b[32m2021-05-03T08:04:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1144, train/total_loss: 0.0007, train/total_loss/avg: 0.1144, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 127ms, time_since_start: 04h 13m 40s 432ms, eta: 03h 06m 54s 129ms\n",
      "\u001b[32m2021-05-03T08:05:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1134, train/total_loss: 0.0012, train/total_loss/avg: 0.1134, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 305ms, time_since_start: 04h 15m 24s 737ms, eta: 03h 05m 27s 285ms\n",
      "\u001b[32m2021-05-03T08:07:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1124, train/total_loss: 0.0012, train/total_loss/avg: 0.1124, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 782ms, time_since_start: 04h 17m 10s 520ms, eta: 03h 06m 17s 378ms\n",
      "\u001b[32m2021-05-03T08:09:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1115, train/total_loss: 0.0012, train/total_loss/avg: 0.1115, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 008ms, time_since_start: 04h 18m 54s 528ms, eta: 03h 01m 24s 232ms\n",
      "\u001b[32m2021-05-03T08:11:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1106, train/total_loss: 0.0012, train/total_loss/avg: 0.1106, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 734ms, time_since_start: 04h 20m 39s 262ms, eta: 03h 53s 879ms\n",
      "\u001b[32m2021-05-03T08:12:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1096, train/total_loss: 0.0013, train/total_loss/avg: 0.1096, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 407ms, time_since_start: 04h 22m 24s 670ms, eta: 03h 16s 501ms\n",
      "\u001b[32m2021-05-03T08:14:35 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T08:14:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T08:14:59 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T08:15:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T08:15:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1087, train/total_loss: 0.0013, train/total_loss/avg: 0.1087, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 46s 396ms, time_since_start: 04h 25m 11s 066ms, eta: 04h 41m 45s 863ms\n",
      "\u001b[32m2021-05-03T08:15:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T08:15:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:15:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:15:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:16:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 3.2215, val/total_loss: 3.2215, val/hateful_memes/accuracy: 0.6120, val/hateful_memes/binary_f1: 0.4670, val/hateful_memes/roc_auc: 0.6586, num_updates: 12000, epoch: 42, iterations: 12000, max_updates: 22000, val_time: 39s 127ms, best_update: 5000, best_iteration: 5000, best_val/hateful_memes/roc_auc: 0.660455\n",
      "\u001b[32m2021-05-03T08:18:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1078, train/total_loss: 0.0013, train/total_loss/avg: 0.1078, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0., ups: 0.78, time: 02m 09s 406ms, time_since_start: 04h 27m 59s 607ms, eta: 03h 36m 56s 254ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:19:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:19:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:20:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1070, train/total_loss: 0.0006, train/total_loss/avg: 0.1070, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 843ms, time_since_start: 04h 29m 45s 451ms, eta: 02h 55m 38s 654ms\n",
      "\u001b[32m2021-05-03T08:21:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1061, train/total_loss: 0.0004, train/total_loss/avg: 0.1061, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 918ms, time_since_start: 04h 31m 29s 369ms, eta: 02h 50m 41s 345ms\n",
      "\u001b[32m2021-05-03T08:23:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1052, train/total_loss: 0.0004, train/total_loss/avg: 0.1052, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 298ms, time_since_start: 04h 33m 13s 668ms, eta: 02h 49m 32s 903ms\n",
      "\u001b[32m2021-05-03T08:25:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1044, train/total_loss: 0.0004, train/total_loss/avg: 0.1044, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 050ms, time_since_start: 04h 34m 58s 719ms, eta: 02h 48m 59s 525ms\n",
      "\u001b[32m2021-05-03T08:27:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.1036, train/total_loss: 0.0004, train/total_loss/avg: 0.1036, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 964ms, time_since_start: 04h 36m 43s 684ms, eta: 02h 47m 04s 551ms\n",
      "\u001b[32m2021-05-03T08:28:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1028, train/total_loss: 0.0003, train/total_loss/avg: 0.1028, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 293ms, time_since_start: 04h 38m 27s 978ms, eta: 02h 44m 14s 510ms\n",
      "\u001b[32m2021-05-03T08:30:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1020, train/total_loss: 0.0003, train/total_loss/avg: 0.1020, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 934ms, time_since_start: 04h 40m 13s 912ms, eta: 02h 45m 01s 905ms\n",
      "\u001b[32m2021-05-03T08:32:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1012, train/total_loss: 0.0003, train/total_loss/avg: 0.1012, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 755ms, time_since_start: 04h 41m 58s 668ms, eta: 02h 41m 25s 293ms\n",
      "\u001b[32m2021-05-03T08:34:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T08:34:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T08:34:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T08:35:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T08:35:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1004, train/total_loss: 0.0003, train/total_loss/avg: 0.1004, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0., ups: 0.60, time: 02m 46s 285ms, time_since_start: 04h 44m 44s 953ms, eta: 04h 13m 25s 127ms\n",
      "\u001b[32m2021-05-03T08:35:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T08:35:11 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:35:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:35:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:35:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T08:36:10 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T08:37:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T08:37:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T08:37:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 3.3003, val/total_loss: 3.3003, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.4586, val/hateful_memes/roc_auc: 0.6647, num_updates: 13000, epoch: 45, iterations: 13000, max_updates: 22000, val_time: 02m 48s 218ms, best_update: 13000, best_iteration: 13000, best_val/hateful_memes/roc_auc: 0.664748\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:38:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:38:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:41:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0996, train/total_loss: 0.0003, train/total_loss/avg: 0.0996, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0., ups: 0.44, time: 03m 45s 480ms, time_since_start: 04h 51m 18s 670ms, eta: 05h 39m 48s 847ms\n",
      "\u001b[32m2021-05-03T08:43:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0989, train/total_loss: 0.0002, train/total_loss/avg: 0.0989, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 646ms, time_since_start: 04h 53m 316ms, eta: 02h 31m 27s 987ms\n",
      "\u001b[32m2021-05-03T08:45:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0981, train/total_loss: 0.0002, train/total_loss/avg: 0.0981, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0., ups: 0.93, time: 01m 47s 731ms, time_since_start: 04h 54m 48s 048ms, eta: 02h 38m 42s 637ms\n",
      "\u001b[32m2021-05-03T08:46:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0974, train/total_loss: 0.0002, train/total_loss/avg: 0.0974, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 895ms, time_since_start: 04h 56m 30s 944ms, eta: 02h 29m 50s 603ms\n",
      "\u001b[32m2021-05-03T08:48:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0967, train/total_loss: 0.0002, train/total_loss/avg: 0.0967, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 209ms, time_since_start: 04h 58m 15s 153ms, eta: 02h 29m 59s 515ms\n",
      "\u001b[32m2021-05-03T08:50:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0960, train/total_loss: 0.0002, train/total_loss/avg: 0.0960, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 911ms, time_since_start: 05h 02s 065ms, eta: 02h 32m 04s 283ms\n",
      "\u001b[32m2021-05-03T08:52:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0953, train/total_loss: 0.0002, train/total_loss/avg: 0.0953, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 551ms, time_since_start: 05h 01m 45s 616ms, eta: 02h 25m 32s 263ms\n",
      "\u001b[32m2021-05-03T08:53:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0947, train/total_loss: 0.0002, train/total_loss/avg: 0.0947, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 266ms, time_since_start: 05h 03m 29s 883ms, eta: 02h 24m 46s 658ms\n",
      "\u001b[32m2021-05-03T08:55:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0940, train/total_loss: 0.0002, train/total_loss/avg: 0.0940, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 179ms, time_since_start: 05h 05m 16s 062ms, eta: 02h 25m 38s 122ms\n",
      "\u001b[32m2021-05-03T08:57:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T08:57:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T08:57:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T08:58:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T08:58:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0934, train/total_loss: 0.0002, train/total_loss/avg: 0.0934, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 45s 529ms, time_since_start: 05h 08m 01s 592ms, eta: 03h 44m 14s 279ms\n",
      "\u001b[32m2021-05-03T08:58:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T08:58:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:58:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:58:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:59:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T08:59:32 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T09:00:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T09:00:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T09:00:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 3.5264, val/total_loss: 3.5264, val/hateful_memes/accuracy: 0.6020, val/hateful_memes/binary_f1: 0.4330, val/hateful_memes/roc_auc: 0.6675, num_updates: 14000, epoch: 49, iterations: 14000, max_updates: 22000, val_time: 02m 16s 039ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.667496\n",
      "\u001b[32m2021-05-03T09:04:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0927, train/total_loss: 0.0001, train/total_loss/avg: 0.0927, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0., ups: 0.46, time: 03m 37s 327ms, time_since_start: 05h 13m 54s 962ms, eta: 04h 50m 43s 550ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:05:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:05:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T09:06:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0921, train/total_loss: 0.0001, train/total_loss/avg: 0.0921, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 029ms, time_since_start: 05h 15m 36s 991ms, eta: 02h 14m 45s 624ms\n",
      "\u001b[32m2021-05-03T09:07:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0914, train/total_loss: 0.0001, train/total_loss/avg: 0.0914, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 710ms, time_since_start: 05h 17m 21s 702ms, eta: 02h 16m 31s 735ms\n",
      "\u001b[32m2021-05-03T09:09:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0908, train/total_loss: 0.0001, train/total_loss/avg: 0.0908, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 414ms, time_since_start: 05h 19m 06s 117ms, eta: 02h 14m 22s 480ms\n",
      "\u001b[32m2021-05-03T09:11:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0902, train/total_loss: 0.0001, train/total_loss/avg: 0.0902, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 876ms, time_since_start: 05h 20m 51s 993ms, eta: 02h 14m 27s 775ms\n",
      "\u001b[32m2021-05-03T09:13:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0895, train/total_loss: 0.0001, train/total_loss/avg: 0.0895, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 678ms, time_since_start: 05h 22m 36s 671ms, eta: 02h 11m 10s 120ms\n",
      "\u001b[32m2021-05-03T09:14:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0889, train/total_loss: 0.0001, train/total_loss/avg: 0.0889, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 346ms, time_since_start: 05h 24m 21s 017ms, eta: 02h 08m 59s 152ms\n",
      "\u001b[32m2021-05-03T09:16:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0883, train/total_loss: 0.0002, train/total_loss/avg: 0.0883, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 563ms, time_since_start: 05h 26m 06s 581ms, eta: 02h 08m 42s 216ms\n",
      "\u001b[32m2021-05-03T09:18:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0878, train/total_loss: 0.0002, train/total_loss/avg: 0.0878, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 313ms, time_since_start: 05h 27m 50s 895ms, eta: 02h 05m 24s 756ms\n",
      "\u001b[32m2021-05-03T09:20:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T09:20:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T09:20:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T09:21:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T09:21:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0872, train/total_loss: 0.0002, train/total_loss/avg: 0.0872, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0., ups: 0.60, time: 02m 46s 708ms, time_since_start: 05h 30m 37s 603ms, eta: 03h 17m 36s 293ms\n",
      "\u001b[32m2021-05-03T09:21:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T09:21:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:21:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:21:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T09:21:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T09:21:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T09:22:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T09:22:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 3.6886, val/total_loss: 3.6886, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.4400, val/hateful_memes/roc_auc: 0.6663, num_updates: 15000, epoch: 52, iterations: 15000, max_updates: 22000, val_time: 01m 29s 267ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.667496\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:23:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:23:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T09:25:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0866, train/total_loss: 0.0001, train/total_loss/avg: 0.0866, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0., ups: 0.68, time: 02m 28s 298ms, time_since_start: 05h 34m 35s 174ms, eta: 02h 53m 16s 311ms\n",
      "\u001b[32m2021-05-03T09:26:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0860, train/total_loss: 0.0001, train/total_loss/avg: 0.0860, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 592ms, time_since_start: 05h 36m 16s 767ms, eta: 01h 56m 58s 833ms\n",
      "\u001b[32m2021-05-03T09:28:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0855, train/total_loss: 0.0001, train/total_loss/avg: 0.0855, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 543ms, time_since_start: 05h 38m 01s 311ms, eta: 01h 58m 36s 518ms\n",
      "\u001b[32m2021-05-03T09:30:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0849, train/total_loss: 0.0001, train/total_loss/avg: 0.0849, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 989ms, time_since_start: 05h 39m 46s 301ms, eta: 01h 57m 20s 172ms\n",
      "\u001b[32m2021-05-03T09:31:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0844, train/total_loss: 0.0001, train/total_loss/avg: 0.0844, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 225ms, time_since_start: 05h 41m 30s 526ms, eta: 01h 54m 43s 045ms\n",
      "\u001b[32m2021-05-03T09:33:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0838, train/total_loss: 0.0001, train/total_loss/avg: 0.0838, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 656ms, time_since_start: 05h 43m 14s 183ms, eta: 01h 52m 20s 183ms\n",
      "\u001b[32m2021-05-03T09:35:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0833, train/total_loss: 0.0001, train/total_loss/avg: 0.0833, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 872ms, time_since_start: 05h 45m 055ms, eta: 01h 52m 56s 663ms\n",
      "\u001b[32m2021-05-03T09:37:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0828, train/total_loss: 0.0001, train/total_loss/avg: 0.0828, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 404ms, time_since_start: 05h 46m 44s 459ms, eta: 01h 49m 36s 645ms\n",
      "\u001b[32m2021-05-03T09:38:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0823, train/total_loss: 0.0001, train/total_loss/avg: 0.0823, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0., ups: 0.93, time: 01m 47s 429ms, time_since_start: 05h 48m 31s 889ms, eta: 01h 50m 58s 037ms\n",
      "\u001b[32m2021-05-03T09:40:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T09:40:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T09:41:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T09:41:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T09:41:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0817, train/total_loss: 0.0001, train/total_loss/avg: 0.0817, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 44s 904ms, time_since_start: 05h 51m 16s 793ms, eta: 02h 47m 32s 562ms\n",
      "\u001b[32m2021-05-03T09:41:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T09:41:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:41:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:41:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T09:42:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T09:42:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T09:43:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T09:43:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 3.5461, val/total_loss: 3.5461, val/hateful_memes/accuracy: 0.6140, val/hateful_memes/binary_f1: 0.4712, val/hateful_memes/roc_auc: 0.6605, num_updates: 16000, epoch: 56, iterations: 16000, max_updates: 22000, val_time: 01m 34s 233ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.667496\n",
      "\u001b[32m2021-05-03T09:45:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0812, train/total_loss: 0.0001, train/total_loss/avg: 0.0812, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0., ups: 0.67, time: 02m 30s 369ms, time_since_start: 05h 55m 21s 399ms, eta: 02h 30m 13s 772ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:47:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:47:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T09:47:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0807, train/total_loss: 0.0001, train/total_loss/avg: 0.0807, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 469ms, time_since_start: 05h 57m 02s 868ms, eta: 01h 39m 39s 403ms\n",
      "\u001b[32m2021-05-03T09:49:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0803, train/total_loss: 0.0001, train/total_loss/avg: 0.0803, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 593ms, time_since_start: 05h 58m 46s 462ms, eta: 01h 39m 59s 328ms\n",
      "\u001b[32m2021-05-03T09:50:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0798, train/total_loss: 0.0001, train/total_loss/avg: 0.0798, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 672ms, time_since_start: 06h 30s 135ms, eta: 01h 38m 18s 566ms\n",
      "\u001b[32m2021-05-03T09:52:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0793, train/total_loss: 0.0001, train/total_loss/avg: 0.0793, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 582ms, time_since_start: 06h 02m 16s 718ms, eta: 01h 39m 15s 853ms\n",
      "\u001b[32m2021-05-03T09:54:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0788, train/total_loss: 0.0001, train/total_loss/avg: 0.0788, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 486ms, time_since_start: 06h 04m 01s 204ms, eta: 01h 35m 32s 524ms\n",
      "\u001b[32m2021-05-03T09:56:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0784, train/total_loss: 0.0001, train/total_loss/avg: 0.0784, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 928ms, time_since_start: 06h 05m 45s 133ms, eta: 01h 33m 16s 357ms\n",
      "\u001b[32m2021-05-03T09:57:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0779, train/total_loss: 0.0001, train/total_loss/avg: 0.0779, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 983ms, time_since_start: 06h 07m 31s 116ms, eta: 01h 33m 19s 301ms\n",
      "\u001b[32m2021-05-03T09:59:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0774, train/total_loss: 0.0001, train/total_loss/avg: 0.0774, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 640ms, time_since_start: 06h 09m 14s 756ms, eta: 01h 29m 30s 211ms\n",
      "\u001b[32m2021-05-03T10:01:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T10:01:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T10:01:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T10:02:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T10:02:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0770, train/total_loss: 0.0001, train/total_loss/avg: 0.0770, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 44s 161ms, time_since_start: 06h 11m 58s 918ms, eta: 02h 18m 59s 419ms\n",
      "\u001b[32m2021-05-03T10:02:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T10:02:25 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:02:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:02:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T10:03:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T10:03:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T10:04:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T10:04:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 3.4223, val/total_loss: 3.4223, val/hateful_memes/accuracy: 0.6200, val/hateful_memes/binary_f1: 0.4837, val/hateful_memes/roc_auc: 0.6647, num_updates: 17000, epoch: 59, iterations: 17000, max_updates: 22000, val_time: 01m 37s 287ms, best_update: 14000, best_iteration: 14000, best_val/hateful_memes/roc_auc: 0.667496\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:05:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:05:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T10:07:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0765, train/total_loss: 0.0001, train/total_loss/avg: 0.0765, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0., ups: 0.55, time: 03m 03s 404ms, time_since_start: 06h 16m 39s 612ms, eta: 02h 32m 10s 614ms\n",
      "\u001b[32m2021-05-03T10:08:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0761, train/total_loss: 0.0001, train/total_loss/avg: 0.0761, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0., ups: 1.00, time: 01m 40s 675ms, time_since_start: 06h 18m 20s 288ms, eta: 01h 21m 49s 727ms\n",
      "\u001b[32m2021-05-03T10:10:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0757, train/total_loss: 0.0001, train/total_loss/avg: 0.0757, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 240ms, time_since_start: 06h 20m 04s 528ms, eta: 01h 22m 57s 696ms\n",
      "\u001b[32m2021-05-03T10:12:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0752, train/total_loss: 0.0001, train/total_loss/avg: 0.0752, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 798ms, time_since_start: 06h 21m 50s 327ms, eta: 01h 22m 24s 600ms\n",
      "\u001b[32m2021-05-03T10:14:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0748, train/total_loss: 0.0001, train/total_loss/avg: 0.0748, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 531ms, time_since_start: 06h 23m 34s 858ms, eta: 01h 19m 39s 166ms\n",
      "\u001b[32m2021-05-03T10:15:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0744, train/total_loss: 0.0001, train/total_loss/avg: 0.0744, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 198ms, time_since_start: 06h 25m 19s 057ms, eta: 01h 17m 38s 109ms\n",
      "\u001b[32m2021-05-03T10:17:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0739, train/total_loss: 0.0001, train/total_loss/avg: 0.0739, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 781ms, time_since_start: 06h 27m 04s 838ms, eta: 01h 17m 01s 363ms\n",
      "\u001b[32m2021-05-03T10:19:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0735, train/total_loss: 0.0001, train/total_loss/avg: 0.0735, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 678ms, time_since_start: 06h 28m 49s 517ms, eta: 01h 14m 26s 845ms\n",
      "\u001b[32m2021-05-03T10:21:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0731, train/total_loss: 0.0001, train/total_loss/avg: 0.0731, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 325ms, time_since_start: 06h 30m 33s 843ms, eta: 01h 12m 25s 796ms\n",
      "\u001b[32m2021-05-03T10:22:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T10:22:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T10:23:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T10:23:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T10:23:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0727, train/total_loss: 0.0001, train/total_loss/avg: 0.0727, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0., ups: 0.60, time: 02m 46s 916ms, time_since_start: 06h 33m 20s 759ms, eta: 01h 53m 03s 483ms\n",
      "\u001b[32m2021-05-03T10:23:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T10:23:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:23:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:23:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T10:24:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T10:24:40 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T10:25:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T10:26:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T10:26:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 3.8340, val/total_loss: 3.8340, val/hateful_memes/accuracy: 0.6120, val/hateful_memes/binary_f1: 0.4457, val/hateful_memes/roc_auc: 0.6717, num_updates: 18000, epoch: 63, iterations: 18000, max_updates: 22000, val_time: 02m 42s 042ms, best_update: 18000, best_iteration: 18000, best_val/hateful_memes/roc_auc: 0.671665\n",
      "\u001b[32m2021-05-03T10:29:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0723, train/total_loss: 0.0001, train/total_loss/avg: 0.0723, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0., ups: 0.55, time: 03m 03s 289ms, time_since_start: 06h 39m 06s 094ms, eta: 02h 01m 02s 655ms\n",
      "\u001b[32m2021-05-03T10:31:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0719, train/total_loss: 0.0001, train/total_loss/avg: 0.0719, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 096ms, time_since_start: 06h 40m 44s 190ms, eta: 01h 03m 07s 299ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:31:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:31:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T10:32:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0715, train/total_loss: 0.0000, train/total_loss/avg: 0.0715, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 008ms, time_since_start: 06h 42m 30s 199ms, eta: 01h 06m 25s 086ms\n",
      "\u001b[32m2021-05-03T10:34:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0711, train/total_loss: 0.0001, train/total_loss/avg: 0.0711, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 793ms, time_since_start: 06h 44m 14s 993ms, eta: 01h 03m 52s 935ms\n",
      "\u001b[32m2021-05-03T10:36:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0707, train/total_loss: 0.0001, train/total_loss/avg: 0.0707, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0., ups: 0.93, time: 01m 48s 993ms, time_since_start: 06h 46m 03s 986ms, eta: 01h 04m 35s 823ms\n",
      "\u001b[32m2021-05-03T10:38:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0704, train/total_loss: 0.0001, train/total_loss/avg: 0.0704, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0., ups: 1.01, time: 01m 39s 171ms, time_since_start: 06h 47m 43s 158ms, eta: 57m 05s 789ms\n",
      "\u001b[32m2021-05-03T10:39:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0700, train/total_loss: 0.0000, train/total_loss/avg: 0.0700, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 773ms, time_since_start: 06h 49m 27s 931ms, eta: 58m 32s 832ms\n",
      "\u001b[32m2021-05-03T10:41:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0696, train/total_loss: 0.0000, train/total_loss/avg: 0.0696, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0., ups: 0.93, time: 01m 47s 631ms, time_since_start: 06h 51m 15s 563ms, eta: 58m 19s 314ms\n",
      "\u001b[32m2021-05-03T10:43:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0693, train/total_loss: 0.0000, train/total_loss/avg: 0.0693, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 581ms, time_since_start: 06h 52m 54s 145ms, eta: 51m 44s 936ms\n",
      "\u001b[32m2021-05-03T10:45:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T10:45:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T10:45:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T10:46:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T10:46:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0689, train/total_loss: 0.0001, train/total_loss/avg: 0.0689, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0., ups: 0.62, time: 02m 41s 971ms, time_since_start: 06h 55m 36s 117ms, eta: 01h 22m 16s 900ms\n",
      "\u001b[32m2021-05-03T10:46:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T10:46:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:46:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:46:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T10:46:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T10:47:02 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T10:47:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T10:48:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T10:48:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.7088, val/total_loss: 3.7088, val/hateful_memes/accuracy: 0.6100, val/hateful_memes/binary_f1: 0.4507, val/hateful_memes/roc_auc: 0.6777, num_updates: 19000, epoch: 66, iterations: 19000, max_updates: 22000, val_time: 02m 10s 749ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.677678\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:50:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:50:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T10:51:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0685, train/total_loss: 0.0001, train/total_loss/avg: 0.0685, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0., ups: 0.54, time: 03m 04s 702ms, time_since_start: 07h 51s 571ms, eta: 01h 30m 42s 075ms\n",
      "\u001b[32m2021-05-03T10:52:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0682, train/total_loss: 0.0000, train/total_loss/avg: 0.0682, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0., ups: 1.03, time: 01m 37s 861ms, time_since_start: 07h 02m 29s 433ms, eta: 46m 23s 978ms\n",
      "\u001b[32m2021-05-03T10:54:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0678, train/total_loss: 0.0000, train/total_loss/avg: 0.0678, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 566ms, time_since_start: 07h 04m 07s 999ms, eta: 45m 03s 863ms\n",
      "\u001b[32m2021-05-03T10:56:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0675, train/total_loss: 0.0000, train/total_loss/avg: 0.0675, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 122ms, time_since_start: 07h 05m 50s 122ms, eta: 44m 57s 673ms\n",
      "\u001b[32m2021-05-03T10:57:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0671, train/total_loss: 0.0000, train/total_loss/avg: 0.0671, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 169ms, time_since_start: 07h 07m 33s 291ms, eta: 43m 40s 508ms\n",
      "\u001b[32m2021-05-03T10:59:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0668, train/total_loss: 0.0000, train/total_loss/avg: 0.0668, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 673ms, time_since_start: 07h 09m 17s 965ms, eta: 42m 32s 350ms\n",
      "\u001b[32m2021-05-03T11:01:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0664, train/total_loss: 0.0000, train/total_loss/avg: 0.0664, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 460ms, time_since_start: 07h 11m 04s 426ms, eta: 41m 27s 780ms\n",
      "\u001b[32m2021-05-03T11:03:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0661, train/total_loss: 0.0000, train/total_loss/avg: 0.0661, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 611ms, time_since_start: 07h 12m 49s 037ms, eta: 38m 58s 282ms\n",
      "\u001b[32m2021-05-03T11:05:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0658, train/total_loss: 0.0000, train/total_loss/avg: 0.0658, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 213ms, time_since_start: 07h 14m 34s 251ms, eta: 37m 24s 829ms\n",
      "\u001b[32m2021-05-03T11:06:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T11:06:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T11:07:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T11:07:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T11:07:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0654, train/total_loss: 0.0000, train/total_loss/avg: 0.0654, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0., ups: 0.60, time: 02m 46s 868ms, time_since_start: 07h 17m 21s 119ms, eta: 56m 30s 768ms\n",
      "\u001b[32m2021-05-03T11:07:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T11:07:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:07:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:07:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T11:08:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T11:08:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T11:09:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T11:09:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 3.5399, val/total_loss: 3.5399, val/hateful_memes/accuracy: 0.6120, val/hateful_memes/binary_f1: 0.4757, val/hateful_memes/roc_auc: 0.6761, num_updates: 20000, epoch: 70, iterations: 20000, max_updates: 22000, val_time: 01m 32s 248ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.677678\n",
      "\u001b[32m2021-05-03T11:11:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0651, train/total_loss: 0.0000, train/total_loss/avg: 0.0651, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 0.67, time: 02m 30s 589ms, time_since_start: 07h 21m 23s 960ms, eta: 48m 26s 989ms\n",
      "\u001b[32m2021-05-03T11:13:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0648, train/total_loss: 0.0000, train/total_loss/avg: 0.0648, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.03, time: 01m 37s 705ms, time_since_start: 07h 23m 01s 666ms, eta: 29m 46s 841ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:13:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:13:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T11:15:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0645, train/total_loss: 0.0000, train/total_loss/avg: 0.0645, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 123ms, time_since_start: 07h 24m 44s 790ms, eta: 29m 41s 159ms\n",
      "\u001b[32m2021-05-03T11:16:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0642, train/total_loss: 0.0000, train/total_loss/avg: 0.0642, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 716ms, time_since_start: 07h 26m 29s 507ms, eta: 28m 22s 278ms\n",
      "\u001b[32m2021-05-03T11:18:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0639, train/total_loss: 0.0000, train/total_loss/avg: 0.0639, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 530ms, time_since_start: 07h 28m 14s 038ms, eta: 26m 33s 052ms\n",
      "\u001b[32m2021-05-03T11:20:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0635, train/total_loss: 0.0000, train/total_loss/avg: 0.0635, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 855ms, time_since_start: 07h 29m 56s 894ms, eta: 24m 23s 020ms\n",
      "\u001b[32m2021-05-03T11:22:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0632, train/total_loss: 0.0000, train/total_loss/avg: 0.0632, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.00, time: 01m 40s 038ms, time_since_start: 07h 31m 36s 932ms, eta: 22m 01s 307ms\n",
      "\u001b[32m2021-05-03T11:23:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0629, train/total_loss: 0.0000, train/total_loss/avg: 0.0629, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 282ms, time_since_start: 07h 33m 22s 215ms, eta: 21m 23s 603ms\n",
      "\u001b[32m2021-05-03T11:25:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0626, train/total_loss: 0.0000, train/total_loss/avg: 0.0626, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 621ms, time_since_start: 07h 35m 07s 836ms, eta: 19m 40s 426ms\n",
      "\u001b[32m2021-05-03T11:27:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T11:27:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T11:27:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T11:28:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T11:28:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0623, train/total_loss: 0.0000, train/total_loss/avg: 0.0623, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 45s 176ms, time_since_start: 07h 37m 53s 013ms, eta: 27m 58s 189ms\n",
      "\u001b[32m2021-05-03T11:28:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T11:28:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:28:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:28:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T11:28:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T11:29:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T11:29:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T11:29:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 3.8786, val/total_loss: 3.8786, val/hateful_memes/accuracy: 0.6140, val/hateful_memes/binary_f1: 0.4406, val/hateful_memes/roc_auc: 0.6720, num_updates: 21000, epoch: 73, iterations: 21000, max_updates: 22000, val_time: 01m 36s 073ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.677678\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:32:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:32:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T11:32:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0620, train/total_loss: 0.0000, train/total_loss/avg: 0.0620, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 0.64, time: 02m 37s 052ms, time_since_start: 07h 42m 06s 144ms, eta: 23m 56s 090ms\n",
      "\u001b[32m2021-05-03T11:34:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0618, train/total_loss: 0.0000, train/total_loss/avg: 0.0618, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 588ms, time_since_start: 07h 43m 48s 732ms, eta: 13m 53s 835ms\n",
      "\u001b[32m2021-05-03T11:35:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0615, train/total_loss: 0.0000, train/total_loss/avg: 0.0615, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 373ms, time_since_start: 07h 45m 33s 105ms, eta: 12m 22s 306ms\n",
      "\u001b[32m2021-05-03T11:37:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0612, train/total_loss: 0.0000, train/total_loss/avg: 0.0612, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 0.93, time: 01m 47s 385ms, time_since_start: 07h 47m 20s 491ms, eta: 10m 54s 621ms\n",
      "\u001b[32m2021-05-03T11:39:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0609, train/total_loss: 0.0000, train/total_loss/avg: 0.0609, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 311ms, time_since_start: 07h 49m 03s 802ms, eta: 08m 44s 821ms\n",
      "\u001b[32m2021-05-03T11:41:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0606, train/total_loss: 0.0000, train/total_loss/avg: 0.0606, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 819ms, time_since_start: 07h 50m 48s 622ms, eta: 07m 05s 985ms\n",
      "\u001b[32m2021-05-03T11:43:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0603, train/total_loss: 0.0000, train/total_loss/avg: 0.0603, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 994ms, time_since_start: 07h 52m 35s 616ms, eta: 05m 26s 119ms\n",
      "\u001b[32m2021-05-03T11:44:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0601, train/total_loss: 0.0000, train/total_loss/avg: 0.0601, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 016ms, time_since_start: 07h 54m 18s 633ms, eta: 03m 29s 330ms\n",
      "\u001b[32m2021-05-03T11:46:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0598, train/total_loss: 0.0000, train/total_loss/avg: 0.0598, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 818ms, time_since_start: 07h 56m 03s 452ms, eta: 01m 46s 495ms\n",
      "\u001b[32m2021-05-03T11:48:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T11:48:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T11:48:40 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T11:49:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T11:49:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0595, train/total_loss: 0.0000, train/total_loss/avg: 0.0595, max mem: 10794.0, experiment: run, epoch: 77, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 0.60, time: 02m 46s 919ms, time_since_start: 07h 58m 50s 372ms, eta: 0ms\n",
      "\u001b[32m2021-05-03T11:49:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T11:49:16 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:49:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:49:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T11:49:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T11:50:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T11:50:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T11:50:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 3.7122, val/total_loss: 3.7122, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.4637, val/hateful_memes/roc_auc: 0.6733, num_updates: 22000, epoch: 77, iterations: 22000, max_updates: 22000, val_time: 01m 26s 517ms, best_update: 19000, best_iteration: 19000, best_val/hateful_memes/roc_auc: 0.677678\n",
      "\u001b[32m2021-05-03T11:50:44 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2021-05-03T11:50:44 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2021-05-03T11:50:44 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2021-05-03T11:51:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-03T11:51:55 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 19000\n",
      "\u001b[32m2021-05-03T11:51:55 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 19000\n",
      "\u001b[32m2021-05-03T11:51:55 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 66\n",
      "\u001b[32m2021-05-03T11:52:02 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-03T11:52:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:52:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T11:52:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "100% 16/16 [00:35<00:00,  2.24s/it]\n",
      "\u001b[32m2021-05-03T11:52:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.7088, val/total_loss: 3.7088, val/hateful_memes/accuracy: 0.6100, val/hateful_memes/binary_f1: 0.4507, val/hateful_memes/roc_auc: 0.6777\n",
      "\u001b[32m2021-05-03T11:52:38 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 08h 02m 12s 365ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/vilbert/defaults.yaml \\\n",
    "  model=vilbert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=train_val \\\n",
    "  training.batch_size=32 \\\n",
    "  env.save_dir=/content/gdrive/MyDrive/colab/pretrained_vilbert_election_memes/ \\\n",
    "  checkpoint.resume_zoo=vilbert.pretrained.vqa2 \\\n",
    "  checkpoint.resume_pretrained=True \\\n",
    "  dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1abUWXmMW8J9"
   },
   "source": [
    "Same thing for Visual BERT (*unimodal* pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2297712,
     "status": "ok",
     "timestamp": 1620081610626,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "tk6Qo9Z9rp54",
    "outputId": "a9e6cf9b-abf7-4c6b-b77d-475dc559f0de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-03 22:02:00.145349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-03T22:02:41 | matplotlib.font_manager: \u001b[0mGenerating new fontManager, this may take some time...\n",
      "\u001b[32m2021-05-03T22:03:02 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/direct.yaml\n",
      "\u001b[32m2021-05-03T22:03:02 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2021-05-03T22:03:02 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-03T22:03:02 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2021-05-03T22:03:02 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.direct\n",
      "\u001b[32m2021-05-03T22:03:02 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
      "\u001b[32m2021-05-03T22:03:03 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2021-05-03T22:03:03 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/direct.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct', 'checkpoint.resume_pretrained=False'])\n",
      "\u001b[32m2021-05-03T22:03:03 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-03T22:03:03 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-03T22:03:03 | mmf_cli.run: \u001b[0mUsing seed 2743257\n",
      "\u001b[32m2021-05-03T22:03:03 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100% 10.3G/10.3G [29:31<00:00, 5.81MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n",
      "Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 168kB/s] \n",
      "[ Starting checksum for extras.tar.gz]\n",
      "[ Checksum successful for extras.tar.gz]\n",
      "Unpacking extras.tar.gz\n",
      "\u001b[32m2021-05-03T22:38:30 | filelock: \u001b[0mLock 140703874967120 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "Downloading: 100% 433/433 [00:00<00:00, 348kB/s]\n",
      "\u001b[32m2021-05-03T22:38:30 | filelock: \u001b[0mLock 140703874967120 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "\u001b[32m2021-05-03T22:38:31 | filelock: \u001b[0mLock 140703903807248 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 676kB/s]\n",
      "\u001b[32m2021-05-03T22:38:32 | filelock: \u001b[0mLock 140703903807248 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:38:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:38:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T22:38:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T22:38:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T22:38:32 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T22:38:32 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2021-05-03T22:38:32 | filelock: \u001b[0mLock 140703869024208 acquired on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Downloading: 100% 440M/440M [00:06<00:00, 65.7MB/s]\n",
      "\u001b[32m2021-05-03T22:38:38 | filelock: \u001b[0mLock 140703869024208 released on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-03T22:38:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-03T22:38:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:38:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:38:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-03T22:38:48 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.finetuned.hateful_memes_direct.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.finetuned.hateful_memes.direct/visual_bert.finetuned.hateful_memes_direct.tar.gz ]\n",
      "Downloading visual_bert.finetuned.hateful_memes_direct.tar.gz: 100% 415M/415M [00:34<00:00, 11.9MB/s]\n",
      "[ Starting checksum for visual_bert.finetuned.hateful_memes_direct.tar.gz]\n",
      "[ Checksum successful for visual_bert.finetuned.hateful_memes_direct.tar.gz]\n",
      "Unpacking visual_bert.finetuned.hateful_memes_direct.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:39:31 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:39:31 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:39:31 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:39:31 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:39:31 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
      "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
      "Unexpected keys if any: []\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:39:31 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:39:31 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:39:31 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:39:31 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2021-05-03T22:39:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-03T22:39:31 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-03T22:39:31 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-03T22:39:31 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-03T22:39:31 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-03T22:39:31 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-03T22:39:31 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2021-05-03T22:39:31 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-03T22:39:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100% 5/5 [00:36<00:00,  7.29s/it]\n",
      "\u001b[32m2021-05-03T22:40:08 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 1.2431, val/total_loss: 1.2431, val/hateful_memes/accuracy: 0.6685, val/hateful_memes/binary_f1: 0.4131, val/hateful_memes/roc_auc: 0.6955\n",
      "\u001b[32m2021-05-03T22:40:08 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01m 19s 385ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/direct.yaml \\\n",
    "  model=visual_bert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=val \\\n",
    "  checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct \\\n",
    "  checkpoint.resume_pretrained=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24554879,
     "status": "ok",
     "timestamp": 1620103867799,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "iEHoK52MXT6i",
    "outputId": "6b665927-b9d7-4915-f39e-1b7ba047f905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-03 22:40:14.767144: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/direct.yaml\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/gdrive/MyDrive/colab/pretrained_visualbert_election_memes/\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.vqa2.full\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to /content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_seen.jsonl\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.train[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.val[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.test[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf: \u001b[0mLogging to: /content/gdrive/MyDrive/colab/pretrained_visualbert_election_memes/train.log\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/direct.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.batch_size=32', 'env.save_dir=/content/gdrive/MyDrive/colab/pretrained_visualbert_election_memes/', 'checkpoint.resume_zoo=visual_bert.pretrained.vqa2.full', 'checkpoint.resume_pretrained=True', 'dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl', 'dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb'])\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf_cli.run: \u001b[0mUsing seed 21106948\n",
      "\u001b[32m2021-05-03T22:40:21 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:40:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:40:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T22:40:25 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T22:40:25 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T22:40:25 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T22:40:25 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-03T22:40:31 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-03T22:40:31 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:40:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:40:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-03T22:40:31 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.vqa2.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.vqa2.full/visual_bert.pretrained.vqa2.tar.gz ]\n",
      "Downloading visual_bert.pretrained.vqa2.tar.gz: 100% 414M/414M [00:34<00:00, 12.1MB/s]\n",
      "[ Starting checksum for visual_bert.pretrained.vqa2.tar.gz]\n",
      "[ Checksum successful for visual_bert.pretrained.vqa2.tar.gz]\n",
      "Unpacking visual_bert.pretrained.vqa2.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:41:13 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:41:13 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:41:13 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:41:13 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2021-05-03T22:41:13 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2021-05-03T22:46:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.5080, train/hateful_memes/cross_entropy/avg: 0.5080, train/total_loss: 0.5080, train/total_loss/avg: 0.5080, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 0.30, time: 05m 32s 351ms, time_since_start: 06m 13s 826ms, eta: 20h 32m 29s 551ms\n",
      "\u001b[32m2021-05-03T22:48:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.5080, train/hateful_memes/cross_entropy/avg: 0.5473, train/total_loss: 0.5080, train/total_loss/avg: 0.5473, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 183ms, time_since_start: 07m 44s 010ms, eta: 05h 32m 54s 601ms\n",
      "\u001b[32m2021-05-03T22:49:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.5866, train/hateful_memes/cross_entropy/avg: 0.5785, train/total_loss: 0.5866, train/total_loss/avg: 0.5785, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.09, time: 01m 32s 819ms, time_since_start: 09m 16s 829ms, eta: 05h 41m 04s 075ms\n",
      "\u001b[32m2021-05-03T22:51:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.5866, train/hateful_memes/cross_entropy/avg: 0.5819, train/total_loss: 0.5866, train/total_loss/avg: 0.5819, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 136ms, time_since_start: 10m 44s 965ms, eta: 05h 22m 22s 033ms\n",
      "\u001b[32m2021-05-03T22:52:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.5920, train/hateful_memes/cross_entropy/avg: 0.6061, train/total_loss: 0.5920, train/total_loss/avg: 0.6061, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 375ms, time_since_start: 12m 15s 341ms, eta: 05h 29m 01s 614ms\n",
      "\u001b[32m2021-05-03T22:54:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.5866, train/hateful_memes/cross_entropy/avg: 0.5745, train/total_loss: 0.5866, train/total_loss/avg: 0.5745, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 253ms, time_since_start: 13m 46s 595ms, eta: 05h 30m 40s 780ms\n",
      "\u001b[32m2021-05-03T22:55:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.5866, train/hateful_memes/cross_entropy/avg: 0.5543, train/total_loss: 0.5866, train/total_loss/avg: 0.5543, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 1.15, time: 01m 27s 382ms, time_since_start: 15m 13s 978ms, eta: 05h 15m 10s 365ms\n",
      "\u001b[32m2021-05-03T22:57:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.5080, train/hateful_memes/cross_entropy/avg: 0.5415, train/total_loss: 0.5080, train/total_loss/avg: 0.5415, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 938ms, time_since_start: 16m 43s 917ms, eta: 05h 22m 52s 115ms\n",
      "\u001b[32m2021-05-03T22:58:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.5080, train/hateful_memes/cross_entropy/avg: 0.5191, train/total_loss: 0.5080, train/total_loss/avg: 0.5191, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 207ms, time_since_start: 18m 15s 124ms, eta: 05h 25m 52s 665ms\n",
      "\u001b[32m2021-05-03T23:00:16 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T23:00:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:05:37 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:06:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:06:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.5080, train/hateful_memes/cross_entropy/avg: 0.5304, train/total_loss: 0.5080, train/total_loss/avg: 0.5304, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 0.23, time: 07m 22s 896ms, time_since_start: 25m 38s 020ms, eta: 26h 14m 56s 330ms\n",
      "\u001b[32m2021-05-03T23:06:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T23:06:09 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:06:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:06:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:06:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:06:54 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T23:07:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:07:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:07:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.8866, val/total_loss: 0.8866, val/hateful_memes/accuracy: 0.6260, val/hateful_memes/binary_f1: 0.5360, val/hateful_memes/roc_auc: 0.7009, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 01m 29s 912ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.700933\n",
      "\u001b[32m2021-05-03T23:09:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.5414, train/hateful_memes/cross_entropy/avg: 0.5314, train/total_loss: 0.5414, train/total_loss/avg: 0.5314, max mem: 9226.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 0.86, time: 01m 56s 167ms, time_since_start: 29m 04s 116ms, eta: 06h 51m 07s 563ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:10:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:10:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:11:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.5080, train/hateful_memes/cross_entropy/avg: 0.5030, train/total_loss: 0.5080, train/total_loss/avg: 0.5030, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 303ms, time_since_start: 30m 34s 419ms, eta: 05h 18m 03s 619ms\n",
      "\u001b[32m2021-05-03T23:12:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.5080, train/hateful_memes/cross_entropy/avg: 0.4889, train/total_loss: 0.5080, train/total_loss/avg: 0.4889, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 728ms, time_since_start: 32m 03s 147ms, eta: 05h 11m 629ms\n",
      "\u001b[32m2021-05-03T23:14:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.5080, train/hateful_memes/cross_entropy/avg: 0.4975, train/total_loss: 0.5080, train/total_loss/avg: 0.4975, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 196ms, time_since_start: 33m 31s 344ms, eta: 05h 07m 39s 145ms\n",
      "\u001b[32m2021-05-03T23:15:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.5080, train/hateful_memes/cross_entropy/avg: 0.4723, train/total_loss: 0.5080, train/total_loss/avg: 0.4723, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 578ms, time_since_start: 35m 02s 922ms, eta: 05h 17m 53s 965ms\n",
      "\u001b[32m2021-05-03T23:17:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.4522, train/hateful_memes/cross_entropy/avg: 0.4515, train/total_loss: 0.4522, train/total_loss/avg: 0.4515, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 288ms, time_since_start: 36m 31s 211ms, eta: 05h 04m 59s 050ms\n",
      "\u001b[32m2021-05-03T23:18:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.4522, train/hateful_memes/cross_entropy/avg: 0.4440, train/total_loss: 0.4522, train/total_loss/avg: 0.4440, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 1.15, time: 01m 27s 746ms, time_since_start: 37m 58s 957ms, eta: 05h 01m 37s 448ms\n",
      "\u001b[32m2021-05-03T23:20:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.4330, train/hateful_memes/cross_entropy/avg: 0.4245, train/total_loss: 0.4330, train/total_loss/avg: 0.4245, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 269ms, time_since_start: 39m 30s 226ms, eta: 05h 12m 11s 347ms\n",
      "\u001b[32m2021-05-03T23:21:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.4330, train/hateful_memes/cross_entropy/avg: 0.4121, train/total_loss: 0.4330, train/total_loss/avg: 0.4121, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 513ms, time_since_start: 40m 58s 740ms, eta: 05h 01m 15s 904ms\n",
      "\u001b[32m2021-05-03T23:22:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T23:22:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:23:20 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:23:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:23:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.4163, train/hateful_memes/cross_entropy/avg: 0.3947, train/total_loss: 0.4163, train/total_loss/avg: 0.3947, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 0.79, time: 02m 06s 121ms, time_since_start: 43m 04s 861ms, eta: 07h 07m 07s 802ms\n",
      "\u001b[32m2021-05-03T23:23:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T23:23:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:23:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:23:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:23:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:24:05 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T23:24:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:24:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:24:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.1076, val/total_loss: 1.1076, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.5823, val/hateful_memes/roc_auc: 0.7157, num_updates: 2000, epoch: 7, iterations: 2000, max_updates: 22000, val_time: 56s 965ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.715687\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:25:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:25:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:26:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.3394, train/hateful_memes/cross_entropy/avg: 0.3813, train/total_loss: 0.3394, train/total_loss/avg: 0.3813, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 0.78, time: 02m 08s 707ms, time_since_start: 46m 10s 539ms, eta: 07h 13m 42s 696ms\n",
      "\u001b[32m2021-05-03T23:28:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.3249, train/hateful_memes/cross_entropy/avg: 0.3688, train/total_loss: 0.3249, train/total_loss/avg: 0.3688, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 1.12, time: 01m 29s 501ms, time_since_start: 47m 40s 041ms, eta: 05h 04s 941ms\n",
      "\u001b[32m2021-05-03T23:29:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.3198, train/hateful_memes/cross_entropy/avg: 0.3541, train/total_loss: 0.3198, train/total_loss/avg: 0.3541, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 798ms, time_since_start: 49m 10s 839ms, eta: 05h 02m 53s 508ms\n",
      "\u001b[32m2021-05-03T23:31:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.1908, train/hateful_memes/cross_entropy/avg: 0.3417, train/total_loss: 0.1908, train/total_loss/avg: 0.3417, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 784ms, time_since_start: 50m 41s 624ms, eta: 05h 01m 18s 463ms\n",
      "\u001b[32m2021-05-03T23:32:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.1889, train/hateful_memes/cross_entropy/avg: 0.3306, train/total_loss: 0.1889, train/total_loss/avg: 0.3306, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 924ms, time_since_start: 52m 10s 548ms, eta: 04h 53m 37s 681ms\n",
      "\u001b[32m2021-05-03T23:34:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.1389, train/hateful_memes/cross_entropy/avg: 0.3183, train/total_loss: 0.1389, train/total_loss/avg: 0.3183, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 453ms, time_since_start: 53m 39s 002ms, eta: 04h 50m 34s 511ms\n",
      "\u001b[32m2021-05-03T23:35:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.1389, train/hateful_memes/cross_entropy/avg: 0.3125, train/total_loss: 0.1389, train/total_loss/avg: 0.3125, max mem: 9226.0, experiment: run, epoch: 10, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 323ms, time_since_start: 55m 09s 325ms, eta: 04h 55m 11s 272ms\n",
      "\u001b[32m2021-05-03T23:37:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.1203, train/hateful_memes/cross_entropy/avg: 0.3017, train/total_loss: 0.1203, train/total_loss/avg: 0.3017, max mem: 9226.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 698ms, time_since_start: 56m 38s 023ms, eta: 04h 48m 22s 627ms\n",
      "\u001b[32m2021-05-03T23:38:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.1125, train/hateful_memes/cross_entropy/avg: 0.2914, train/total_loss: 0.1125, train/total_loss/avg: 0.2914, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 1.09, time: 01m 32s 160ms, time_since_start: 58m 10s 184ms, eta: 04h 58m 04s 233ms\n",
      "\u001b[32m2021-05-03T23:40:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T23:40:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:40:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:40:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:40:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.1064, train/hateful_memes/cross_entropy/avg: 0.2836, train/total_loss: 0.1064, train/total_loss/avg: 0.2836, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 0.78, time: 02m 08s 387ms, time_since_start: 01h 18s 571ms, eta: 06h 53m 03s 831ms\n",
      "\u001b[32m2021-05-03T23:40:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T23:40:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:40:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:40:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:41:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:41:16 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T23:41:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:41:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:41:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.4387, val/total_loss: 1.4387, val/hateful_memes/accuracy: 0.6440, val/hateful_memes/binary_f1: 0.6197, val/hateful_memes/roc_auc: 0.7200, num_updates: 3000, epoch: 11, iterations: 3000, max_updates: 22000, val_time: 52s 788ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.720024\n",
      "\u001b[32m2021-05-03T23:43:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1064, train/hateful_memes/cross_entropy/avg: 0.2783, train/total_loss: 0.1064, train/total_loss/avg: 0.2783, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 0.76, time: 02m 11s 082ms, time_since_start: 01h 03m 22s 443ms, eta: 06h 59m 30s 972ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:45:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:45:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:45:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.1050, train/hateful_memes/cross_entropy/avg: 0.2729, train/total_loss: 0.1050, train/total_loss/avg: 0.2729, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 160ms, time_since_start: 01h 04m 53s 603ms, eta: 04h 50m 12s 341ms\n",
      "\u001b[32m2021-05-03T23:46:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0929, train/hateful_memes/cross_entropy/avg: 0.2647, train/total_loss: 0.0929, train/total_loss/avg: 0.2647, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 615ms, time_since_start: 01h 06m 22s 219ms, eta: 04h 40m 36s 237ms\n",
      "\u001b[32m2021-05-03T23:48:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0653, train/hateful_memes/cross_entropy/avg: 0.2572, train/total_loss: 0.0653, train/total_loss/avg: 0.2572, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 602ms, time_since_start: 01h 07m 50s 821ms, eta: 04h 39m 03s 776ms\n",
      "\u001b[32m2021-05-03T23:49:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0639, train/hateful_memes/cross_entropy/avg: 0.2499, train/total_loss: 0.0639, train/total_loss/avg: 0.2499, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 1.09, time: 01m 32s 017ms, time_since_start: 01h 09m 22s 839ms, eta: 04h 48m 15s 534ms\n",
      "\u001b[32m2021-05-03T23:51:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0566, train/hateful_memes/cross_entropy/avg: 0.2440, train/total_loss: 0.0566, train/total_loss/avg: 0.2440, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 513ms, time_since_start: 01h 10m 51s 353ms, eta: 04h 35m 47s 161ms\n",
      "\u001b[32m2021-05-03T23:52:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0563, train/hateful_memes/cross_entropy/avg: 0.2375, train/total_loss: 0.0563, train/total_loss/avg: 0.2375, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 1.12, time: 01m 29s 833ms, time_since_start: 01h 12m 21s 186ms, eta: 04h 38m 22s 511ms\n",
      "\u001b[32m2021-05-03T23:54:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0375, train/hateful_memes/cross_entropy/avg: 0.2317, train/total_loss: 0.0375, train/total_loss/avg: 0.2317, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 365ms, time_since_start: 01h 13m 52s 551ms, eta: 04h 41m 34s 519ms\n",
      "\u001b[32m2021-05-03T23:55:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0309, train/hateful_memes/cross_entropy/avg: 0.2258, train/total_loss: 0.0309, train/total_loss/avg: 0.2258, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 1.12, time: 01m 29s 236ms, time_since_start: 01h 15m 21s 787ms, eta: 04h 33m 30s 170ms\n",
      "\u001b[32m2021-05-03T23:57:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T23:57:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:57:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:58:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:58:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0159, train/hateful_memes/cross_entropy/avg: 0.2203, train/total_loss: 0.0159, train/total_loss/avg: 0.2203, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 0.76, time: 02m 12s 629ms, time_since_start: 01h 17m 34s 416ms, eta: 06h 44m 15s 208ms\n",
      "\u001b[32m2021-05-03T23:58:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T23:58:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:58:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:58:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:58:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:58:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:58:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:58:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.8420, val/total_loss: 1.8420, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5422, val/hateful_memes/roc_auc: 0.7166, num_updates: 4000, epoch: 14, iterations: 4000, max_updates: 22000, val_time: 34s 896ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.720024\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:59:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:59:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:00:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0124, train/hateful_memes/cross_entropy/avg: 0.2151, train/total_loss: 0.0124, train/total_loss/avg: 0.2151, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 1.05, time: 01m 35s 479ms, time_since_start: 01h 19m 44s 796ms, eta: 04h 49m 24s 296ms\n",
      "\u001b[32m2021-05-04T00:01:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0112, train/hateful_memes/cross_entropy/avg: 0.2101, train/total_loss: 0.0112, train/total_loss/avg: 0.2101, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 425ms, time_since_start: 01h 21m 13s 221ms, eta: 04h 26m 31s 618ms\n",
      "\u001b[32m2021-05-04T00:03:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0112, train/hateful_memes/cross_entropy/avg: 0.2059, train/total_loss: 0.0112, train/total_loss/avg: 0.2059, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 181ms, time_since_start: 01h 22m 42s 403ms, eta: 04h 27m 17s 696ms\n",
      "\u001b[32m2021-05-04T00:04:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0112, train/hateful_memes/cross_entropy/avg: 0.2023, train/total_loss: 0.0112, train/total_loss/avg: 0.2023, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 538ms, time_since_start: 01h 24m 13s 942ms, eta: 04h 32m 48s 622ms\n",
      "\u001b[32m2021-05-04T00:06:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0104, train/hateful_memes/cross_entropy/avg: 0.1980, train/total_loss: 0.0104, train/total_loss/avg: 0.1980, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 1.15, time: 01m 27s 384ms, time_since_start: 01h 25m 41s 326ms, eta: 04h 18m 56s 890ms\n",
      "\u001b[32m2021-05-04T00:07:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0112, train/hateful_memes/cross_entropy/avg: 0.1942, train/total_loss: 0.0112, train/total_loss/avg: 0.1942, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 819ms, time_since_start: 01h 27m 10s 145ms, eta: 04h 21m 41s 792ms\n",
      "\u001b[32m2021-05-04T00:09:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0096, train/hateful_memes/cross_entropy/avg: 0.1901, train/total_loss: 0.0096, train/total_loss/avg: 0.1901, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 522ms, time_since_start: 01h 28m 41s 667ms, eta: 04h 28m 06s 681ms\n",
      "\u001b[32m2021-05-04T00:10:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.1863, train/total_loss: 0.0069, train/total_loss/avg: 0.1863, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 974ms, time_since_start: 01h 30m 10s 642ms, eta: 04h 19m 08s 480ms\n",
      "\u001b[32m2021-05-04T00:12:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.1825, train/total_loss: 0.0069, train/total_loss/avg: 0.1825, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 691ms, time_since_start: 01h 31m 39s 334ms, eta: 04h 16m 48s 956ms\n",
      "\u001b[32m2021-05-04T00:13:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T00:13:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:13:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:14:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:14:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.1804, train/total_loss: 0.0069, train/total_loss/avg: 0.1804, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 0.87, time: 01m 55s 518ms, time_since_start: 01h 33m 34s 852ms, eta: 05h 32m 32s 345ms\n",
      "\u001b[32m2021-05-04T00:14:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T00:14:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:14:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:14:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:14:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:14:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:15:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:15:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 2.6096, val/total_loss: 2.6096, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.4146, val/hateful_memes/roc_auc: 0.7032, num_updates: 5000, epoch: 18, iterations: 5000, max_updates: 22000, val_time: 57s 875ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.720024\n",
      "\u001b[32m2021-05-04T00:16:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1769, train/total_loss: 0.0062, train/total_loss/avg: 0.1769, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 0.93, time: 01m 48s 196ms, time_since_start: 01h 36m 20s 932ms, eta: 05h 09m 37s 847ms\n",
      "\u001b[32m2021-05-04T00:18:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1737, train/total_loss: 0.0062, train/total_loss/avg: 0.1737, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 985ms, time_since_start: 01h 37m 49s 917ms, eta: 04h 13m 08s 676ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:18:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:18:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:19:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1704, train/total_loss: 0.0062, train/total_loss/avg: 0.1704, max mem: 9226.0, experiment: run, epoch: 19, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 286ms, time_since_start: 01h 39m 21s 203ms, eta: 04h 18m 08s 796ms\n",
      "\u001b[32m2021-05-04T00:21:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1676, train/total_loss: 0.0062, train/total_loss/avg: 0.1676, max mem: 9226.0, experiment: run, epoch: 19, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 323ms, time_since_start: 01h 40m 50s 527ms, eta: 04h 11m 04s 997ms\n",
      "\u001b[32m2021-05-04T00:22:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1647, train/total_loss: 0.0062, train/total_loss/avg: 0.1647, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 1.08, time: 01m 33s 265ms, time_since_start: 01h 42m 23s 793ms, eta: 04h 20m 35s 043ms\n",
      "\u001b[32m2021-05-04T00:24:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1621, train/total_loss: 0.0062, train/total_loss/avg: 0.1621, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 1.15, time: 01m 27s 665ms, time_since_start: 01h 43m 51s 459ms, eta: 04h 03m 27s 225ms\n",
      "\u001b[32m2021-05-04T00:25:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.1598, train/total_loss: 0.0069, train/total_loss/avg: 0.1598, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 543ms, time_since_start: 01h 45m 21s 002ms, eta: 04h 07m 09s 143ms\n",
      "\u001b[32m2021-05-04T00:27:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1571, train/total_loss: 0.0067, train/total_loss/avg: 0.1571, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 1.09, time: 01m 32s 320ms, time_since_start: 01h 46m 53s 323ms, eta: 04h 13m 15s 221ms\n",
      "\u001b[32m2021-05-04T00:28:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.1546, train/total_loss: 0.0069, train/total_loss/avg: 0.1546, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 748ms, time_since_start: 01h 48m 22s 072ms, eta: 04h 01m 57s 185ms\n",
      "\u001b[32m2021-05-04T00:30:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T00:30:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:30:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:30:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:30:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0069, train/hateful_memes/cross_entropy/avg: 0.1520, train/total_loss: 0.0069, train/total_loss/avg: 0.1520, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 0.87, time: 01m 55s 603ms, time_since_start: 01h 50m 17s 675ms, eta: 05h 13m 12s 463ms\n",
      "\u001b[32m2021-05-04T00:30:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T00:30:49 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:30:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:30:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:31:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:31:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:31:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:31:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.2729, val/total_loss: 2.2729, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5398, val/hateful_memes/roc_auc: 0.7038, num_updates: 6000, epoch: 21, iterations: 6000, max_updates: 22000, val_time: 38s 071ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.720024\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:32:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:32:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:33:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1496, train/total_loss: 0.0067, train/total_loss/avg: 0.1496, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 1.04, time: 01m 36s 336ms, time_since_start: 01h 52m 32s 087ms, eta: 04h 19m 22s 519ms\n",
      "\u001b[32m2021-05-04T00:34:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0081, train/hateful_memes/cross_entropy/avg: 0.1474, train/total_loss: 0.0081, train/total_loss/avg: 0.1474, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 1.11, time: 01m 30s 081ms, time_since_start: 01h 54m 02s 168ms, eta: 04h 01m 558ms\n",
      "\u001b[32m2021-05-04T00:36:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1451, train/total_loss: 0.0067, train/total_loss/avg: 0.1451, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 011ms, time_since_start: 01h 55m 33s 179ms, eta: 04h 01m 57s 398ms\n",
      "\u001b[32m2021-05-04T00:37:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1433, train/total_loss: 0.0067, train/total_loss/avg: 0.1433, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 1.09, time: 01m 32s 632ms, time_since_start: 01h 57m 05s 813ms, eta: 04h 04m 41s 972ms\n",
      "\u001b[32m2021-05-04T00:39:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1411, train/total_loss: 0.0062, train/total_loss/avg: 0.1411, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 1.11, time: 01m 30s 266ms, time_since_start: 01h 58m 36s 079ms, eta: 03h 56m 55s 118ms\n",
      "\u001b[32m2021-05-04T00:40:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.1390, train/total_loss: 0.0056, train/total_loss/avg: 0.1390, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 437ms, time_since_start: 02h 05s 516ms, eta: 03h 53m 13s 772ms\n",
      "\u001b[32m2021-05-04T00:42:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.1369, train/total_loss: 0.0056, train/total_loss/avg: 0.1369, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 471ms, time_since_start: 02h 01m 36s 988ms, eta: 03h 56m 59s 136ms\n",
      "\u001b[32m2021-05-04T00:43:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.1355, train/total_loss: 0.0056, train/total_loss/avg: 0.1355, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 961ms, time_since_start: 02h 03m 05s 950ms, eta: 03h 48m 58s 526ms\n",
      "\u001b[32m2021-05-04T00:45:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.1335, train/total_loss: 0.0056, train/total_loss/avg: 0.1335, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 1.11, time: 01m 30s 565ms, time_since_start: 02h 04m 36s 515ms, eta: 03h 51m 34s 139ms\n",
      "\u001b[32m2021-05-04T00:46:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T00:46:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:46:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:47:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:47:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.1331, train/total_loss: 0.0056, train/total_loss/avg: 0.1331, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 0.85, time: 01m 57s 357ms, time_since_start: 02h 06m 33s 873ms, eta: 04h 58m 05s 376ms\n",
      "\u001b[32m2021-05-04T00:47:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T00:47:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:47:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:47:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:47:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:47:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:47:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:47:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.6477, val/total_loss: 2.6477, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.4483, val/hateful_memes/roc_auc: 0.7128, num_updates: 7000, epoch: 25, iterations: 7000, max_updates: 22000, val_time: 43s 266ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.720024\n",
      "\u001b[32m2021-05-04T00:49:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1317, train/total_loss: 0.0067, train/total_loss/avg: 0.1317, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 1.06, time: 01m 34s 899ms, time_since_start: 02h 08m 52s 042ms, eta: 03h 59m 26s 244ms\n",
      "\u001b[32m2021-05-04T00:50:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.1299, train/total_loss: 0.0056, train/total_loss/avg: 0.1299, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 681ms, time_since_start: 02h 10m 21s 724ms, eta: 03h 44m 45s 257ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:51:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:51:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:52:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0056, train/hateful_memes/cross_entropy/avg: 0.1281, train/total_loss: 0.0056, train/total_loss/avg: 0.1281, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 1.09, time: 01m 32s 062ms, time_since_start: 02h 11m 53s 786ms, eta: 03h 49m 09s 678ms\n",
      "\u001b[32m2021-05-04T00:53:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0054, train/hateful_memes/cross_entropy/avg: 0.1264, train/total_loss: 0.0054, train/total_loss/avg: 0.1264, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 508ms, time_since_start: 02h 13m 23s 295ms, eta: 03h 41m 17s 371ms\n",
      "\u001b[32m2021-05-04T00:55:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1247, train/total_loss: 0.0024, train/total_loss/avg: 0.1247, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 468ms, time_since_start: 02h 14m 52s 764ms, eta: 03h 39m 40s 511ms\n",
      "\u001b[32m2021-05-04T00:56:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1233, train/total_loss: 0.0024, train/total_loss/avg: 0.1233, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 640ms, time_since_start: 02h 16m 24s 404ms, eta: 03h 43m 27s 364ms\n",
      "\u001b[32m2021-05-04T00:58:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1218, train/total_loss: 0.0013, train/total_loss/avg: 0.1218, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 545ms, time_since_start: 02h 17m 52s 950ms, eta: 03h 34m 24s 672ms\n",
      "\u001b[32m2021-05-04T00:59:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1203, train/total_loss: 0.0013, train/total_loss/avg: 0.1203, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 268ms, time_since_start: 02h 19m 22s 218ms, eta: 03h 34m 38s 895ms\n",
      "\u001b[32m2021-05-04T01:01:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1188, train/total_loss: 0.0013, train/total_loss/avg: 0.1188, max mem: 9226.0, experiment: run, epoch: 28, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 463ms, time_since_start: 02h 20m 53s 682ms, eta: 03h 38m 22s 680ms\n",
      "\u001b[32m2021-05-04T01:02:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T01:02:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:03:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:03:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:03:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1173, train/total_loss: 0.0013, train/total_loss/avg: 0.1173, max mem: 9226.0, experiment: run, epoch: 28, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 0.88, time: 01m 54s 021ms, time_since_start: 02h 22m 47s 703ms, eta: 04h 30m 18s 414ms\n",
      "\u001b[32m2021-05-04T01:03:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T01:03:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:03:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:03:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:03:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:03:45 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-04T01:03:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:04:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:04:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 1.9728, val/total_loss: 1.9728, val/hateful_memes/accuracy: 0.6520, val/hateful_memes/binary_f1: 0.5606, val/hateful_memes/roc_auc: 0.7389, num_updates: 8000, epoch: 28, iterations: 8000, max_updates: 22000, val_time: 54s 351ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.738890\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:06:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:06:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:06:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1159, train/total_loss: 0.0013, train/total_loss/avg: 0.1159, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 0.75, time: 02m 14s 209ms, time_since_start: 02h 25m 56s 278ms, eta: 05h 15m 53s 608ms\n",
      "\u001b[32m2021-05-04T01:07:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1145, train/total_loss: 0.0013, train/total_loss/avg: 0.1145, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 485ms, time_since_start: 02h 27m 24s 764ms, eta: 03h 26m 46s 338ms\n",
      "\u001b[32m2021-05-04T01:09:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1131, train/total_loss: 0.0013, train/total_loss/avg: 0.1131, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 167ms, time_since_start: 02h 28m 53s 931ms, eta: 03h 26m 51s 408ms\n",
      "\u001b[32m2021-05-04T01:10:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1118, train/total_loss: 0.0012, train/total_loss/avg: 0.1118, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 762ms, time_since_start: 02h 30m 25s 694ms, eta: 03h 31m 19s 400ms\n",
      "\u001b[32m2021-05-04T01:12:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1105, train/total_loss: 0.0012, train/total_loss/avg: 0.1105, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 1.15, time: 01m 27s 525ms, time_since_start: 02h 31m 53s 220ms, eta: 03h 20m 05s 056ms\n",
      "\u001b[32m2021-05-04T01:13:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1092, train/total_loss: 0.0013, train/total_loss/avg: 0.1092, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 954ms, time_since_start: 02h 33m 22s 174ms, eta: 03h 21m 50s 646ms\n",
      "\u001b[32m2021-05-04T01:15:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1080, train/total_loss: 0.0013, train/total_loss/avg: 0.1080, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 064ms, time_since_start: 02h 34m 54s 239ms, eta: 03h 27m 20s 491ms\n",
      "\u001b[32m2021-05-04T01:16:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1068, train/total_loss: 0.0013, train/total_loss/avg: 0.1068, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 196ms, time_since_start: 02h 36m 22s 436ms, eta: 03h 17m 08s 255ms\n",
      "\u001b[32m2021-05-04T01:18:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1056, train/total_loss: 0.0013, train/total_loss/avg: 0.1056, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 020ms, time_since_start: 02h 37m 51s 456ms, eta: 03h 17m 28s 242ms\n",
      "\u001b[32m2021-05-04T01:19:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T01:19:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:20:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:20:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:20:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1044, train/total_loss: 0.0013, train/total_loss/avg: 0.1044, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 0.88, time: 01m 54s 540ms, time_since_start: 02h 39m 45s 997ms, eta: 04h 12m 08s 511ms\n",
      "\u001b[32m2021-05-04T01:20:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T01:20:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:20:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:20:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:20:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:20:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:21:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:21:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.6957, val/total_loss: 2.6957, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5227, val/hateful_memes/roc_auc: 0.7171, num_updates: 9000, epoch: 32, iterations: 9000, max_updates: 22000, val_time: 48s 059ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.738890\n",
      "\u001b[32m2021-05-04T01:22:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1033, train/total_loss: 0.0013, train/total_loss/avg: 0.1033, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 1.06, time: 01m 34s 264ms, time_since_start: 02h 42m 08s 325ms, eta: 03h 25m 54s 741ms\n",
      "\u001b[32m2021-05-04T01:24:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1022, train/total_loss: 0.0013, train/total_loss/avg: 0.1022, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 152ms, time_since_start: 02h 43m 37s 478ms, eta: 03h 13m 14s 091ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:24:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:24:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:25:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1011, train/total_loss: 0.0013, train/total_loss/avg: 0.1011, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 735ms, time_since_start: 02h 45m 10s 213ms, eta: 03h 19m 25s 787ms\n",
      "\u001b[32m2021-05-04T01:27:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1001, train/total_loss: 0.0013, train/total_loss/avg: 0.1001, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 241ms, time_since_start: 02h 46m 39s 455ms, eta: 03h 10m 24s 407ms\n",
      "\u001b[32m2021-05-04T01:28:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0990, train/total_loss: 0.0013, train/total_loss/avg: 0.0990, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 353ms, time_since_start: 02h 48m 09s 809ms, eta: 03h 11m 14s 872ms\n",
      "\u001b[32m2021-05-04T01:30:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0980, train/total_loss: 0.0013, train/total_loss/avg: 0.0980, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 881ms, time_since_start: 02h 49m 41s 690ms, eta: 03h 12m 55s 550ms\n",
      "\u001b[32m2021-05-04T01:31:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0970, train/total_loss: 0.0013, train/total_loss/avg: 0.0970, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 425ms, time_since_start: 02h 51m 12s 115ms, eta: 03h 08m 20s 237ms\n",
      "\u001b[32m2021-05-04T01:33:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0960, train/total_loss: 0.0013, train/total_loss/avg: 0.0960, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 143ms, time_since_start: 02h 52m 44s 259ms, eta: 03h 10m 21s 395ms\n",
      "\u001b[32m2021-05-04T01:34:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0951, train/total_loss: 0.0010, train/total_loss/avg: 0.0951, max mem: 9226.0, experiment: run, epoch: 35, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 209ms, time_since_start: 02h 54m 15s 469ms, eta: 03h 06m 52s 955ms\n",
      "\u001b[32m2021-05-04T01:36:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T01:36:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:36:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:36:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:36:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0947, train/total_loss: 0.0010, train/total_loss/avg: 0.0947, max mem: 9226.0, experiment: run, epoch: 35, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 0.88, time: 01m 53s 783ms, time_since_start: 02h 56m 09s 253ms, eta: 03h 51m 12s 510ms\n",
      "\u001b[32m2021-05-04T01:36:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T01:36:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:36:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:36:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:37:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:37:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:37:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:37:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 1.8996, val/total_loss: 1.8996, val/hateful_memes/accuracy: 0.6480, val/hateful_memes/binary_f1: 0.5622, val/hateful_memes/roc_auc: 0.7204, num_updates: 10000, epoch: 35, iterations: 10000, max_updates: 22000, val_time: 43s 676ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.738890\n",
      "\u001b[32m2021-05-04T01:38:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0944, train/total_loss: 0.0013, train/total_loss/avg: 0.0944, max mem: 9226.0, experiment: run, epoch: 35, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 802ms, time_since_start: 02h 58m 25s 735ms, eta: 03h 07m 150ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:39:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:39:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:40:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0935, train/total_loss: 0.0013, train/total_loss/avg: 0.0935, max mem: 9226.0, experiment: run, epoch: 36, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 832ms, time_since_start: 02h 59m 56s 568ms, eta: 03h 01m 29s 759ms\n",
      "\u001b[32m2021-05-04T01:41:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0928, train/total_loss: 0.0013, train/total_loss/avg: 0.0928, max mem: 9226.0, experiment: run, epoch: 36, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 744ms, time_since_start: 03h 01m 26s 313ms, eta: 02h 57m 48s 116ms\n",
      "\u001b[32m2021-05-04T01:43:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.0919, train/total_loss: 0.0015, train/total_loss/avg: 0.0919, max mem: 9226.0, experiment: run, epoch: 36, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 744ms, time_since_start: 03h 02m 56s 057ms, eta: 02h 56m 16s 934ms\n",
      "\u001b[32m2021-05-04T01:45:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.0910, train/total_loss: 0.0015, train/total_loss/avg: 0.0910, max mem: 9226.0, experiment: run, epoch: 37, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 471ms, time_since_start: 03h 04m 28s 529ms, eta: 03h 04s 387ms\n",
      "\u001b[32m2021-05-04T01:46:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.0903, train/total_loss: 0.0015, train/total_loss/avg: 0.0903, max mem: 9226.0, experiment: run, epoch: 37, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 668ms, time_since_start: 03h 05m 57s 197ms, eta: 02h 51m 09s 928ms\n",
      "\u001b[32m2021-05-04T01:48:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.0895, train/total_loss: 0.0015, train/total_loss/avg: 0.0895, max mem: 9226.0, experiment: run, epoch: 38, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 1.08, time: 01m 33s 371ms, time_since_start: 03h 07m 30s 569ms, eta: 02h 58m 39s 792ms\n",
      "\u001b[32m2021-05-04T01:49:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0887, train/total_loss: 0.0013, train/total_loss/avg: 0.0887, max mem: 9226.0, experiment: run, epoch: 38, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 407ms, time_since_start: 03h 08m 58s 976ms, eta: 02h 47m 40s 055ms\n",
      "\u001b[32m2021-05-04T01:50:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0878, train/total_loss: 0.0013, train/total_loss/avg: 0.0878, max mem: 9226.0, experiment: run, epoch: 38, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 833ms, time_since_start: 03h 10m 27s 810ms, eta: 02h 46m 58s 317ms\n",
      "\u001b[32m2021-05-04T01:52:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T01:52:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:52:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:52:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:52:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0871, train/total_loss: 0.0013, train/total_loss/avg: 0.0871, max mem: 9226.0, experiment: run, epoch: 39, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 0.88, time: 01m 54s 981ms, time_since_start: 03h 12m 22s 791ms, eta: 03h 34m 10s 296ms\n",
      "\u001b[32m2021-05-04T01:52:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T01:52:54 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:52:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:52:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:53:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:53:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:53:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:53:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.0842, val/total_loss: 2.0842, val/hateful_memes/accuracy: 0.6340, val/hateful_memes/binary_f1: 0.5590, val/hateful_memes/roc_auc: 0.7261, num_updates: 11000, epoch: 39, iterations: 11000, max_updates: 22000, val_time: 44s 628ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.738890\n",
      "\u001b[32m2021-05-04T01:55:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0863, train/total_loss: 0.0009, train/total_loss/avg: 0.0863, max mem: 9226.0, experiment: run, epoch: 39, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 511ms, time_since_start: 03h 14m 38s 934ms, eta: 02h 48m 54s 384ms\n",
      "\u001b[32m2021-05-04T01:56:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0855, train/total_loss: 0.0007, train/total_loss/avg: 0.0855, max mem: 9226.0, experiment: run, epoch: 39, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 135ms, time_since_start: 03h 16m 08s 069ms, eta: 02h 43m 631ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:57:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:57:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:58:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0848, train/total_loss: 0.0009, train/total_loss/avg: 0.0848, max mem: 9226.0, experiment: run, epoch: 40, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 271ms, time_since_start: 03h 17m 40s 341ms, eta: 02h 47m 11s 014ms\n",
      "\u001b[32m2021-05-04T01:59:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0840, train/total_loss: 0.0007, train/total_loss/avg: 0.0840, max mem: 9226.0, experiment: run, epoch: 40, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 800ms, time_since_start: 03h 19m 09s 141ms, eta: 02h 39m 23s 485ms\n",
      "\u001b[32m2021-05-04T02:01:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0834, train/total_loss: 0.0007, train/total_loss/avg: 0.0834, max mem: 9226.0, experiment: run, epoch: 40, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 091ms, time_since_start: 03h 20m 39s 233ms, eta: 02h 40m 11s 011ms\n",
      "\u001b[32m2021-05-04T02:02:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0827, train/total_loss: 0.0009, train/total_loss/avg: 0.0827, max mem: 9226.0, experiment: run, epoch: 41, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 883ms, time_since_start: 03h 22m 10s 117ms, eta: 02h 40m 03s 131ms\n",
      "\u001b[32m2021-05-04T02:04:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0820, train/total_loss: 0.0007, train/total_loss/avg: 0.0820, max mem: 9226.0, experiment: run, epoch: 41, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 385ms, time_since_start: 03h 23m 39s 503ms, eta: 02h 35m 54s 045ms\n",
      "\u001b[32m2021-05-04T02:05:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0830, train/total_loss: 0.0009, train/total_loss/avg: 0.0830, max mem: 9226.0, experiment: run, epoch: 41, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 922ms, time_since_start: 03h 25m 08s 425ms, eta: 02h 33m 35s 194ms\n",
      "\u001b[32m2021-05-04T02:07:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0823, train/total_loss: 0.0009, train/total_loss/avg: 0.0823, max mem: 9226.0, experiment: run, epoch: 42, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 298ms, time_since_start: 03h 26m 40s 724ms, eta: 02h 37m 51s 355ms\n",
      "\u001b[32m2021-05-04T02:08:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T02:08:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:08:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:09:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:09:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0816, train/total_loss: 0.0007, train/total_loss/avg: 0.0816, max mem: 9226.0, experiment: run, epoch: 42, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 0.89, time: 01m 52s 140ms, time_since_start: 03h 28m 32s 865ms, eta: 03h 09m 53s 452ms\n",
      "\u001b[32m2021-05-04T02:09:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T02:09:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:09:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:09:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:09:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:09:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:09:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:09:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.3125, val/total_loss: 2.3125, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.5478, val/hateful_memes/roc_auc: 0.7227, num_updates: 12000, epoch: 42, iterations: 12000, max_updates: 22000, val_time: 40s 198ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.738890\n",
      "\u001b[32m2021-05-04T02:11:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0810, train/total_loss: 0.0007, train/total_loss/avg: 0.0810, max mem: 9226.0, experiment: run, epoch: 42, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 195ms, time_since_start: 03h 30m 45s 263ms, eta: 02h 34m 33s 432ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:11:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:11:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:12:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0804, train/total_loss: 0.0007, train/total_loss/avg: 0.0804, max mem: 9226.0, experiment: run, epoch: 43, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 334ms, time_since_start: 03h 32m 16s 597ms, eta: 02h 31m 34s 000ms\n",
      "\u001b[32m2021-05-04T02:14:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0802, train/total_loss: 0.0007, train/total_loss/avg: 0.0802, max mem: 9226.0, experiment: run, epoch: 43, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 572ms, time_since_start: 03h 33m 46s 170ms, eta: 02h 27m 07s 536ms\n",
      "\u001b[32m2021-05-04T02:15:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0796, train/total_loss: 0.0007, train/total_loss/avg: 0.0796, max mem: 9226.0, experiment: run, epoch: 43, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 515ms, time_since_start: 03h 35m 14s 685ms, eta: 02h 23m 53s 428ms\n",
      "\u001b[32m2021-05-04T02:17:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0791, train/total_loss: 0.0007, train/total_loss/avg: 0.0791, max mem: 9226.0, experiment: run, epoch: 44, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 607ms, time_since_start: 03h 36m 45s 292ms, eta: 02h 25m 45s 408ms\n",
      "\u001b[32m2021-05-04T02:18:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0784, train/total_loss: 0.0007, train/total_loss/avg: 0.0784, max mem: 9226.0, experiment: run, epoch: 44, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 105ms, time_since_start: 03h 38m 13s 398ms, eta: 02h 20m 14s 438ms\n",
      "\u001b[32m2021-05-04T02:20:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0780, train/total_loss: 0.0007, train/total_loss/avg: 0.0780, max mem: 9226.0, experiment: run, epoch: 44, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 778ms, time_since_start: 03h 39m 42s 177ms, eta: 02h 19m 48s 522ms\n",
      "\u001b[32m2021-05-04T02:21:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0774, train/total_loss: 0.0005, train/total_loss/avg: 0.0774, max mem: 9226.0, experiment: run, epoch: 45, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 952ms, time_since_start: 03h 41m 13s 129ms, eta: 02h 21m 41s 519ms\n",
      "\u001b[32m2021-05-04T02:23:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0768, train/total_loss: 0.0009, train/total_loss/avg: 0.0768, max mem: 9226.0, experiment: run, epoch: 45, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 921ms, time_since_start: 03h 42m 42s 051ms, eta: 02h 17m 01s 316ms\n",
      "\u001b[32m2021-05-04T02:24:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T02:24:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:24:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:25:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:25:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0763, train/total_loss: 0.0005, train/total_loss/avg: 0.0763, max mem: 9226.0, experiment: run, epoch: 45, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 0.91, time: 01m 50s 057ms, time_since_start: 03h 44m 32s 108ms, eta: 02h 47m 43s 675ms\n",
      "\u001b[32m2021-05-04T02:25:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T02:25:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:25:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:25:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:25:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.3357, val/total_loss: 2.3357, val/hateful_memes/accuracy: 0.6700, val/hateful_memes/binary_f1: 0.5780, val/hateful_memes/roc_auc: 0.7321, num_updates: 13000, epoch: 45, iterations: 13000, max_updates: 22000, val_time: 29s 462ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.738890\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:25:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:25:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:27:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0757, train/total_loss: 0.0005, train/total_loss/avg: 0.0757, max mem: 9226.0, experiment: run, epoch: 46, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 1.04, time: 01m 36s 503ms, time_since_start: 03h 46m 38s 077ms, eta: 02h 25m 26s 258ms\n",
      "\u001b[32m2021-05-04T02:28:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0751, train/total_loss: 0.0005, train/total_loss/avg: 0.0751, max mem: 9226.0, experiment: run, epoch: 46, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 822ms, time_since_start: 03h 48m 06s 899ms, eta: 02h 12m 21s 410ms\n",
      "\u001b[32m2021-05-04T02:30:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0745, train/total_loss: 0.0005, train/total_loss/avg: 0.0745, max mem: 9226.0, experiment: run, epoch: 47, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 1.08, time: 01m 33s 436ms, time_since_start: 03h 49m 40s 335ms, eta: 02h 17m 39s 021ms\n",
      "\u001b[32m2021-05-04T02:31:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0740, train/total_loss: 0.0004, train/total_loss/avg: 0.0740, max mem: 9226.0, experiment: run, epoch: 47, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 1.15, time: 01m 27s 869ms, time_since_start: 03h 51m 08s 205ms, eta: 02h 07m 57s 680ms\n",
      "\u001b[32m2021-05-04T02:33:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0734, train/total_loss: 0.0003, train/total_loss/avg: 0.0734, max mem: 9226.0, experiment: run, epoch: 47, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 857ms, time_since_start: 03h 52m 39s 062ms, eta: 02h 10m 46s 449ms\n",
      "\u001b[32m2021-05-04T02:34:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0729, train/total_loss: 0.0003, train/total_loss/avg: 0.0729, max mem: 9226.0, experiment: run, epoch: 48, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 1.08, time: 01m 33s 329ms, time_since_start: 03h 54m 12s 392ms, eta: 02h 12m 45s 124ms\n",
      "\u001b[32m2021-05-04T02:36:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0724, train/total_loss: 0.0003, train/total_loss/avg: 0.0724, max mem: 9226.0, experiment: run, epoch: 48, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 970ms, time_since_start: 03h 55m 42s 362ms, eta: 02h 06m 27s 007ms\n",
      "\u001b[32m2021-05-04T02:37:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0719, train/total_loss: 0.0003, train/total_loss/avg: 0.0719, max mem: 9226.0, experiment: run, epoch: 48, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 480ms, time_since_start: 03h 57m 11s 843ms, eta: 02h 04m 14s 833ms\n",
      "\u001b[32m2021-05-04T02:39:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0714, train/total_loss: 0.0003, train/total_loss/avg: 0.0714, max mem: 9226.0, experiment: run, epoch: 49, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 379ms, time_since_start: 03h 58m 42s 222ms, eta: 02h 03m 57s 856ms\n",
      "\u001b[32m2021-05-04T02:40:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T02:40:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:40:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:41:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:41:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0709, train/total_loss: 0.0003, train/total_loss/avg: 0.0709, max mem: 9226.0, experiment: run, epoch: 49, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 0.91, time: 01m 50s 734ms, time_since_start: 04h 32s 957ms, eta: 02h 30m 519ms\n",
      "\u001b[32m2021-05-04T02:41:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T02:41:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:41:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:41:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:41:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.8520, val/total_loss: 2.8520, val/hateful_memes/accuracy: 0.6520, val/hateful_memes/binary_f1: 0.5693, val/hateful_memes/roc_auc: 0.7268, num_updates: 14000, epoch: 49, iterations: 14000, max_updates: 22000, val_time: 23s 223ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.738890\n",
      "\u001b[32m2021-05-04T02:42:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0704, train/total_loss: 0.0003, train/total_loss/avg: 0.0704, max mem: 9226.0, experiment: run, epoch: 49, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 515ms, time_since_start: 04h 02m 25s 702ms, eta: 01h 59m 44s 858ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:43:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:43:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:44:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0699, train/total_loss: 0.0002, train/total_loss/avg: 0.0699, max mem: 9226.0, experiment: run, epoch: 50, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 750ms, time_since_start: 04h 03m 57s 452ms, eta: 02h 01m 11s 048ms\n",
      "\u001b[32m2021-05-04T02:45:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0694, train/total_loss: 0.0002, train/total_loss/avg: 0.0694, max mem: 9226.0, experiment: run, epoch: 50, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 941ms, time_since_start: 04h 05m 26s 394ms, eta: 01h 55m 58s 065ms\n",
      "\u001b[32m2021-05-04T02:47:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0689, train/total_loss: 0.0002, train/total_loss/avg: 0.0689, max mem: 9226.0, experiment: run, epoch: 50, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 914ms, time_since_start: 04h 06m 56s 308ms, eta: 01h 55m 42s 817ms\n",
      "\u001b[32m2021-05-04T02:48:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0684, train/total_loss: 0.0002, train/total_loss/avg: 0.0684, max mem: 9226.0, experiment: run, epoch: 51, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 956ms, time_since_start: 04h 08m 27s 264ms, eta: 01h 55m 30s 894ms\n",
      "\u001b[32m2021-05-04T02:50:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0680, train/total_loss: 0.0001, train/total_loss/avg: 0.0680, max mem: 9226.0, experiment: run, epoch: 51, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 248ms, time_since_start: 04h 09m 55s 513ms, eta: 01h 50m 34s 874ms\n",
      "\u001b[32m2021-05-04T02:51:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0675, train/total_loss: 0.0001, train/total_loss/avg: 0.0675, max mem: 9226.0, experiment: run, epoch: 51, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 776ms, time_since_start: 04h 11m 24s 289ms, eta: 01h 49m 44s 355ms\n",
      "\u001b[32m2021-05-04T02:53:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0671, train/total_loss: 0.0001, train/total_loss/avg: 0.0671, max mem: 9226.0, experiment: run, epoch: 52, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 759ms, time_since_start: 04h 12m 54s 049ms, eta: 01h 49m 26s 108ms\n",
      "\u001b[32m2021-05-04T02:54:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0666, train/total_loss: 0.0001, train/total_loss/avg: 0.0666, max mem: 9226.0, experiment: run, epoch: 52, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 141ms, time_since_start: 04h 14m 22s 190ms, eta: 01h 45m 58s 146ms\n",
      "\u001b[32m2021-05-04T02:56:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T02:56:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:56:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:56:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:56:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0662, train/total_loss: 0.0001, train/total_loss/avg: 0.0662, max mem: 9226.0, experiment: run, epoch: 52, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 0.90, time: 01m 51s 352ms, time_since_start: 04h 16m 13s 543ms, eta: 02h 11m 59s 408ms\n",
      "\u001b[32m2021-05-04T02:56:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T02:56:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:56:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:56:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:57:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:57:17 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-04T02:57:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:57:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:57:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.8017, val/total_loss: 2.8017, val/hateful_memes/accuracy: 0.6740, val/hateful_memes/binary_f1: 0.6200, val/hateful_memes/roc_auc: 0.7397, num_updates: 15000, epoch: 52, iterations: 15000, max_updates: 22000, val_time: 01m 311ms, best_update: 15000, best_iteration: 15000, best_val/hateful_memes/roc_auc: 0.739743\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:58:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:58:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:59:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0657, train/total_loss: 0.0001, train/total_loss/avg: 0.0657, max mem: 9226.0, experiment: run, epoch: 53, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 0.76, time: 02m 11s 600ms, time_since_start: 04h 19m 25s 462ms, eta: 02h 33m 45s 743ms\n",
      "\u001b[32m2021-05-04T03:01:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0653, train/total_loss: 0.0000, train/total_loss/avg: 0.0653, max mem: 9226.0, experiment: run, epoch: 53, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 105ms, time_since_start: 04h 20m 53s 567ms, eta: 01h 41m 27s 034ms\n",
      "\u001b[32m2021-05-04T03:02:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0649, train/total_loss: 0.0000, train/total_loss/avg: 0.0649, max mem: 9226.0, experiment: run, epoch: 53, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 445ms, time_since_start: 04h 22m 23s 013ms, eta: 01h 41m 28s 729ms\n",
      "\u001b[32m2021-05-04T03:04:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0645, train/total_loss: 0.0000, train/total_loss/avg: 0.0645, max mem: 9226.0, experiment: run, epoch: 54, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 206ms, time_since_start: 04h 23m 53s 219ms, eta: 01h 40m 48s 909ms\n",
      "\u001b[32m2021-05-04T03:05:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0640, train/total_loss: 0.0000, train/total_loss/avg: 0.0640, max mem: 9226.0, experiment: run, epoch: 54, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 677ms, time_since_start: 04h 25m 21s 897ms, eta: 01h 37m 36s 293ms\n",
      "\u001b[32m2021-05-04T03:07:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0636, train/total_loss: 0.0000, train/total_loss/avg: 0.0636, max mem: 9226.0, experiment: run, epoch: 54, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 261ms, time_since_start: 04h 26m 51s 159ms, eta: 01h 36m 44s 154ms\n",
      "\u001b[32m2021-05-04T03:08:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0632, train/total_loss: 0.0000, train/total_loss/avg: 0.0632, max mem: 9226.0, experiment: run, epoch: 55, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 607ms, time_since_start: 04h 28m 22s 767ms, eta: 01h 37m 43s 632ms\n",
      "\u001b[32m2021-05-04T03:10:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0628, train/total_loss: 0.0000, train/total_loss/avg: 0.0628, max mem: 9226.0, experiment: run, epoch: 55, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 290ms, time_since_start: 04h 29m 52s 058ms, eta: 01h 33m 44s 618ms\n",
      "\u001b[32m2021-05-04T03:11:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0624, train/total_loss: 0.0000, train/total_loss/avg: 0.0624, max mem: 9226.0, experiment: run, epoch: 56, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 162ms, time_since_start: 04h 31m 24s 220ms, eta: 01h 35m 11s 847ms\n",
      "\u001b[32m2021-05-04T03:13:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T03:13:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:13:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:13:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:13:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0620, train/total_loss: 0.0000, train/total_loss/avg: 0.0620, max mem: 9226.0, experiment: run, epoch: 56, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 0.90, time: 01m 51s 282ms, time_since_start: 04h 33m 15s 502ms, eta: 01h 53m 03s 760ms\n",
      "\u001b[32m2021-05-04T03:13:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T03:13:47 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:13:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:13:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:14:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:14:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:14:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:14:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.3718, val/total_loss: 2.3718, val/hateful_memes/accuracy: 0.6660, val/hateful_memes/binary_f1: 0.5835, val/hateful_memes/roc_auc: 0.7366, num_updates: 16000, epoch: 56, iterations: 16000, max_updates: 22000, val_time: 42s 302ms, best_update: 15000, best_iteration: 15000, best_val/hateful_memes/roc_auc: 0.739743\n",
      "\u001b[32m2021-05-04T03:16:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0617, train/total_loss: 0.0000, train/total_loss/avg: 0.0617, max mem: 9226.0, experiment: run, epoch: 56, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 714ms, time_since_start: 04h 35m 29s 523ms, eta: 01h 31m 37s 753ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:17:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:17:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:17:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0613, train/total_loss: 0.0000, train/total_loss/avg: 0.0613, max mem: 9226.0, experiment: run, epoch: 57, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 532ms, time_since_start: 04h 37m 01s 056ms, eta: 01h 29m 53s 844ms\n",
      "\u001b[32m2021-05-04T03:19:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0609, train/total_loss: 0.0000, train/total_loss/avg: 0.0609, max mem: 9226.0, experiment: run, epoch: 57, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 587ms, time_since_start: 04h 38m 28s 643ms, eta: 01h 24m 32s 341ms\n",
      "\u001b[32m2021-05-04T03:20:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0605, train/total_loss: 0.0000, train/total_loss/avg: 0.0605, max mem: 9226.0, experiment: run, epoch: 57, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 611ms, time_since_start: 04h 39m 57s 255ms, eta: 01h 24m 01s 652ms\n",
      "\u001b[32m2021-05-04T03:22:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0602, train/total_loss: 0.0000, train/total_loss/avg: 0.0602, max mem: 9226.0, experiment: run, epoch: 58, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 773ms, time_since_start: 04h 41m 29s 028ms, eta: 01h 25m 28s 318ms\n",
      "\u001b[32m2021-05-04T03:23:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0598, train/total_loss: 0.0000, train/total_loss/avg: 0.0598, max mem: 9226.0, experiment: run, epoch: 58, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 883ms, time_since_start: 04h 42m 56s 912ms, eta: 01h 20m 21s 629ms\n",
      "\u001b[32m2021-05-04T03:24:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0595, train/total_loss: 0.0000, train/total_loss/avg: 0.0595, max mem: 9226.0, experiment: run, epoch: 58, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 378ms, time_since_start: 04h 44m 26s 290ms, eta: 01h 20m 12s 838ms\n",
      "\u001b[32m2021-05-04T03:26:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0591, train/total_loss: 0.0000, train/total_loss/avg: 0.0591, max mem: 9226.0, experiment: run, epoch: 59, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 014ms, time_since_start: 04h 45m 57s 305ms, eta: 01h 20m 08s 482ms\n",
      "\u001b[32m2021-05-04T03:27:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0588, train/total_loss: 0.0000, train/total_loss/avg: 0.0588, max mem: 9226.0, experiment: run, epoch: 59, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 364ms, time_since_start: 04h 47m 24s 669ms, eta: 01h 15m 26s 891ms\n",
      "\u001b[32m2021-05-04T03:29:24 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T03:29:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:29:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:29:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:29:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0584, train/total_loss: 0.0000, train/total_loss/avg: 0.0584, max mem: 9226.0, experiment: run, epoch: 59, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 0.90, time: 01m 51s 919ms, time_since_start: 04h 49m 16s 589ms, eta: 01h 34m 45s 522ms\n",
      "\u001b[32m2021-05-04T03:29:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T03:29:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:29:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:29:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:30:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:30:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:30:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:30:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 2.7203, val/total_loss: 2.7203, val/hateful_memes/accuracy: 0.6480, val/hateful_memes/binary_f1: 0.5269, val/hateful_memes/roc_auc: 0.7330, num_updates: 17000, epoch: 59, iterations: 17000, max_updates: 22000, val_time: 41s 614ms, best_update: 15000, best_iteration: 15000, best_val/hateful_memes/roc_auc: 0.739743\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:31:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:31:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:32:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0581, train/total_loss: 0.0000, train/total_loss/avg: 0.0581, max mem: 9226.0, experiment: run, epoch: 60, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 591ms, time_since_start: 04h 51m 33s 805ms, eta: 01h 19m 18s 936ms\n",
      "\u001b[32m2021-05-04T03:33:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0578, train/total_loss: 0.0001, train/total_loss/avg: 0.0578, max mem: 9226.0, experiment: run, epoch: 60, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 505ms, time_since_start: 04h 53m 02s 310ms, eta: 01h 11m 56s 222ms\n",
      "\u001b[32m2021-05-04T03:35:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0574, train/total_loss: 0.0000, train/total_loss/avg: 0.0574, max mem: 9226.0, experiment: run, epoch: 60, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 026ms, time_since_start: 04h 54m 32s 337ms, eta: 01h 11m 38s 958ms\n",
      "\u001b[32m2021-05-04T03:36:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0571, train/total_loss: 0.0000, train/total_loss/avg: 0.0571, max mem: 9226.0, experiment: run, epoch: 61, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 959ms, time_since_start: 04h 56m 03s 297ms, eta: 01h 10m 51s 094ms\n",
      "\u001b[32m2021-05-04T03:38:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0568, train/total_loss: 0.0000, train/total_loss/avg: 0.0568, max mem: 9226.0, experiment: run, epoch: 61, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 566ms, time_since_start: 04h 57m 31s 863ms, eta: 01h 07m 29s 250ms\n",
      "\u001b[32m2021-05-04T03:39:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0565, train/total_loss: 0.0000, train/total_loss/avg: 0.0565, max mem: 9226.0, experiment: run, epoch: 61, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 735ms, time_since_start: 04h 59m 599ms, eta: 01h 06m 06s 842ms\n",
      "\u001b[32m2021-05-04T03:41:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0561, train/total_loss: 0.0000, train/total_loss/avg: 0.0561, max mem: 9226.0, experiment: run, epoch: 62, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 366ms, time_since_start: 05h 30s 965ms, eta: 01h 05m 47s 923ms\n",
      "\u001b[32m2021-05-04T03:42:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0558, train/total_loss: 0.0000, train/total_loss/avg: 0.0558, max mem: 9226.0, experiment: run, epoch: 62, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 003ms, time_since_start: 05h 01m 59s 968ms, eta: 01h 03m 17s 947ms\n",
      "\u001b[32m2021-05-04T03:43:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0555, train/total_loss: 0.0000, train/total_loss/avg: 0.0555, max mem: 9226.0, experiment: run, epoch: 62, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 918ms, time_since_start: 05h 03m 27s 887ms, eta: 01h 01m 02s 323ms\n",
      "\u001b[32m2021-05-04T03:45:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T03:45:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:45:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:45:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:45:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0552, train/total_loss: 0.0000, train/total_loss/avg: 0.0552, max mem: 9226.0, experiment: run, epoch: 63, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 212ms, time_since_start: 05h 05m 22s 099ms, eta: 01h 17m 21s 595ms\n",
      "\u001b[32m2021-05-04T03:45:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T03:45:54 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:45:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:45:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:46:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:46:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:46:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:46:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 3.0306, val/total_loss: 3.0306, val/hateful_memes/accuracy: 0.6540, val/hateful_memes/binary_f1: 0.5435, val/hateful_memes/roc_auc: 0.7297, num_updates: 18000, epoch: 63, iterations: 18000, max_updates: 22000, val_time: 46s 585ms, best_update: 15000, best_iteration: 15000, best_val/hateful_memes/roc_auc: 0.739743\n",
      "\u001b[32m2021-05-04T03:48:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0549, train/total_loss: 0.0000, train/total_loss/avg: 0.0549, max mem: 9226.0, experiment: run, epoch: 63, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 456ms, time_since_start: 05h 07m 52s 164ms, eta: 01h 08m 19s 364ms\n",
      "\u001b[32m2021-05-04T03:49:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0546, train/total_loss: 0.0000, train/total_loss/avg: 0.0546, max mem: 9226.0, experiment: run, epoch: 63, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 812ms, time_since_start: 05h 09m 20s 977ms, eta: 57m 08s 893ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:49:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:49:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:51:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0543, train/total_loss: 0.0000, train/total_loss/avg: 0.0543, max mem: 9226.0, experiment: run, epoch: 64, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 582ms, time_since_start: 05h 10m 51s 559ms, eta: 56m 45s 176ms\n",
      "\u001b[32m2021-05-04T03:52:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0540, train/total_loss: 0.0000, train/total_loss/avg: 0.0540, max mem: 9226.0, experiment: run, epoch: 64, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 963ms, time_since_start: 05h 12m 20s 523ms, eta: 54m 13s 939ms\n",
      "\u001b[32m2021-05-04T03:54:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0537, train/total_loss: 0.0000, train/total_loss/avg: 0.0537, max mem: 9226.0, experiment: run, epoch: 65, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 657ms, time_since_start: 05h 13m 52s 180ms, eta: 54m 19s 328ms\n",
      "\u001b[32m2021-05-04T03:55:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0534, train/total_loss: 0.0000, train/total_loss/avg: 0.0534, max mem: 9226.0, experiment: run, epoch: 65, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 107ms, time_since_start: 05h 15m 19s 288ms, eta: 50m 09s 053ms\n",
      "\u001b[32m2021-05-04T03:57:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0531, train/total_loss: 0.0000, train/total_loss/avg: 0.0531, max mem: 9226.0, experiment: run, epoch: 65, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 731ms, time_since_start: 05h 16m 48s 020ms, eta: 49m 34s 985ms\n",
      "\u001b[32m2021-05-04T03:58:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0529, train/total_loss: 0.0000, train/total_loss/avg: 0.0529, max mem: 9226.0, experiment: run, epoch: 66, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 898ms, time_since_start: 05h 18m 21s 918ms, eta: 50m 52s 840ms\n",
      "\u001b[32m2021-05-04T04:00:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0526, train/total_loss: 0.0000, train/total_loss/avg: 0.0526, max mem: 9226.0, experiment: run, epoch: 66, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 174ms, time_since_start: 05h 19m 49s 093ms, eta: 45m 45s 659ms\n",
      "\u001b[32m2021-05-04T04:01:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T04:01:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:02:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:02:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:02:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0523, train/total_loss: 0.0000, train/total_loss/avg: 0.0523, max mem: 9226.0, experiment: run, epoch: 66, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 0.89, time: 01m 52s 261ms, time_since_start: 05h 21m 41s 355ms, eta: 57m 01s 723ms\n",
      "\u001b[32m2021-05-04T04:02:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T04:02:13 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:02:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:02:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:02:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:02:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:02:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:02:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.3202, val/total_loss: 3.3202, val/hateful_memes/accuracy: 0.6600, val/hateful_memes/binary_f1: 0.5503, val/hateful_memes/roc_auc: 0.7283, num_updates: 19000, epoch: 66, iterations: 19000, max_updates: 22000, val_time: 39s 762ms, best_update: 15000, best_iteration: 15000, best_val/hateful_memes/roc_auc: 0.739743\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:04:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:04:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:04:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0520, train/total_loss: 0.0000, train/total_loss/avg: 0.0520, max mem: 9226.0, experiment: run, epoch: 67, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 028ms, time_since_start: 05h 23m 56s 151ms, eta: 46m 39s 933ms\n",
      "\u001b[32m2021-05-04T04:05:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0518, train/total_loss: 0.0000, train/total_loss/avg: 0.0518, max mem: 9226.0, experiment: run, epoch: 67, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 476ms, time_since_start: 05h 25m 24s 627ms, eta: 41m 56s 976ms\n",
      "\u001b[32m2021-05-04T04:07:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0515, train/total_loss: 0.0000, train/total_loss/avg: 0.0515, max mem: 9226.0, experiment: run, epoch: 67, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 082ms, time_since_start: 05h 26m 54s 710ms, eta: 41m 11s 151ms\n",
      "\u001b[32m2021-05-04T04:08:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0512, train/total_loss: 0.0000, train/total_loss/avg: 0.0512, max mem: 9226.0, experiment: run, epoch: 68, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 941ms, time_since_start: 05h 28m 26s 652ms, eta: 40m 28s 731ms\n",
      "\u001b[32m2021-05-04T04:10:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0510, train/total_loss: 0.0000, train/total_loss/avg: 0.0510, max mem: 9226.0, experiment: run, epoch: 68, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 769ms, time_since_start: 05h 29m 55s 422ms, eta: 37m 34s 748ms\n",
      "\u001b[32m2021-05-04T04:11:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0507, train/total_loss: 0.0000, train/total_loss/avg: 0.0507, max mem: 9226.0, experiment: run, epoch: 68, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 136ms, time_since_start: 05h 31m 23s 558ms, eta: 35m 49s 112ms\n",
      "\u001b[32m2021-05-04T04:13:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0504, train/total_loss: 0.0000, train/total_loss/avg: 0.0504, max mem: 9226.0, experiment: run, epoch: 69, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 290ms, time_since_start: 05h 32m 54s 848ms, eta: 35m 33s 281ms\n",
      "\u001b[32m2021-05-04T04:14:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0502, train/total_loss: 0.0000, train/total_loss/avg: 0.0502, max mem: 9226.0, experiment: run, epoch: 69, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 599ms, time_since_start: 05h 34m 24s 448ms, eta: 33m 22s 723ms\n",
      "\u001b[32m2021-05-04T04:16:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0499, train/total_loss: 0.0000, train/total_loss/avg: 0.0499, max mem: 9226.0, experiment: run, epoch: 69, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 708ms, time_since_start: 05h 35m 55s 156ms, eta: 32m 15s 347ms\n",
      "\u001b[32m2021-05-04T04:17:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T04:17:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:18:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:18:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:18:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0497, train/total_loss: 0.0000, train/total_loss/avg: 0.0497, max mem: 9226.0, experiment: run, epoch: 70, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 303ms, time_since_start: 05h 37m 51s 459ms, eta: 39m 23s 287ms\n",
      "\u001b[32m2021-05-04T04:18:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T04:18:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:18:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:18:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:18:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 3.4133, val/total_loss: 3.4133, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5581, val/hateful_memes/roc_auc: 0.7276, num_updates: 20000, epoch: 70, iterations: 20000, max_updates: 22000, val_time: 25s 833ms, best_update: 15000, best_iteration: 15000, best_val/hateful_memes/roc_auc: 0.739743\n",
      "\u001b[32m2021-05-04T04:20:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0494, train/total_loss: 0.0000, train/total_loss/avg: 0.0494, max mem: 9226.0, experiment: run, epoch: 70, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 986ms, time_since_start: 05h 39m 46s 284ms, eta: 28m 37s 800ms\n",
      "\u001b[32m2021-05-04T04:21:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0492, train/total_loss: 0.0000, train/total_loss/avg: 0.0492, max mem: 9226.0, experiment: run, epoch: 70, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.11, time: 01m 30s 629ms, time_since_start: 05h 41m 16s 913ms, eta: 27m 37s 430ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:22:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:22:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:23:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0489, train/total_loss: 0.0000, train/total_loss/avg: 0.0489, max mem: 9226.0, experiment: run, epoch: 71, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.10, time: 01m 31s 410ms, time_since_start: 05h 42m 48s 323ms, eta: 26m 18s 838ms\n",
      "\u001b[32m2021-05-04T04:24:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0487, train/total_loss: 0.0000, train/total_loss/avg: 0.0487, max mem: 9226.0, experiment: run, epoch: 71, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 774ms, time_since_start: 05h 44m 18s 098ms, eta: 24m 19s 374ms\n",
      "\u001b[32m2021-05-04T04:26:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0485, train/total_loss: 0.0000, train/total_loss/avg: 0.0485, max mem: 9226.0, experiment: run, epoch: 71, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 288ms, time_since_start: 05h 45m 47s 386ms, eta: 22m 40s 751ms\n",
      "\u001b[32m2021-05-04T04:27:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0482, train/total_loss: 0.0000, train/total_loss/avg: 0.0482, max mem: 9226.0, experiment: run, epoch: 72, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.11, time: 01m 30s 652ms, time_since_start: 05h 47m 18s 039ms, eta: 21m 29s 443ms\n",
      "\u001b[32m2021-05-04T04:29:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0480, train/total_loss: 0.0000, train/total_loss/avg: 0.0480, max mem: 9226.0, experiment: run, epoch: 72, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 020ms, time_since_start: 05h 48m 47s 059ms, eta: 19m 35s 784ms\n",
      "\u001b[32m2021-05-04T04:30:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0478, train/total_loss: 0.0000, train/total_loss/avg: 0.0478, max mem: 9226.0, experiment: run, epoch: 72, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 958ms, time_since_start: 05h 50m 16s 018ms, eta: 18m 04s 577ms\n",
      "\u001b[32m2021-05-04T04:32:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0475, train/total_loss: 0.0000, train/total_loss/avg: 0.0475, max mem: 9226.0, experiment: run, epoch: 73, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.10, time: 01m 31s 330ms, time_since_start: 05h 51m 47s 348ms, eta: 17m 712ms\n",
      "\u001b[32m2021-05-04T04:33:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T04:33:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:34:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:34:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:34:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0473, train/total_loss: 0.0000, train/total_loss/avg: 0.0473, max mem: 9226.0, experiment: run, epoch: 73, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 0.89, time: 01m 52s 726ms, time_since_start: 05h 53m 40s 075ms, eta: 19m 05s 297ms\n",
      "\u001b[32m2021-05-04T04:34:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T04:34:12 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:34:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:34:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:34:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 3.4725, val/total_loss: 3.4725, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5649, val/hateful_memes/roc_auc: 0.7296, num_updates: 21000, epoch: 73, iterations: 21000, max_updates: 22000, val_time: 20s 662ms, best_update: 15000, best_iteration: 15000, best_val/hateful_memes/roc_auc: 0.739743\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:35:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:35:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:36:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0471, train/total_loss: 0.0000, train/total_loss/avg: 0.0471, max mem: 9226.0, experiment: run, epoch: 74, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.09, time: 01m 32s 720ms, time_since_start: 05h 55m 33s 461ms, eta: 14m 07s 833ms\n",
      "\u001b[32m2021-05-04T04:37:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0469, train/total_loss: 0.0000, train/total_loss/avg: 0.0469, max mem: 9226.0, experiment: run, epoch: 74, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 034ms, time_since_start: 05h 57m 01s 496ms, eta: 11m 55s 542ms\n",
      "\u001b[32m2021-05-04T04:39:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0466, train/total_loss: 0.0000, train/total_loss/avg: 0.0466, max mem: 9226.0, experiment: run, epoch: 74, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 984ms, time_since_start: 05h 58m 31s 480ms, eta: 10m 39s 969ms\n",
      "\u001b[32m2021-05-04T04:40:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0464, train/total_loss: 0.0000, train/total_loss/avg: 0.0464, max mem: 9226.0, experiment: run, epoch: 75, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.09, time: 01m 32s 110ms, time_since_start: 06h 03s 591ms, eta: 09m 21s 506ms\n",
      "\u001b[32m2021-05-04T04:42:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0462, train/total_loss: 0.0000, train/total_loss/avg: 0.0462, max mem: 9226.0, experiment: run, epoch: 75, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 526ms, time_since_start: 06h 01m 32s 117ms, eta: 07m 29s 716ms\n",
      "\u001b[32m2021-05-04T04:43:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0460, train/total_loss: 0.0000, train/total_loss/avg: 0.0460, max mem: 9226.0, experiment: run, epoch: 75, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 503ms, time_since_start: 06h 03m 621ms, eta: 05m 59s 679ms\n",
      "\u001b[32m2021-05-04T04:45:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0458, train/total_loss: 0.0000, train/total_loss/avg: 0.0458, max mem: 9226.0, experiment: run, epoch: 76, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 180ms, time_since_start: 06h 04m 33s 802ms, eta: 04m 44s 015ms\n",
      "\u001b[32m2021-05-04T04:46:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0456, train/total_loss: 0.0000, train/total_loss/avg: 0.0456, max mem: 9226.0, experiment: run, epoch: 76, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 240ms, time_since_start: 06h 06m 02s 043ms, eta: 02m 59s 304ms\n",
      "\u001b[32m2021-05-04T04:48:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0454, train/total_loss: 0.0000, train/total_loss/avg: 0.0454, max mem: 9226.0, experiment: run, epoch: 76, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 970ms, time_since_start: 06h 07m 32s 013ms, eta: 01m 31s 409ms\n",
      "\u001b[32m2021-05-04T04:49:36 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T04:49:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:49:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:50:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:50:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0452, train/total_loss: 0.0000, train/total_loss/avg: 0.0452, max mem: 9226.0, experiment: run, epoch: 77, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 0.85, time: 01m 57s 801ms, time_since_start: 06h 09m 29s 815ms, eta: 0ms\n",
      "\u001b[32m2021-05-04T04:50:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T04:50:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:50:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:50:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:50:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 3.5544, val/total_loss: 3.5544, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5604, val/hateful_memes/roc_auc: 0.7277, num_updates: 22000, epoch: 77, iterations: 22000, max_updates: 22000, val_time: 21s 080ms, best_update: 15000, best_iteration: 15000, best_val/hateful_memes/roc_auc: 0.739743\n",
      "\u001b[32m2021-05-04T04:50:23 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2021-05-04T04:50:23 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2021-05-04T04:50:23 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2021-05-04T04:50:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-04T04:50:45 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 15000\n",
      "\u001b[32m2021-05-04T04:50:45 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 15000\n",
      "\u001b[32m2021-05-04T04:50:45 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 52\n",
      "\u001b[32m2021-05-04T04:50:48 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-04T04:50:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:50:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:50:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "100% 16/16 [00:15<00:00,  1.03it/s]\n",
      "\u001b[32m2021-05-04T04:51:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.8017, val/total_loss: 2.8017, val/hateful_memes/accuracy: 0.6740, val/hateful_memes/binary_f1: 0.6200, val/hateful_memes/roc_auc: 0.7397\n",
      "\u001b[32m2021-05-04T04:51:03 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 06h 10m 31s 839ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/direct.yaml \\\n",
    "  model=visual_bert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=train_val \\\n",
    "  training.batch_size=32 \\\n",
    "  env.save_dir=/content/gdrive/MyDrive/colab/pretrained_visualbert_election_memes/ \\\n",
    "  checkpoint.resume_zoo=visual_bert.pretrained.vqa2.full \\\n",
    "  checkpoint.resume_pretrained=True \\\n",
    "  dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 165481,
     "status": "ok",
     "timestamp": 1620104603697,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "D7eNv5dZi8k1",
    "outputId": "95106d21-0b16-416c-cd94-f42943732a7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-04 05:00:56.401341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-04T05:02:08 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/direct.yaml\n",
      "\u001b[32m2021-05-04T05:02:08 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2021-05-04T05:02:08 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-04T05:02:08 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2021-05-04T05:02:08 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.direct\n",
      "\u001b[32m2021-05-04T05:02:08 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
      "\u001b[32m2021-05-04T05:02:08 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2021-05-04T05:02:08 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2021-05-04T05:02:09 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/direct.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct', 'checkpoint.resume_pretrained=False', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl'])\n",
      "\u001b[32m2021-05-04T05:02:09 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-04T05:02:09 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-04T05:02:09 | mmf_cli.run: \u001b[0mUsing seed 8707385\n",
      "\u001b[32m2021-05-04T05:02:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:02:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T05:02:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T05:02:11 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T05:02:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-04T05:02:26 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-04T05:02:26 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-04T05:02:27 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:35 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:35 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:35 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:35 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:35 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
      "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
      "Unexpected keys if any: []\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:35 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:35 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:35 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:02:35 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2021-05-04T05:02:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-04T05:02:35 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-04T05:02:35 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-04T05:02:35 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-04T05:02:35 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-04T05:02:35 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-04T05:02:35 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2021-05-04T05:02:35 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-04T05:02:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100% 4/4 [00:45<00:00, 11.40s/it]\n",
      "\u001b[32m2021-05-04T05:03:22 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 1.4959, val/total_loss: 1.4959, val/hateful_memes/accuracy: 0.5980, val/hateful_memes/binary_f1: 0.4401, val/hateful_memes/roc_auc: 0.7104\n",
      "\u001b[32m2021-05-04T05:03:22 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 54s 706ms\n"
     ]
    }
   ],
   "source": [
    "# Baseline on dev_seen.jsonl Visual BERT\n",
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/direct.yaml \\\n",
    "  model=visual_bert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=val \\\n",
    "  checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct \\\n",
    "  checkpoint.resume_pretrained=False \\\n",
    "  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151999,
     "status": "ok",
     "timestamp": 1620104909260,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "RE93g72Xv75Z",
    "outputId": "3307ee30-f23d-4f53-e21e-1ade32db80ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-04 05:05:59.531677: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/defaults.yaml\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.direct\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/defaults.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct', 'checkpoint.resume_pretrained=False', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl'])\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf_cli.run: \u001b[0mUsing seed 4411114\n",
      "\u001b[32m2021-05-04T05:06:04 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:06:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:06:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:06:06 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T05:06:06 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T05:06:06 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T05:06:06 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-04T05:06:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-04T05:06:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:06:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:06:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-04T05:06:13 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_direct.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.finetuned.hateful_memes.direct/vilbert.finetuned.hateful_memes_direct.tar.gz ]\n",
      "Downloading vilbert.finetuned.hateful_memes_direct.tar.gz: 100% 918M/918M [01:16<00:00, 12.0MB/s]\n",
      "[ Starting checksum for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
      "[ Checksum successful for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
      "Unpacking vilbert.finetuned.hateful_memes_direct.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:07:43 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:07:43 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:07:43 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:07:43 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:07:44 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
      "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
      "Unexpected keys if any: []\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:07:44 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:07:44 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:07:44 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:07:44 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2021-05-04T05:07:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-04T05:07:44 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-04T05:07:44 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-04T05:07:44 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-04T05:07:44 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-04T05:07:44 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
      "  (model): ViLBERTForClassification(\n",
      "    (bert): ViLBERTBase(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (v_embeddings): BertImageFeatureEmbeddings(\n",
      "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (v_layer): ModuleList(\n",
      "          (0): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (c_layer): ModuleList(\n",
      "          (0): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (t_pooler): BertTextPooler(\n",
      "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (v_pooler): BertImagePooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-04T05:07:44 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
      "\u001b[32m2021-05-04T05:07:44 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-04T05:07:44 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100% 16/16 [00:42<00:00,  2.68s/it]\n",
      "\u001b[32m2021-05-04T05:08:27 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 2.7673, val/total_loss: 2.7673, val/hateful_memes/accuracy: 0.6120, val/hateful_memes/binary_f1: 0.4393, val/hateful_memes/roc_auc: 0.6881\n",
      "\u001b[32m2021-05-04T05:08:27 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 02m 14s 500ms\n"
     ]
    }
   ],
   "source": [
    "# Baseline on dev_seen.jsonl ViLBERT\n",
    "!mmf_run config=projects/hateful_memes/configs/vilbert/defaults.yaml \\\n",
    "  model=vilbert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=val \\\n",
    "  checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct \\\n",
    "  checkpoint.resume_pretrained=False \\\n",
    "  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMmfxSpc46fPEfEmxHsqkna",
   "collapsed_sections": [],
   "name": "Pretrained ViLBERT + Visual BERT (Hateful and Election Memes).ipynb",
   "provenance": [
    {
     "file_id": "1ZZHsoTlNErUrhFCbmS6pTz7GLvHYw_Wp",
     "timestamp": 1620011798791
    },
    {
     "file_id": "1zJ_PgMxUEzdqlZoJPeoOv0T0SYkbYsM7",
     "timestamp": 1620005734396
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
