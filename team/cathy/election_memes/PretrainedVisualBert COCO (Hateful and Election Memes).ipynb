{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50069,
     "status": "ok",
     "timestamp": 1619590854479,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "jSPOfKQom8AF",
    "outputId": "5140b945-41fe-4ec3-88c5-838dda699509"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50063,
     "status": "ok",
     "timestamp": 1619590854480,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "SlKoJSAknjuq",
    "outputId": "027a6c63-a31a-4099-e4be-5aae5bab71ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/colab\n"
     ]
    }
   ],
   "source": [
    "%cd gdrive/MyDrive/colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50414,
     "status": "ok",
     "timestamp": 1619590854834,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "o-zreWCXnpGw",
    "outputId": "e602472f-576b-442d-b30a-052748918d0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf\n"
     ]
    }
   ],
   "source": [
    "%cd mmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 271245,
     "status": "ok",
     "timestamp": 1619591075669,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "6hM3BET1nrBO",
    "outputId": "9d14b851-32f9-42fc-cfed-233bcd618994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting datasets==1.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 10.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.0.2)\n",
      "Collecting GitPython==3.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 13.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.0)\n",
      "Collecting transformers==3.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 11.8MB/s \n",
      "\u001b[?25hCollecting demjson==2.2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/67/6db789e2533158963d4af689f961b644ddd9200615b8ce92d6cad695c65a/demjson-2.2.4.tar.gz (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 45.1MB/s \n",
      "\u001b[?25hCollecting torchtext==0.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 11.5MB/s \n",
      "\u001b[?25hCollecting torch<=1.8.1,>=1.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n",
      "\u001b[K     |████████████████████████████████| 804.1MB 22kB/s \n",
      "\u001b[?25hCollecting lmdb==0.98\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/5c/d56dbc2532ecf14fa004c543927500c0f645eaca8bd7ec39420c7546396a/lmdb-0.98.tar.gz (869kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 52.1MB/s \n",
      "\u001b[?25hCollecting matplotlib==3.3.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/3d/db9a6b3c83c9511301152dbb64a029c3a4313c86eaef12c237b13ecf91d6/matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6MB 48.7MB/s \n",
      "\u001b[?25hCollecting pytorch-lightning==1.2.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/13/fb401b8f9d9c5e2aa08769d230bb401bf11dee0bc93e069d7337a4201ec8/pytorch_lightning-1.2.7-py3-none-any.whl (830kB)\n",
      "\u001b[K     |████████████████████████████████| 839kB 51.2MB/s \n",
      "\u001b[?25hCollecting omegaconf==2.0.6\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
      "Collecting fasttext==0.9.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.0MB/s \n",
      "\u001b[?25hCollecting tqdm<4.50.0,>=4.43.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n",
      "\u001b[?25hCollecting torchvision<=0.9.1,>=0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/8a/82062a33b5eb7f696bf23f8ccf04bf6fc81d1a4972740fb21c2569ada0a6/torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4MB)\n",
      "\u001b[K     |████████████████████████████████| 17.4MB 342kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.19.5)\n",
      "Collecting nltk==3.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 49.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n",
      "Collecting iopath==0.1.7\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/d5/1c70fea7632640e8a9fb5a176676e555238119b3e7ee8b6dc49980ec5769/iopath-0.1.7-py3-none-any.whl\n",
      "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n",
      "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n",
      "Collecting ftfy==5.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 11.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (1.1.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.10.1)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.70.11.1)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.3.3)\n",
      "Collecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 58.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (56.0.0)\n",
      "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (0.29.22)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 11.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (0.22.2.post1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.12.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (2019.12.20)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 42.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.0.12)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 53.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (20.9)\n",
      "Collecting tokenizers==0.9.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/e7/edf655ae34925aeaefb7b7fcc3dd0887d2a1203ee6b0df4d1170d1a19d4f/tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 43.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->mmf==1.0.0rc12) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.8.1,>=1.6.0->mmf==1.0.0rc12) (3.7.4.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (7.1.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (0.10.0)\n",
      "Collecting fsspec[http]>=0.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 62.0MB/s \n",
      "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/99/dc59248df9a50349d537ffb3403c1bdc1fa69077109d46feaa0843488001/torchmetrics-0.3.1-py3-none-any.whl (271kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 61.4MB/s \n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 54.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.7->mmf==1.0.0rc12) (2.4.1)\n",
      "Collecting PyYAML!=5.4.*,>=5.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 57.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (2.6.2)\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1->mmf==1.0.0rc12) (2018.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.2.1->mmf==1.0.0rc12) (3.4.1)\n",
      "Collecting smmap<5,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (7.1.2)\n",
      "Collecting aiohttp; extra == \"http\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 51.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.4.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.32.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.8.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.36.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.28.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (20.3.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 59.7MB/s \n",
      "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 60.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (4.2.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.4.8)\n",
      "Building wheels for collected packages: demjson, lmdb, fasttext, nltk, ftfy, future, PyYAML\n",
      "  Building wheel for demjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for demjson: filename=demjson-2.2.4-cp37-none-any.whl size=73546 sha256=8fa4f2dad54856ca5b0f021a24b15d890ffd5b9fcd07c39013888582eb35ec45\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/d2/ab/a54fb5ea53ac3badba098160e8452fa126a51febda80440ded\n",
      "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=219686 sha256=511c3f858db68a6b8292399c40ba0a01138157e660399ff00ba159d644e11203\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/97/8c/7721e4b6b0ac723c6cc45ecca60599a80f75e2367330647390\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2466809 sha256=7a938f4362e60a46d109dbdf1626a20a212b5a490ea023270433bea90872b99f\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449905 sha256=bcf88a660cc14ad6569d143c69ae1bb97055a8a69bf2190e71ab86ba947b4f65\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-5.8-cp37-none-any.whl size=45613 sha256=b3435e0c9b570750df5e8f75e754c812eb00f6b29ba02fd9d8040d8fb74462cd\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=5f924dbc5c6d7d90abf2976573cd8208ab3d2ee8a029cbc475ef72e36a55fdaf\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=349c1014d44c2b69963fe6672afcd007b0314fe073266ab2e161556630ee60a8\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "Successfully built demjson lmdb fasttext nltk ftfy future PyYAML\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tqdm, xxhash, datasets, smmap, gitdb, GitPython, sentencepiece, sacremoses, tokenizers, transformers, demjson, torch, torchtext, lmdb, matplotlib, multidict, async-timeout, yarl, aiohttp, fsspec, torchmetrics, future, PyYAML, pytorch-lightning, omegaconf, fasttext, torchvision, nltk, portalocker, iopath, ftfy, mmf\n",
      "  Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "  Found existing installation: torch 1.8.1+cu101\n",
      "    Uninstalling torch-1.8.1+cu101:\n",
      "      Successfully uninstalled torch-1.8.1+cu101\n",
      "  Found existing installation: torchtext 0.9.1\n",
      "    Uninstalling torchtext-0.9.1:\n",
      "      Successfully uninstalled torchtext-0.9.1\n",
      "  Found existing installation: lmdb 0.99\n",
      "    Uninstalling lmdb-0.99:\n",
      "      Successfully uninstalled lmdb-0.99\n",
      "  Found existing installation: matplotlib 3.2.2\n",
      "    Uninstalling matplotlib-3.2.2:\n",
      "      Successfully uninstalled matplotlib-3.2.2\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Found existing installation: torchvision 0.9.1+cu101\n",
      "    Uninstalling torchvision-0.9.1+cu101:\n",
      "      Successfully uninstalled torchvision-0.9.1+cu101\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "  Running setup.py develop for mmf\n",
      "Successfully installed GitPython-3.1.0 PyYAML-5.3.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.2.1 demjson-2.2.4 fasttext-0.9.1 fsspec-2021.4.0 ftfy-5.8 future-0.18.2 gitdb-4.0.7 iopath-0.1.7 lmdb-0.98 matplotlib-3.3.4 mmf multidict-5.1.0 nltk-3.4.5 omegaconf-2.0.6 portalocker-2.3.0 pytorch-lightning-1.2.7 sacremoses-0.0.45 sentencepiece-0.1.95 smmap-4.0.0 tokenizers-0.9.2 torch-1.8.1 torchmetrics-0.3.1 torchtext-0.5.0 torchvision-0.9.1 tqdm-4.49.0 transformers-3.4.0 xxhash-2.0.2 yarl-1.6.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --editable ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 275972,
     "status": "ok",
     "timestamp": 1619591080399,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "x4PQkXOxnsOA",
    "outputId": "bfb0c1e3-94f0-4db2-e282-0cd9ca236156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyYAML==5.3.1 in /usr/local/lib/python3.7/dist-packages (5.3.1)\n",
      "Collecting imgaug==0.2.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
      "\r",
      "\u001b[K     |▌                               | 10kB 24.9MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 20kB 16.1MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 30kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 40kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 51kB 7.7MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 61kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 71kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 81kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 92kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 102kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 112kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 122kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 133kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 143kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 153kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 163kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 174kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 184kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 194kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 204kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 215kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 225kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 235kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 245kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 256kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 266kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 276kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 286kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 296kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 307kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 317kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 327kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 337kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 348kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 358kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 368kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 378kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 389kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 399kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 409kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 419kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 430kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 440kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 450kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 460kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 471kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 481kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 491kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 501kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 512kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 522kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 532kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 542kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 552kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 563kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 573kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 583kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 593kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 604kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 614kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 624kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 634kB 7.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.4.1)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (0.16.2)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.15.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (1.1.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (3.3.4)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.5.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.4.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (7.1.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (0.10.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.6) (4.4.2)\n",
      "Building wheels for collected packages: imgaug\n",
      "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for imgaug: filename=imgaug-0.2.6-cp37-none-any.whl size=654019 sha256=0ea845552882cd4a78953201641a2109b33fe68718b4bd9a1df1d93ff23de3ef\n",
      "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
      "Successfully built imgaug\n",
      "Installing collected packages: imgaug\n",
      "  Found existing installation: imgaug 0.2.9\n",
      "    Uninstalling imgaug-0.2.9:\n",
      "      Successfully uninstalled imgaug-0.2.9\n",
      "Successfully installed imgaug-0.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install PyYAML==5.3.1 imgaug==0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 275969,
     "status": "ok",
     "timestamp": 1619591080400,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "TSrAzqf9nvUO",
    "outputId": "403a5712-bbfd-481b-fb65-9a9255a06813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/colab\n"
     ]
    }
   ],
   "source": [
    "%cd /content/gdrive/MyDrive/colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 498714,
     "status": "ok",
     "timestamp": 1619591303148,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "4qRna4BrnxaZ",
    "outputId": "c982fefb-4759-45a7-ac96-3aa9d08163f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 06:24:52.731268: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Data folder is /root/.cache/torch/mmf/data\n",
      "Zip path is ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Starting checksum for XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Checksum successful\n",
      "Copying ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Unzipping ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Extracting the zip can take time. Sit back and relax.\n",
      "Moving train.jsonl\n",
      "Moving dev_seen.jsonl\n",
      "Moving test_seen.jsonl\n",
      "Moving dev_unseen.jsonl\n",
      "Moving test_unseen.jsonl\n",
      "Moving img\n"
     ]
    }
   ],
   "source": [
    "!mmf_convert_hm --zip_file=\"./XjiOc5ycDBRRNwbhRlgH.zip\" --password=REDACTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1125935,
     "status": "ok",
     "timestamp": 1619591930372,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "UlF9T15Wt92g",
    "outputId": "d958736f-f3ab-483d-cf3a-80f43339755d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 06:28:29.306347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-04-28T06:29:00 | matplotlib.font_manager: \u001b[0mGenerating new fontManager, this may take some time...\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.coco\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=visual_bert.pretrained.coco', 'checkpoint.resume_pretrained=False'])\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf_cli.run: \u001b[0mUsing seed 16078365\n",
      "\u001b[32m2021-04-28T06:29:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100% 10.3G/10.3G [02:26<00:00, 70.1MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n",
      "Downloading extras.tar.gz: 100% 211k/211k [00:00<00:00, 518kB/s]  \n",
      "[ Starting checksum for extras.tar.gz]\n",
      "[ Checksum successful for extras.tar.gz]\n",
      "Unpacking extras.tar.gz\n",
      "\u001b[32m2021-04-28T06:37:37 | filelock: \u001b[0mLock 140183445904144 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "Downloading: 100% 433/433 [00:00<00:00, 448kB/s]\n",
      "\u001b[32m2021-04-28T06:37:37 | filelock: \u001b[0mLock 140183445904144 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "\u001b[32m2021-04-28T06:37:37 | filelock: \u001b[0mLock 140183479745168 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 899kB/s]\n",
      "\u001b[32m2021-04-28T06:37:38 | filelock: \u001b[0mLock 140183479745168 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:37:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:37:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T06:37:38 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:37:38 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:37:38 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:37:38 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2021-04-28T06:37:38 | filelock: \u001b[0mLock 140183444531536 acquired on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Downloading: 100% 440M/440M [00:12<00:00, 35.9MB/s]\n",
      "\u001b[32m2021-04-28T06:37:51 | filelock: \u001b[0mLock 140183444531536 released on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-04-28T06:38:00 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-04-28T06:38:00 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-04-28T06:38:00 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.coco_train_val.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.coco.defaults/visual_bert.pretrained.coco_train_val.tar.gz ]\n",
      "Downloading visual_bert.pretrained.coco_train_val.tar.gz: 100% 415M/415M [00:06<00:00, 65.0MB/s]\n",
      "[ Starting checksum for visual_bert.pretrained.coco_train_val.tar.gz]\n",
      "[ Checksum successful for visual_bert.pretrained.coco_train_val.tar.gz]\n",
      "Unpacking visual_bert.pretrained.coco_train_val.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:14 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:14 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:14 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:14 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:14 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids', 'model.classifier.0.dense.weight', 'model.classifier.0.dense.bias', 'model.classifier.0.LayerNorm.weight', 'model.classifier.0.LayerNorm.bias', 'model.classifier.1.weight', 'model.classifier.1.bias'] in the checkpoint.\n",
      "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
      "Unexpected keys if any: ['model.cls.predictions.bias', 'model.cls.predictions.transform.dense.weight', 'model.cls.predictions.transform.dense.bias', 'model.cls.predictions.transform.LayerNorm.weight', 'model.cls.predictions.transform.LayerNorm.bias', 'model.cls.predictions.decoder.weight', 'model.cls.seq_relationship.weight', 'model.cls.seq_relationship.bias']\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:14 | mmf.utils.checkpoint: \u001b[0mUnexpected keys in state dict: ['model.cls.predictions.bias', 'model.cls.predictions.transform.dense.weight', 'model.cls.predictions.transform.dense.bias', 'model.cls.predictions.transform.LayerNorm.weight', 'model.cls.predictions.transform.LayerNorm.bias', 'model.cls.predictions.decoder.weight', 'model.cls.seq_relationship.weight', 'model.cls.seq_relationship.bias'] \n",
      "This is usually not a problem with pretrained models, but if this is your own model, please double check. \n",
      "If you think this is an issue, please open up a bug at MMF GitHub.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:14 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:14 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:14 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:38:14 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2021-04-28T06:38:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-04-28T06:38:14 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-04-28T06:38:14 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-04-28T06:38:14 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-04-28T06:38:14 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-04-28T06:38:14 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-04-28T06:38:14 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2021-04-28T06:38:14 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-04-28T06:38:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100% 9/9 [00:33<00:00,  3.73s/it]\n",
      "\u001b[32m2021-04-28T06:38:48 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 0.7582, val/total_loss: 0.7582, val/hateful_memes/accuracy: 0.4130, val/hateful_memes/binary_f1: 0.5262, val/hateful_memes/roc_auc: 0.5200\n",
      "\u001b[32m2021-04-28T06:38:48 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 47s 527ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/from_coco.yaml \\\n",
    "  model=visual_bert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=val \\\n",
    "  checkpoint.resume_zoo=visual_bert.pretrained.coco \\\n",
    "  checkpoint.resume_pretrained=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1126089,
     "status": "ok",
     "timestamp": 1619591930527,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "91xRv19Pnz_a"
   },
   "outputs": [],
   "source": [
    "!cd /root/.cache/torch/mmf/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1126652,
     "status": "ok",
     "timestamp": 1619591931091,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "JROCKvp4n1r0"
   },
   "outputs": [],
   "source": [
    "!cp /content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/train_hateful_and_election.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwAfY1esqz6N"
   },
   "source": [
    "Using pretrained Visual Bert model on COCO. Fine-tuning on all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22125484,
     "status": "ok",
     "timestamp": 1619614060706,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "j2Lhcy3fn7M0",
    "outputId": "758e7f59-9aca-45e6-cd41-2e34b530df76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 06:38:54.860184: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/gdrive/MyDrive/colab/pretrained_visualbertcoco_election_memes/\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.coco\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_hateful_and_election.jsonl\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_seen.jsonl\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.train[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.val[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.test[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf: \u001b[0mLogging to: /content/gdrive/MyDrive/colab/pretrained_visualbertcoco_election_memes/train.log\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.batch_size=32', 'env.save_dir=/content/gdrive/MyDrive/colab/pretrained_visualbertcoco_election_memes/', 'checkpoint.resume_zoo=visual_bert.pretrained.coco', 'checkpoint.resume_pretrained=True', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_hateful_and_election.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl', 'dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb'])\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf_cli.run: \u001b[0mUsing seed 858100\n",
      "\u001b[32m2021-04-28T06:39:00 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:39:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:39:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T06:39:02 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:39:02 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:39:02 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:39:02 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-04-28T06:39:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-04-28T06:39:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:39:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:39:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-04-28T06:39:09 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:39:10 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:39:10 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:39:10 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:39:10 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2021-04-28T06:39:10 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2021-04-28T06:44:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.7088, train/hateful_memes/cross_entropy/avg: 0.7088, train/total_loss: 0.7088, train/total_loss/avg: 0.7088, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 0.32, time: 05m 15s 761ms, time_since_start: 05m 17s 446ms, eta: 19h 30m 58s 136ms\n",
      "\u001b[32m2021-04-28T06:45:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6264, train/hateful_memes/cross_entropy/avg: 0.6676, train/total_loss: 0.6264, train/total_loss/avg: 0.6676, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 660ms, time_since_start: 06m 46s 107ms, eta: 05h 27m 17s 309ms\n",
      "\u001b[32m2021-04-28T06:47:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6264, train/hateful_memes/cross_entropy/avg: 0.6344, train/total_loss: 0.6264, train/total_loss/avg: 0.6344, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.09, time: 01m 32s 464ms, time_since_start: 08m 18s 572ms, eta: 05h 39m 45s 839ms\n",
      "\u001b[32m2021-04-28T06:48:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.5748, train/hateful_memes/cross_entropy/avg: 0.6195, train/total_loss: 0.5748, train/total_loss/avg: 0.6195, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.16, time: 01m 26s 922ms, time_since_start: 09m 45s 495ms, eta: 05h 17m 55s 739ms\n",
      "\u001b[32m2021-04-28T06:50:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.5748, train/hateful_memes/cross_entropy/avg: 0.5840, train/total_loss: 0.5748, train/total_loss/avg: 0.5840, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 016ms, time_since_start: 11m 14s 511ms, eta: 05h 24m 04s 820ms\n",
      "\u001b[32m2021-04-28T06:51:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.5681, train/hateful_memes/cross_entropy/avg: 0.5346, train/total_loss: 0.5681, train/total_loss/avg: 0.5346, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 256ms, time_since_start: 12m 46s 767ms, eta: 05h 34m 18s 683ms\n",
      "\u001b[32m2021-04-28T06:53:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.5681, train/hateful_memes/cross_entropy/avg: 0.5290, train/total_loss: 0.5681, train/total_loss/avg: 0.5290, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 140ms, time_since_start: 14m 14s 908ms, eta: 05h 17m 54s 382ms\n",
      "\u001b[32m2021-04-28T06:54:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.4954, train/hateful_memes/cross_entropy/avg: 0.4959, train/total_loss: 0.4954, train/total_loss/avg: 0.4959, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 191ms, time_since_start: 15m 44s 100ms, eta: 05h 20m 11s 092ms\n",
      "\u001b[32m2021-04-28T06:56:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.4954, train/hateful_memes/cross_entropy/avg: 0.4514, train/total_loss: 0.4954, train/total_loss/avg: 0.4514, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 296ms, time_since_start: 17m 16s 397ms, eta: 05h 29m 46s 245ms\n",
      "\u001b[32m2021-04-28T06:57:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T06:57:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:01:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:01:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:01:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.4954, train/hateful_memes/cross_entropy/avg: 0.4585, train/total_loss: 0.4954, train/total_loss/avg: 0.4585, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 0.31, time: 05m 26s 043ms, time_since_start: 22m 42s 440ms, eta: 19h 19m 24s 598ms\n",
      "\u001b[32m2021-04-28T07:01:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T07:01:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:01:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:01:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:02:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:02:17 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-04-28T07:02:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:02:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:02:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 1.0682, val/total_loss: 1.0682, val/hateful_memes/accuracy: 0.6100, val/hateful_memes/binary_f1: 0.4987, val/hateful_memes/roc_auc: 0.7033, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 54s 458ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.703269\n",
      "\u001b[32m2021-04-28T07:04:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.4954, train/hateful_memes/cross_entropy/avg: 0.4452, train/total_loss: 0.4954, train/total_loss/avg: 0.4452, max mem: 9226.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 1.05, time: 01m 35s 100ms, time_since_start: 25m 12s 003ms, eta: 05h 36m 34s 089ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:05:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:05:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:05:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.4419, train/hateful_memes/cross_entropy/avg: 0.4414, train/total_loss: 0.4419, train/total_loss/avg: 0.4414, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 957ms, time_since_start: 26m 42s 961ms, eta: 05h 20m 21s 974ms\n",
      "\u001b[32m2021-04-28T07:07:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.4419, train/hateful_memes/cross_entropy/avg: 0.4198, train/total_loss: 0.4419, train/total_loss/avg: 0.4198, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 440ms, time_since_start: 28m 12s 402ms, eta: 05h 13m 30s 359ms\n",
      "\u001b[32m2021-04-28T07:08:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.3995, train/hateful_memes/cross_entropy/avg: 0.4089, train/total_loss: 0.3995, train/total_loss/avg: 0.4089, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 141ms, time_since_start: 29m 42s 544ms, eta: 05h 14m 26s 363ms\n",
      "\u001b[32m2021-04-28T07:10:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.3995, train/hateful_memes/cross_entropy/avg: 0.3899, train/total_loss: 0.3995, train/total_loss/avg: 0.3899, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 1.11, time: 01m 30s 568ms, time_since_start: 31m 13s 112ms, eta: 05h 14m 23s 584ms\n",
      "\u001b[32m2021-04-28T07:11:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.3122, train/hateful_memes/cross_entropy/avg: 0.3750, train/total_loss: 0.3122, train/total_loss/avg: 0.3750, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 431ms, time_since_start: 32m 42s 544ms, eta: 05h 08m 55s 905ms\n",
      "\u001b[32m2021-04-28T07:13:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.3122, train/hateful_memes/cross_entropy/avg: 0.3641, train/total_loss: 0.3122, train/total_loss/avg: 0.3641, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 558ms, time_since_start: 34m 14s 102ms, eta: 05h 14m 43s 725ms\n",
      "\u001b[32m2021-04-28T07:14:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.2878, train/hateful_memes/cross_entropy/avg: 0.3521, train/total_loss: 0.2878, train/total_loss/avg: 0.3521, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 1.08, time: 01m 33s 272ms, time_since_start: 35m 47s 374ms, eta: 05h 19m 02s 452ms\n",
      "\u001b[32m2021-04-28T07:16:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.2878, train/hateful_memes/cross_entropy/avg: 0.3390, train/total_loss: 0.2878, train/total_loss/avg: 0.3390, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 615ms, time_since_start: 37m 17s 990ms, eta: 05h 08m 25s 210ms\n",
      "\u001b[32m2021-04-28T07:17:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T07:17:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:18:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:18:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:18:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.2677, train/hateful_memes/cross_entropy/avg: 0.3314, train/total_loss: 0.2677, train/total_loss/avg: 0.3314, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 0.91, time: 01m 50s 667ms, time_since_start: 39m 08s 658ms, eta: 06h 14m 47s 597ms\n",
      "\u001b[32m2021-04-28T07:18:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T07:18:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:18:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:18:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:18:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:18:46 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-04-28T07:18:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:19:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:19:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.1368, val/total_loss: 1.1368, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5714, val/hateful_memes/roc_auc: 0.7353, num_updates: 2000, epoch: 7, iterations: 2000, max_updates: 22000, val_time: 58s 676ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:20:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:20:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:21:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.2642, train/hateful_memes/cross_entropy/avg: 0.3232, train/total_loss: 0.2642, train/total_loss/avg: 0.3232, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 0.79, time: 02m 06s 010ms, time_since_start: 42m 13s 350ms, eta: 07h 04m 37s 322ms\n",
      "\u001b[32m2021-04-28T07:22:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.1907, train/hateful_memes/cross_entropy/avg: 0.3168, train/total_loss: 0.1907, train/total_loss/avg: 0.3168, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 922ms, time_since_start: 43m 42s 273ms, eta: 04h 58m 08s 421ms\n",
      "\u001b[32m2021-04-28T07:24:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.1878, train/hateful_memes/cross_entropy/avg: 0.3063, train/total_loss: 0.1878, train/total_loss/avg: 0.3063, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 1.12, time: 01m 29s 029ms, time_since_start: 45m 11s 303ms, eta: 04h 56m 59s 425ms\n",
      "\u001b[32m2021-04-28T07:25:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.1825, train/hateful_memes/cross_entropy/avg: 0.2954, train/total_loss: 0.1825, train/total_loss/avg: 0.2954, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 926ms, time_since_start: 46m 42s 229ms, eta: 05h 01m 46s 711ms\n",
      "\u001b[32m2021-04-28T07:27:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.1598, train/hateful_memes/cross_entropy/avg: 0.2867, train/total_loss: 0.1598, train/total_loss/avg: 0.2867, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 1.12, time: 01m 29s 086ms, time_since_start: 48m 11s 316ms, eta: 04h 54m 09s 918ms\n",
      "\u001b[32m2021-04-28T07:28:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.1582, train/hateful_memes/cross_entropy/avg: 0.2765, train/total_loss: 0.1582, train/total_loss/avg: 0.2765, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 827ms, time_since_start: 49m 40s 143ms, eta: 04h 51m 48s 243ms\n",
      "\u001b[32m2021-04-28T07:30:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.1505, train/hateful_memes/cross_entropy/avg: 0.2669, train/total_loss: 0.1505, train/total_loss/avg: 0.2669, max mem: 9226.0, experiment: run, epoch: 10, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 370ms, time_since_start: 51m 11s 514ms, eta: 04h 58m 36s 645ms\n",
      "\u001b[32m2021-04-28T07:31:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.1472, train/hateful_memes/cross_entropy/avg: 0.2576, train/total_loss: 0.1472, train/total_loss/avg: 0.2576, max mem: 9226.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 1.12, time: 01m 29s 058ms, time_since_start: 52m 40s 572ms, eta: 04h 49m 32s 787ms\n",
      "\u001b[32m2021-04-28T07:33:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.1472, train/hateful_memes/cross_entropy/avg: 0.2491, train/total_loss: 0.1472, train/total_loss/avg: 0.2491, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 706ms, time_since_start: 54m 12s 279ms, eta: 04h 56m 36s 195ms\n",
      "\u001b[32m2021-04-28T07:34:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T07:34:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:35:01 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:35:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:35:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.1243, train/hateful_memes/cross_entropy/avg: 0.2431, train/total_loss: 0.1243, train/total_loss/avg: 0.2431, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 0.88, time: 01m 53s 279ms, time_since_start: 56m 05s 559ms, eta: 06h 04m 27s 534ms\n",
      "\u001b[32m2021-04-28T07:35:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T07:35:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:35:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:35:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:35:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:35:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:35:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:35:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.5222, val/total_loss: 1.5222, val/hateful_memes/accuracy: 0.6300, val/hateful_memes/binary_f1: 0.4986, val/hateful_memes/roc_auc: 0.7321, num_updates: 3000, epoch: 11, iterations: 3000, max_updates: 22000, val_time: 42s 017ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T07:37:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1037, train/hateful_memes/cross_entropy/avg: 0.2353, train/total_loss: 0.1037, train/total_loss/avg: 0.2353, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 1.06, time: 01m 34s 039ms, time_since_start: 58m 21s 618ms, eta: 05h 57s 806ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:38:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:38:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:39:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.0779, train/hateful_memes/cross_entropy/avg: 0.2280, train/total_loss: 0.0779, train/total_loss/avg: 0.2280, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 1.09, time: 01m 32s 064ms, time_since_start: 59m 53s 683ms, eta: 04h 53m 05s 132ms\n",
      "\u001b[32m2021-04-28T07:40:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0759, train/hateful_memes/cross_entropy/avg: 0.2213, train/total_loss: 0.0759, train/total_loss/avg: 0.2213, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 366ms, time_since_start: 01h 01m 22s 049ms, eta: 04h 39m 48s 907ms\n",
      "\u001b[32m2021-04-28T07:41:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0704, train/hateful_memes/cross_entropy/avg: 0.2155, train/total_loss: 0.0704, train/total_loss/avg: 0.2155, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 335ms, time_since_start: 01h 02m 50s 385ms, eta: 04h 38m 13s 263ms\n",
      "\u001b[32m2021-04-28T07:43:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0445, train/hateful_memes/cross_entropy/avg: 0.2094, train/total_loss: 0.0445, train/total_loss/avg: 0.2094, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 417ms, time_since_start: 01h 04m 20s 802ms, eta: 04h 43m 14s 806ms\n",
      "\u001b[32m2021-04-28T07:44:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0407, train/hateful_memes/cross_entropy/avg: 0.2047, train/total_loss: 0.0407, train/total_loss/avg: 0.2047, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 104ms, time_since_start: 01h 05m 48s 906ms, eta: 04h 34m 30s 585ms\n",
      "\u001b[32m2021-04-28T07:46:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0373, train/hateful_memes/cross_entropy/avg: 0.2002, train/total_loss: 0.0373, train/total_loss/avg: 0.2002, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 853ms, time_since_start: 01h 07m 17s 760ms, eta: 04h 35m 20s 383ms\n",
      "\u001b[32m2021-04-28T07:47:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0373, train/hateful_memes/cross_entropy/avg: 0.1977, train/total_loss: 0.0373, train/total_loss/avg: 0.1977, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 668ms, time_since_start: 01h 08m 49s 429ms, eta: 04h 42m 30s 673ms\n",
      "\u001b[32m2021-04-28T07:49:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0240, train/hateful_memes/cross_entropy/avg: 0.1928, train/total_loss: 0.0240, train/total_loss/avg: 0.1928, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 158ms, time_since_start: 01h 10m 17s 587ms, eta: 04h 30m 11s 924ms\n",
      "\u001b[32m2021-04-28T07:50:55 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T07:50:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:51:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:51:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:51:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0240, train/hateful_memes/cross_entropy/avg: 0.1920, train/total_loss: 0.0240, train/total_loss/avg: 0.1920, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 0.93, time: 01m 48s 839ms, time_since_start: 01h 12m 06s 427ms, eta: 05h 31m 44s 592ms\n",
      "\u001b[32m2021-04-28T07:51:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T07:51:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:51:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:51:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:51:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:51:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:51:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:51:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.9803, val/total_loss: 1.9803, val/hateful_memes/accuracy: 0.6200, val/hateful_memes/binary_f1: 0.5274, val/hateful_memes/roc_auc: 0.7023, num_updates: 4000, epoch: 14, iterations: 4000, max_updates: 22000, val_time: 41s 493ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:52:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:52:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:53:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0240, train/hateful_memes/cross_entropy/avg: 0.1903, train/total_loss: 0.0240, train/total_loss/avg: 0.1903, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 1.08, time: 01m 33s 108ms, time_since_start: 01h 14m 21s 039ms, eta: 04h 42m 13s 126ms\n",
      "\u001b[32m2021-04-28T07:54:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0216, train/hateful_memes/cross_entropy/avg: 0.1859, train/total_loss: 0.0216, train/total_loss/avg: 0.1859, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 935ms, time_since_start: 01h 15m 49s 974ms, eta: 04h 28m 03s 802ms\n",
      "\u001b[32m2021-04-28T07:56:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0216, train/hateful_memes/cross_entropy/avg: 0.1823, train/total_loss: 0.0216, train/total_loss/avg: 0.1823, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 1.15, time: 01m 27s 828ms, time_since_start: 01h 17m 17s 803ms, eta: 04h 23m 14s 372ms\n",
      "\u001b[32m2021-04-28T07:57:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0174, train/hateful_memes/cross_entropy/avg: 0.1782, train/total_loss: 0.0174, train/total_loss/avg: 0.1782, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 1.11, time: 01m 30s 987ms, time_since_start: 01h 18m 48s 790ms, eta: 04h 31m 10s 020ms\n",
      "\u001b[32m2021-04-28T07:59:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1743, train/total_loss: 0.0088, train/total_loss/avg: 0.1743, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 510ms, time_since_start: 01h 20m 18s 301ms, eta: 04h 25m 14s 936ms\n",
      "\u001b[32m2021-04-28T08:00:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1708, train/total_loss: 0.0088, train/total_loss/avg: 0.1708, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 128ms, time_since_start: 01h 21m 47s 429ms, eta: 04h 22m 36s 424ms\n",
      "\u001b[32m2021-04-28T08:02:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1674, train/total_loss: 0.0088, train/total_loss/avg: 0.1674, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 1.11, time: 01m 30s 671ms, time_since_start: 01h 23m 18s 101ms, eta: 04h 25m 37s 195ms\n",
      "\u001b[32m2021-04-28T08:03:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.1640, train/total_loss: 0.0088, train/total_loss/avg: 0.1640, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 011ms, time_since_start: 01h 24m 47s 112ms, eta: 04h 19m 14s 946ms\n",
      "\u001b[32m2021-04-28T08:05:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.1608, train/total_loss: 0.0101, train/total_loss/avg: 0.1608, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 602ms, time_since_start: 01h 26m 15s 715ms, eta: 04h 16m 33s 495ms\n",
      "\u001b[32m2021-04-28T08:06:55 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T08:06:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T08:07:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T08:07:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T08:07:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0067, train/hateful_memes/cross_entropy/avg: 0.1577, train/total_loss: 0.0067, train/total_loss/avg: 0.1577, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 0.88, time: 01m 54s 443ms, time_since_start: 01h 28m 10s 159ms, eta: 05h 29m 26s 757ms\n",
      "\u001b[32m2021-04-28T08:07:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T08:07:19 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:07:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:07:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:07:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T08:07:47 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T08:08:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T08:08:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 2.0718, val/total_loss: 2.0718, val/hateful_memes/accuracy: 0.6780, val/hateful_memes/binary_f1: 0.6083, val/hateful_memes/roc_auc: 0.7343, num_updates: 5000, epoch: 18, iterations: 5000, max_updates: 22000, val_time: 41s 286ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T08:09:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1548, train/total_loss: 0.0073, train/total_loss/avg: 0.1548, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 493ms, time_since_start: 01h 30m 22s 950ms, eta: 04h 21m 49s 814ms\n",
      "\u001b[32m2021-04-28T08:11:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.1540, train/total_loss: 0.0101, train/total_loss/avg: 0.1540, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 716ms, time_since_start: 01h 31m 51s 667ms, eta: 04h 12m 22s 791ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:11:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:11:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:12:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0121, train/hateful_memes/cross_entropy/avg: 0.1515, train/total_loss: 0.0121, train/total_loss/avg: 0.1515, max mem: 9226.0, experiment: run, epoch: 19, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 111ms, time_since_start: 01h 33m 22s 778ms, eta: 04h 17m 39s 037ms\n",
      "\u001b[32m2021-04-28T08:14:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.1487, train/total_loss: 0.0101, train/total_loss/avg: 0.1487, max mem: 9226.0, experiment: run, epoch: 19, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 492ms, time_since_start: 01h 34m 51s 270ms, eta: 04h 08m 44s 727ms\n",
      "\u001b[32m2021-04-28T08:15:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.1461, train/total_loss: 0.0101, train/total_loss/avg: 0.1461, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 1.09, time: 01m 32s 580ms, time_since_start: 01h 36m 23s 850ms, eta: 04h 18m 40s 161ms\n",
      "\u001b[32m2021-04-28T08:17:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.1444, train/total_loss: 0.0101, train/total_loss/avg: 0.1444, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 1.15, time: 01m 27s 645ms, time_since_start: 01h 37m 51s 496ms, eta: 04h 03m 23s 829ms\n",
      "\u001b[32m2021-04-28T08:18:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1420, train/total_loss: 0.0073, train/total_loss/avg: 0.1420, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 119ms, time_since_start: 01h 39m 20s 615ms, eta: 04h 05m 58s 862ms\n",
      "\u001b[32m2021-04-28T08:20:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1397, train/total_loss: 0.0073, train/total_loss/avg: 0.1397, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 1.09, time: 01m 32s 224ms, time_since_start: 01h 40m 52s 840ms, eta: 04h 12m 59s 465ms\n",
      "\u001b[32m2021-04-28T08:21:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1374, train/total_loss: 0.0073, train/total_loss/avg: 0.1374, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 113ms, time_since_start: 01h 42m 21s 953ms, eta: 04h 02m 56s 793ms\n",
      "\u001b[32m2021-04-28T08:23:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T08:23:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T08:23:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T08:23:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T08:23:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.1351, train/total_loss: 0.0071, train/total_loss/avg: 0.1351, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 0.88, time: 01m 53s 271ms, time_since_start: 01h 44m 15s 225ms, eta: 05h 06m 53s 432ms\n",
      "\u001b[32m2021-04-28T08:23:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T08:23:24 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:23:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:23:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:23:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T08:23:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T08:24:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T08:24:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.0059, val/total_loss: 2.0059, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5403, val/hateful_memes/roc_auc: 0.7320, num_updates: 6000, epoch: 21, iterations: 6000, max_updates: 22000, val_time: 41s 042ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:25:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:25:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:25:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.1334, train/total_loss: 0.0071, train/total_loss/avg: 0.1334, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 1.06, time: 01m 34s 236ms, time_since_start: 01h 46m 30s 506ms, eta: 04h 13m 43s 330ms\n",
      "\u001b[32m2021-04-28T08:27:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.1313, train/total_loss: 0.0071, train/total_loss/avg: 0.1313, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 560ms, time_since_start: 01h 47m 59s 067ms, eta: 03h 56m 56s 471ms\n",
      "\u001b[32m2021-04-28T08:28:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0071, train/hateful_memes/cross_entropy/avg: 0.1293, train/total_loss: 0.0071, train/total_loss/avg: 0.1293, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 548ms, time_since_start: 01h 49m 27s 616ms, eta: 03h 55m 24s 616ms\n",
      "\u001b[32m2021-04-28T08:30:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1277, train/total_loss: 0.0073, train/total_loss/avg: 0.1277, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 606ms, time_since_start: 01h 50m 59s 222ms, eta: 04h 01m 59s 214ms\n",
      "\u001b[32m2021-04-28T08:31:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.1265, train/total_loss: 0.0101, train/total_loss/avg: 0.1265, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 641ms, time_since_start: 01h 52m 28s 864ms, eta: 03h 55m 16s 787ms\n",
      "\u001b[32m2021-04-28T08:33:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.1267, train/total_loss: 0.0101, train/total_loss/avg: 0.1267, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 1.11, time: 01m 30s 373ms, time_since_start: 01h 53m 59s 237ms, eta: 03h 55m 40s 157ms\n",
      "\u001b[32m2021-04-28T08:34:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.1257, train/total_loss: 0.0101, train/total_loss/avg: 0.1257, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 851ms, time_since_start: 01h 55m 31s 088ms, eta: 03h 57m 58s 066ms\n",
      "\u001b[32m2021-04-28T08:36:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0101, train/hateful_memes/cross_entropy/avg: 0.1239, train/total_loss: 0.0101, train/total_loss/avg: 0.1239, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 666ms, time_since_start: 01h 56m 59s 755ms, eta: 03h 48m 13s 052ms\n",
      "\u001b[32m2021-04-28T08:37:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.1223, train/total_loss: 0.0103, train/total_loss/avg: 0.1223, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 979ms, time_since_start: 01h 58m 28s 734ms, eta: 03h 47m 30s 830ms\n",
      "\u001b[32m2021-04-28T08:42:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T08:42:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T08:47:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T08:47:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T08:47:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.1206, train/total_loss: 0.0103, train/total_loss/avg: 0.1206, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 0.17, time: 10m 04s 349ms, time_since_start: 02h 08m 33s 084ms, eta: 25h 35m 02s 896ms\n",
      "\u001b[32m2021-04-28T08:47:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T08:47:42 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:47:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:47:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:47:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.2636, val/total_loss: 2.2636, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.5940, val/hateful_memes/roc_auc: 0.7135, num_updates: 7000, epoch: 25, iterations: 7000, max_updates: 22000, val_time: 17s 023ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T08:49:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0104, train/hateful_memes/cross_entropy/avg: 0.1190, train/total_loss: 0.0104, train/total_loss/avg: 0.1190, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 127ms, time_since_start: 02h 10m 18s 238ms, eta: 03h 42m 21s 056ms\n",
      "\u001b[32m2021-04-28T08:50:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0104, train/hateful_memes/cross_entropy/avg: 0.1178, train/total_loss: 0.0104, train/total_loss/avg: 0.1178, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 375ms, time_since_start: 02h 11m 47s 613ms, eta: 03h 43m 59s 170ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:51:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:51:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:52:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.1162, train/total_loss: 0.0103, train/total_loss/avg: 0.1162, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 404ms, time_since_start: 02h 13m 19s 018ms, eta: 03h 47m 31s 428ms\n",
      "\u001b[32m2021-04-28T08:53:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.1147, train/total_loss: 0.0103, train/total_loss/avg: 0.1147, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 714ms, time_since_start: 02h 14m 48s 732ms, eta: 03h 41m 47s 865ms\n",
      "\u001b[32m2021-04-28T08:55:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.1132, train/total_loss: 0.0103, train/total_loss/avg: 0.1132, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 432ms, time_since_start: 02h 16m 18s 165ms, eta: 03h 39m 35s 208ms\n",
      "\u001b[32m2021-04-28T08:56:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.1118, train/total_loss: 0.0103, train/total_loss/avg: 0.1118, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 1.09, time: 01m 32s 158ms, time_since_start: 02h 17m 50s 323ms, eta: 03h 44m 43s 140ms\n",
      "\u001b[32m2021-04-28T08:58:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.1104, train/total_loss: 0.0103, train/total_loss/avg: 0.1104, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 1.11, time: 01m 30s 271ms, time_since_start: 02h 19m 20s 595ms, eta: 03h 38m 35s 442ms\n",
      "\u001b[32m2021-04-28T08:59:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0046, train/hateful_memes/cross_entropy/avg: 0.1090, train/total_loss: 0.0046, train/total_loss/avg: 0.1090, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 717ms, time_since_start: 02h 20m 50s 313ms, eta: 03h 35m 43s 719ms\n",
      "\u001b[32m2021-04-28T09:01:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0046, train/hateful_memes/cross_entropy/avg: 0.1076, train/total_loss: 0.0046, train/total_loss/avg: 0.1076, max mem: 9226.0, experiment: run, epoch: 28, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 942ms, time_since_start: 02h 22m 22s 256ms, eta: 03h 39m 31s 368ms\n",
      "\u001b[32m2021-04-28T09:03:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T09:03:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T09:03:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T09:03:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T09:03:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.1065, train/total_loss: 0.0103, train/total_loss/avg: 0.1065, max mem: 9226.0, experiment: run, epoch: 28, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 0.88, time: 01m 53s 404ms, time_since_start: 02h 24m 15s 660ms, eta: 04h 28m 50s 714ms\n",
      "\u001b[32m2021-04-28T09:03:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T09:03:24 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:03:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:03:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:03:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.6132, val/total_loss: 2.6132, val/hateful_memes/accuracy: 0.6320, val/hateful_memes/binary_f1: 0.4831, val/hateful_memes/roc_auc: 0.7070, num_updates: 8000, epoch: 28, iterations: 8000, max_updates: 22000, val_time: 22s 007ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:05:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:05:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:05:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0046, train/hateful_memes/cross_entropy/avg: 0.1052, train/total_loss: 0.0046, train/total_loss/avg: 0.1052, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 290ms, time_since_start: 02h 26m 09s 973ms, eta: 03h 37m 13s 678ms\n",
      "\u001b[32m2021-04-28T09:06:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0046, train/hateful_memes/cross_entropy/avg: 0.1039, train/total_loss: 0.0046, train/total_loss/avg: 0.1039, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 388ms, time_since_start: 02h 27m 38s 362ms, eta: 03h 26m 32s 786ms\n",
      "\u001b[32m2021-04-28T09:08:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1027, train/total_loss: 0.0037, train/total_loss/avg: 0.1027, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 410ms, time_since_start: 02h 29m 09s 772ms, eta: 03h 32m 03s 591ms\n",
      "\u001b[32m2021-04-28T09:09:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0037, train/hateful_memes/cross_entropy/avg: 0.1045, train/total_loss: 0.0037, train/total_loss/avg: 0.1045, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 1.08, time: 01m 33s 879ms, time_since_start: 02h 30m 43s 652ms, eta: 03h 36m 11s 885ms\n",
      "\u001b[32m2021-04-28T09:11:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1033, train/total_loss: 0.0033, train/total_loss/avg: 0.1033, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 590ms, time_since_start: 02h 32m 12s 242ms, eta: 03h 22m 31s 062ms\n",
      "\u001b[32m2021-04-28T09:12:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1021, train/total_loss: 0.0033, train/total_loss/avg: 0.1021, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 251ms, time_since_start: 02h 33m 42s 494ms, eta: 03h 24m 47s 221ms\n",
      "\u001b[32m2021-04-28T09:14:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0033, train/hateful_memes/cross_entropy/avg: 0.1012, train/total_loss: 0.0033, train/total_loss/avg: 0.1012, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 1.08, time: 01m 33s 362ms, time_since_start: 02h 35m 15s 856ms, eta: 03h 30m 15s 889ms\n",
      "\u001b[32m2021-04-28T09:15:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.1001, train/total_loss: 0.0026, train/total_loss/avg: 0.1001, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 561ms, time_since_start: 02h 36m 46s 418ms, eta: 03h 22m 25s 395ms\n",
      "\u001b[32m2021-04-28T09:17:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0989, train/total_loss: 0.0024, train/total_loss/avg: 0.0989, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 393ms, time_since_start: 02h 38m 16s 812ms, eta: 03h 20m 31s 038ms\n",
      "\u001b[32m2021-04-28T09:18:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T09:18:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T09:19:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T09:19:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T09:19:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.0978, train/total_loss: 0.0024, train/total_loss/avg: 0.0978, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 0.85, time: 01m 57s 272ms, time_since_start: 02h 40m 14s 084ms, eta: 04h 18m 09s 366ms\n",
      "\u001b[32m2021-04-28T09:19:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T09:19:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:19:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:19:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:19:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.7524, val/total_loss: 2.7524, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.5179, val/hateful_memes/roc_auc: 0.7266, num_updates: 9000, epoch: 32, iterations: 9000, max_updates: 22000, val_time: 23s 031ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T09:21:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.0968, train/total_loss: 0.0023, train/total_loss/avg: 0.0968, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 412ms, time_since_start: 02h 42m 05s 533ms, eta: 03h 13m 07s 722ms\n",
      "\u001b[32m2021-04-28T09:22:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.0957, train/total_loss: 0.0021, train/total_loss/avg: 0.0957, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 518ms, time_since_start: 02h 43m 36s 051ms, eta: 03h 16m 11s 711ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:23:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:23:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:24:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.0947, train/total_loss: 0.0019, train/total_loss/avg: 0.0947, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 868ms, time_since_start: 02h 45m 08s 919ms, eta: 03h 19m 42s 990ms\n",
      "\u001b[32m2021-04-28T09:25:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0937, train/total_loss: 0.0013, train/total_loss/avg: 0.0937, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 970ms, time_since_start: 02h 46m 38s 890ms, eta: 03h 11m 57s 729ms\n",
      "\u001b[32m2021-04-28T09:27:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0940, train/total_loss: 0.0013, train/total_loss/avg: 0.0940, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 843ms, time_since_start: 02h 48m 08s 734ms, eta: 03h 10m 10s 176ms\n",
      "\u001b[32m2021-04-28T09:28:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0931, train/total_loss: 0.0010, train/total_loss/avg: 0.0931, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 461ms, time_since_start: 02h 49m 41s 196ms, eta: 03h 14m 08s 657ms\n",
      "\u001b[32m2021-04-28T09:30:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0921, train/total_loss: 0.0010, train/total_loss/avg: 0.0921, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 434ms, time_since_start: 02h 51m 10s 630ms, eta: 03h 06m 16s 448ms\n",
      "\u001b[32m2021-04-28T09:31:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0912, train/total_loss: 0.0009, train/total_loss/avg: 0.0912, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 327ms, time_since_start: 02h 52m 40s 958ms, eta: 03h 06m 36s 293ms\n",
      "\u001b[32m2021-04-28T09:33:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0903, train/total_loss: 0.0009, train/total_loss/avg: 0.0903, max mem: 9226.0, experiment: run, epoch: 35, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 444ms, time_since_start: 02h 54m 12s 402ms, eta: 03h 07m 21s 802ms\n",
      "\u001b[32m2021-04-28T09:34:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T09:34:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T09:35:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T09:35:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T09:35:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0894, train/total_loss: 0.0009, train/total_loss/avg: 0.0894, max mem: 9226.0, experiment: run, epoch: 35, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 0.87, time: 01m 55s 791ms, time_since_start: 02h 56m 08s 194ms, eta: 03h 55m 17s 323ms\n",
      "\u001b[32m2021-04-28T09:35:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T09:35:17 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:35:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:35:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:35:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.1085, val/total_loss: 2.1085, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5326, val/hateful_memes/roc_auc: 0.6976, num_updates: 10000, epoch: 35, iterations: 10000, max_updates: 22000, val_time: 18s 610ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T09:37:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0885, train/total_loss: 0.0009, train/total_loss/avg: 0.0885, max mem: 9226.0, experiment: run, epoch: 35, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 070ms, time_since_start: 02h 57m 55s 877ms, eta: 02h 59m 28s 995ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:37:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:37:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:38:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0877, train/total_loss: 0.0009, train/total_loss/avg: 0.0877, max mem: 9226.0, experiment: run, epoch: 36, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 597ms, time_since_start: 02h 59m 27s 475ms, eta: 03h 03m 01s 494ms\n",
      "\u001b[32m2021-04-28T09:40:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0868, train/total_loss: 0.0009, train/total_loss/avg: 0.0868, max mem: 9226.0, experiment: run, epoch: 36, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 751ms, time_since_start: 03h 58s 226ms, eta: 02h 59m 47s 803ms\n",
      "\u001b[32m2021-04-28T09:41:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0868, train/total_loss: 0.0009, train/total_loss/avg: 0.0868, max mem: 9226.0, experiment: run, epoch: 36, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 846ms, time_since_start: 03h 02m 29s 073ms, eta: 02h 58m 26s 773ms\n",
      "\u001b[32m2021-04-28T09:43:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0859, train/total_loss: 0.0009, train/total_loss/avg: 0.0859, max mem: 9226.0, experiment: run, epoch: 37, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 614ms, time_since_start: 03h 04m 688ms, eta: 02h 58m 24s 293ms\n",
      "\u001b[32m2021-04-28T09:44:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0851, train/total_loss: 0.0009, train/total_loss/avg: 0.0851, max mem: 9226.0, experiment: run, epoch: 37, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 760ms, time_since_start: 03h 05m 31s 449ms, eta: 02h 55m 12s 302ms\n",
      "\u001b[32m2021-04-28T09:46:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0844, train/total_loss: 0.0009, train/total_loss/avg: 0.0844, max mem: 9226.0, experiment: run, epoch: 38, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 1.08, time: 01m 33s 480ms, time_since_start: 03h 07m 04s 929ms, eta: 02h 58m 52s 258ms\n",
      "\u001b[32m2021-04-28T09:47:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0836, train/total_loss: 0.0007, train/total_loss/avg: 0.0836, max mem: 9226.0, experiment: run, epoch: 38, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 116ms, time_since_start: 03h 08m 33s 045ms, eta: 02h 47m 06s 914ms\n",
      "\u001b[32m2021-04-28T09:49:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0829, train/total_loss: 0.0005, train/total_loss/avg: 0.0829, max mem: 9226.0, experiment: run, epoch: 38, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 569ms, time_since_start: 03h 10m 02s 614ms, eta: 02h 48m 21s 283ms\n",
      "\u001b[32m2021-04-28T09:50:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T09:50:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T09:50:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T09:51:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T09:51:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0821, train/total_loss: 0.0005, train/total_loss/avg: 0.0821, max mem: 9226.0, experiment: run, epoch: 39, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 0.85, time: 01m 57s 405ms, time_since_start: 03h 12m 019ms, eta: 03h 38m 41s 186ms\n",
      "\u001b[32m2021-04-28T09:51:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T09:51:09 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:51:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:51:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:51:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.7836, val/total_loss: 2.7836, val/hateful_memes/accuracy: 0.6380, val/hateful_memes/binary_f1: 0.4901, val/hateful_memes/roc_auc: 0.7182, num_updates: 11000, epoch: 39, iterations: 11000, max_updates: 22000, val_time: 19s 761ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T09:52:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0814, train/total_loss: 0.0005, train/total_loss/avg: 0.0814, max mem: 9226.0, experiment: run, epoch: 39, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 820ms, time_since_start: 03h 13m 48s 608ms, eta: 02h 43m 56s 374ms\n",
      "\u001b[32m2021-04-28T09:54:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0806, train/total_loss: 0.0005, train/total_loss/avg: 0.0806, max mem: 9226.0, experiment: run, epoch: 39, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 754ms, time_since_start: 03h 15m 19s 362ms, eta: 02h 45m 58s 259ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:55:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:55:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:56:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0800, train/total_loss: 0.0005, train/total_loss/avg: 0.0800, max mem: 9226.0, experiment: run, epoch: 40, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 954ms, time_since_start: 03h 16m 51s 316ms, eta: 02h 46m 36s 590ms\n",
      "\u001b[32m2021-04-28T09:57:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0793, train/total_loss: 0.0005, train/total_loss/avg: 0.0793, max mem: 9226.0, experiment: run, epoch: 40, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 384ms, time_since_start: 03h 18m 19s 701ms, eta: 02h 38m 38s 620ms\n",
      "\u001b[32m2021-04-28T09:58:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0786, train/total_loss: 0.0005, train/total_loss/avg: 0.0786, max mem: 9226.0, experiment: run, epoch: 40, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 133ms, time_since_start: 03h 19m 48s 834ms, eta: 02h 38m 28s 731ms\n",
      "\u001b[32m2021-04-28T10:00:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0780, train/total_loss: 0.0005, train/total_loss/avg: 0.0780, max mem: 9226.0, experiment: run, epoch: 41, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 338ms, time_since_start: 03h 21m 21s 172ms, eta: 02h 42m 36s 823ms\n",
      "\u001b[32m2021-04-28T10:01:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0774, train/total_loss: 0.0005, train/total_loss/avg: 0.0774, max mem: 9226.0, experiment: run, epoch: 41, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 304ms, time_since_start: 03h 22m 50s 477ms, eta: 02h 35m 45s 563ms\n",
      "\u001b[32m2021-04-28T10:03:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0767, train/total_loss: 0.0005, train/total_loss/avg: 0.0767, max mem: 9226.0, experiment: run, epoch: 41, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 160ms, time_since_start: 03h 24m 20s 637ms, eta: 02h 35m 43s 509ms\n",
      "\u001b[32m2021-04-28T10:05:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0761, train/total_loss: 0.0005, train/total_loss/avg: 0.0761, max mem: 9226.0, experiment: run, epoch: 42, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 690ms, time_since_start: 03h 25m 52s 328ms, eta: 02h 36m 48s 900ms\n",
      "\u001b[32m2021-04-28T10:06:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T10:06:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T10:06:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T10:06:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T10:06:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0754, train/total_loss: 0.0005, train/total_loss/avg: 0.0754, max mem: 9226.0, experiment: run, epoch: 42, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 0.89, time: 01m 52s 956ms, time_since_start: 03h 27m 45s 284ms, eta: 03h 11m 16s 357ms\n",
      "\u001b[32m2021-04-28T10:06:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T10:06:54 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:06:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:06:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:07:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.9253, val/total_loss: 2.9253, val/hateful_memes/accuracy: 0.6340, val/hateful_memes/binary_f1: 0.5414, val/hateful_memes/roc_auc: 0.7000, num_updates: 12000, epoch: 42, iterations: 12000, max_updates: 22000, val_time: 21s 060ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T10:08:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0748, train/total_loss: 0.0005, train/total_loss/avg: 0.0748, max mem: 9226.0, experiment: run, epoch: 42, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 148ms, time_since_start: 03h 29m 35s 497ms, eta: 02h 29m 26s 946ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:09:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:09:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:10:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0742, train/total_loss: 0.0003, train/total_loss/avg: 0.0742, max mem: 9226.0, experiment: run, epoch: 43, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 146ms, time_since_start: 03h 31m 07s 644ms, eta: 02h 32m 54s 830ms\n",
      "\u001b[32m2021-04-28T10:11:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0736, train/total_loss: 0.0003, train/total_loss/avg: 0.0736, max mem: 9226.0, experiment: run, epoch: 43, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 785ms, time_since_start: 03h 32m 37s 429ms, eta: 02h 27m 28s 510ms\n",
      "\u001b[32m2021-04-28T10:13:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0730, train/total_loss: 0.0003, train/total_loss/avg: 0.0730, max mem: 9226.0, experiment: run, epoch: 43, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 002ms, time_since_start: 03h 34m 07s 432ms, eta: 02h 26m 18s 521ms\n",
      "\u001b[32m2021-04-28T10:14:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0725, train/total_loss: 0.0002, train/total_loss/avg: 0.0725, max mem: 9226.0, experiment: run, epoch: 44, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 785ms, time_since_start: 03h 35m 39s 217ms, eta: 02h 27m 39s 138ms\n",
      "\u001b[32m2021-04-28T10:16:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0719, train/total_loss: 0.0002, train/total_loss/avg: 0.0719, max mem: 9226.0, experiment: run, epoch: 44, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 259ms, time_since_start: 03h 37m 08s 477ms, eta: 02h 22m 04s 657ms\n",
      "\u001b[32m2021-04-28T10:17:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0714, train/total_loss: 0.0002, train/total_loss/avg: 0.0714, max mem: 9226.0, experiment: run, epoch: 44, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 059ms, time_since_start: 03h 38m 39s 537ms, eta: 02h 23m 24s 074ms\n",
      "\u001b[32m2021-04-28T10:19:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0708, train/total_loss: 0.0002, train/total_loss/avg: 0.0708, max mem: 9226.0, experiment: run, epoch: 45, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 255ms, time_since_start: 03h 40m 10s 792ms, eta: 02h 22m 09s 805ms\n",
      "\u001b[32m2021-04-28T10:20:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0703, train/total_loss: 0.0002, train/total_loss/avg: 0.0703, max mem: 9226.0, experiment: run, epoch: 45, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 555ms, time_since_start: 03h 41m 41s 348ms, eta: 02h 19m 32s 381ms\n",
      "\u001b[32m2021-04-28T10:22:20 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T10:22:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T10:22:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T10:22:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T10:22:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0697, train/total_loss: 0.0002, train/total_loss/avg: 0.0697, max mem: 9226.0, experiment: run, epoch: 45, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 0.89, time: 01m 52s 703ms, time_since_start: 03h 43m 34s 051ms, eta: 02h 51m 45s 571ms\n",
      "\u001b[32m2021-04-28T10:22:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T10:22:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:22:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:22:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:23:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.5199, val/total_loss: 2.5199, val/hateful_memes/accuracy: 0.6380, val/hateful_memes/binary_f1: 0.5486, val/hateful_memes/roc_auc: 0.6818, num_updates: 13000, epoch: 45, iterations: 13000, max_updates: 22000, val_time: 21s 595ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:23:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:23:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:24:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0692, train/total_loss: 0.0003, train/total_loss/avg: 0.0692, max mem: 9226.0, experiment: run, epoch: 46, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 1.05, time: 01m 35s 458ms, time_since_start: 03h 45m 31s 110ms, eta: 02h 23m 51s 771ms\n",
      "\u001b[32m2021-04-28T10:26:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0690, train/total_loss: 0.0004, train/total_loss/avg: 0.0690, max mem: 9226.0, experiment: run, epoch: 46, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 430ms, time_since_start: 03h 47m 01s 540ms, eta: 02h 14m 45s 213ms\n",
      "\u001b[32m2021-04-28T10:27:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0685, train/total_loss: 0.0004, train/total_loss/avg: 0.0685, max mem: 9226.0, experiment: run, epoch: 47, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 1.06, time: 01m 34s 775ms, time_since_start: 03h 48m 36s 316ms, eta: 02h 19m 37s 404ms\n",
      "\u001b[32m2021-04-28T10:29:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0680, train/total_loss: 0.0003, train/total_loss/avg: 0.0680, max mem: 9226.0, experiment: run, epoch: 47, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 870ms, time_since_start: 03h 50m 05s 186ms, eta: 02h 09m 25s 126ms\n",
      "\u001b[32m2021-04-28T10:30:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0675, train/total_loss: 0.0003, train/total_loss/avg: 0.0675, max mem: 9226.0, experiment: run, epoch: 47, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 721ms, time_since_start: 03h 51m 35s 908ms, eta: 02h 10m 34s 752ms\n",
      "\u001b[32m2021-04-28T10:32:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0670, train/total_loss: 0.0002, train/total_loss/avg: 0.0670, max mem: 9226.0, experiment: run, epoch: 48, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 1.08, time: 01m 33s 978ms, time_since_start: 03h 53m 09s 887ms, eta: 02h 13m 40s 500ms\n",
      "\u001b[32m2021-04-28T10:33:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0666, train/total_loss: 0.0003, train/total_loss/avg: 0.0666, max mem: 9226.0, experiment: run, epoch: 48, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 216ms, time_since_start: 03h 54m 40s 104ms, eta: 02h 06m 47s 819ms\n",
      "\u001b[32m2021-04-28T10:35:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0661, train/total_loss: 0.0003, train/total_loss/avg: 0.0661, max mem: 9226.0, experiment: run, epoch: 48, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 759ms, time_since_start: 03h 56m 09s 863ms, eta: 02h 04m 38s 006ms\n",
      "\u001b[32m2021-04-28T10:36:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0656, train/total_loss: 0.0003, train/total_loss/avg: 0.0656, max mem: 9226.0, experiment: run, epoch: 49, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 745ms, time_since_start: 03h 57m 42s 609ms, eta: 02h 07m 12s 602ms\n",
      "\u001b[32m2021-04-28T10:38:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T10:38:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T10:38:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T10:38:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T10:38:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0652, train/total_loss: 0.0003, train/total_loss/avg: 0.0652, max mem: 9226.0, experiment: run, epoch: 49, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 0.88, time: 01m 54s 863ms, time_since_start: 03h 59m 37s 472ms, eta: 02h 35m 36s 122ms\n",
      "\u001b[32m2021-04-28T10:38:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T10:38:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:38:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:38:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:39:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.5112, val/total_loss: 2.5112, val/hateful_memes/accuracy: 0.6440, val/hateful_memes/binary_f1: 0.5412, val/hateful_memes/roc_auc: 0.7019, num_updates: 14000, epoch: 49, iterations: 14000, max_updates: 22000, val_time: 40s 256ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T10:40:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0647, train/total_loss: 0.0003, train/total_loss/avg: 0.0647, max mem: 9226.0, experiment: run, epoch: 49, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 379ms, time_since_start: 04h 01m 46s 116ms, eta: 01h 58m 13s 662ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:41:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:41:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:42:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0643, train/total_loss: 0.0003, train/total_loss/avg: 0.0643, max mem: 9226.0, experiment: run, epoch: 50, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 1.08, time: 01m 33s 522ms, time_since_start: 04h 03m 19s 639ms, eta: 02h 03m 31s 499ms\n",
      "\u001b[32m2021-04-28T10:44:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0638, train/total_loss: 0.0003, train/total_loss/avg: 0.0638, max mem: 9226.0, experiment: run, epoch: 50, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 392ms, time_since_start: 04h 04m 51s 031ms, eta: 01h 59m 09s 795ms\n",
      "\u001b[32m2021-04-28T10:45:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0643, train/total_loss: 0.0003, train/total_loss/avg: 0.0643, max mem: 9226.0, experiment: run, epoch: 50, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 082ms, time_since_start: 04h 06m 23s 114ms, eta: 01h 58m 30s 281ms\n",
      "\u001b[32m2021-04-28T10:47:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0639, train/total_loss: 0.0003, train/total_loss/avg: 0.0639, max mem: 9226.0, experiment: run, epoch: 51, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 1.08, time: 01m 33s 831ms, time_since_start: 04h 07m 56s 946ms, eta: 01h 59m 09s 957ms\n",
      "\u001b[32m2021-04-28T10:48:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0634, train/total_loss: 0.0003, train/total_loss/avg: 0.0634, max mem: 9226.0, experiment: run, epoch: 51, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 877ms, time_since_start: 04h 09m 28s 823ms, eta: 01h 55m 07s 704ms\n",
      "\u001b[32m2021-04-28T10:50:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0630, train/total_loss: 0.0003, train/total_loss/avg: 0.0630, max mem: 9226.0, experiment: run, epoch: 51, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 303ms, time_since_start: 04h 11m 127ms, eta: 01h 52m 51s 828ms\n",
      "\u001b[32m2021-04-28T10:51:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0626, train/total_loss: 0.0003, train/total_loss/avg: 0.0626, max mem: 9226.0, experiment: run, epoch: 52, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 1.08, time: 01m 33s 372ms, time_since_start: 04h 12m 33s 499ms, eta: 01h 53m 50s 355ms\n",
      "\u001b[32m2021-04-28T10:53:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0622, train/total_loss: 0.0003, train/total_loss/avg: 0.0622, max mem: 9226.0, experiment: run, epoch: 52, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 963ms, time_since_start: 04h 14m 04s 463ms, eta: 01h 49m 21s 773ms\n",
      "\u001b[32m2021-04-28T10:54:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T10:54:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T10:54:57 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T10:55:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T10:55:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0618, train/total_loss: 0.0003, train/total_loss/avg: 0.0618, max mem: 9226.0, experiment: run, epoch: 52, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 0.86, time: 01m 56s 563ms, time_since_start: 04h 16m 01s 027ms, eta: 02h 18m 09s 986ms\n",
      "\u001b[32m2021-04-28T10:55:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T10:55:10 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:55:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:55:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:55:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 2.8398, val/total_loss: 2.8398, val/hateful_memes/accuracy: 0.6340, val/hateful_memes/binary_f1: 0.5146, val/hateful_memes/roc_auc: 0.7118, num_updates: 15000, epoch: 52, iterations: 15000, max_updates: 22000, val_time: 21s 589ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:55:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:55:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:57:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0614, train/total_loss: 0.0002, train/total_loss/avg: 0.0614, max mem: 9226.0, experiment: run, epoch: 53, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 1.08, time: 01m 33s 304ms, time_since_start: 04h 17m 55s 924ms, eta: 01h 49m 01s 047ms\n",
      "\u001b[32m2021-04-28T10:58:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0609, train/total_loss: 0.0002, train/total_loss/avg: 0.0609, max mem: 9226.0, experiment: run, epoch: 53, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 872ms, time_since_start: 04h 19m 28s 796ms, eta: 01h 46m 56s 359ms\n",
      "\u001b[32m2021-04-28T11:00:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0605, train/total_loss: 0.0001, train/total_loss/avg: 0.0605, max mem: 9226.0, experiment: run, epoch: 53, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 341ms, time_since_start: 04h 21m 01s 138ms, eta: 01h 44m 45s 881ms\n",
      "\u001b[32m2021-04-28T11:01:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0602, train/total_loss: 0.0001, train/total_loss/avg: 0.0602, max mem: 9226.0, experiment: run, epoch: 54, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 1.06, time: 01m 34s 050ms, time_since_start: 04h 22m 35s 188ms, eta: 01h 45m 06s 638ms\n",
      "\u001b[32m2021-04-28T11:03:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0598, train/total_loss: 0.0001, train/total_loss/avg: 0.0598, max mem: 9226.0, experiment: run, epoch: 54, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 568ms, time_since_start: 04h 24m 06s 756ms, eta: 01h 40m 47s 177ms\n",
      "\u001b[32m2021-04-28T11:04:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0594, train/total_loss: 0.0001, train/total_loss/avg: 0.0594, max mem: 9226.0, experiment: run, epoch: 54, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 545ms, time_since_start: 04h 25m 38s 302ms, eta: 01h 39m 12s 637ms\n",
      "\u001b[32m2021-04-28T11:06:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0590, train/total_loss: 0.0000, train/total_loss/avg: 0.0590, max mem: 9226.0, experiment: run, epoch: 55, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 1.08, time: 01m 33s 391ms, time_since_start: 04h 27m 11s 693ms, eta: 01h 39m 37s 810ms\n",
      "\u001b[32m2021-04-28T11:07:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0586, train/total_loss: 0.0000, train/total_loss/avg: 0.0586, max mem: 9226.0, experiment: run, epoch: 55, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 375ms, time_since_start: 04h 28m 43s 069ms, eta: 01h 35m 55s 956ms\n",
      "\u001b[32m2021-04-28T11:09:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0583, train/total_loss: 0.0000, train/total_loss/avg: 0.0583, max mem: 9226.0, experiment: run, epoch: 56, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 1.06, time: 01m 34s 629ms, time_since_start: 04h 30m 17s 699ms, eta: 01h 37m 44s 784ms\n",
      "\u001b[32m2021-04-28T11:10:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T11:10:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T11:11:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T11:11:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T11:11:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0579, train/total_loss: 0.0000, train/total_loss/avg: 0.0579, max mem: 9226.0, experiment: run, epoch: 56, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 0.89, time: 01m 52s 162ms, time_since_start: 04h 32m 09s 862ms, eta: 01h 53m 57s 414ms\n",
      "\u001b[32m2021-04-28T11:11:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T11:11:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:11:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:11:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:11:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.7467, val/total_loss: 2.7467, val/hateful_memes/accuracy: 0.6480, val/hateful_memes/binary_f1: 0.5600, val/hateful_memes/roc_auc: 0.7124, num_updates: 16000, epoch: 56, iterations: 16000, max_updates: 22000, val_time: 21s 831ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T11:13:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0576, train/total_loss: 0.0000, train/total_loss/avg: 0.0576, max mem: 9226.0, experiment: run, epoch: 56, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 715ms, time_since_start: 04h 34m 01s 415ms, eta: 01h 29m 37s 931ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:14:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:14:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:14:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0572, train/total_loss: 0.0000, train/total_loss/avg: 0.0572, max mem: 9226.0, experiment: run, epoch: 57, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 1.09, time: 01m 32s 748ms, time_since_start: 04h 35m 34s 164ms, eta: 01h 31m 05s 513ms\n",
      "\u001b[32m2021-04-28T11:16:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0569, train/total_loss: 0.0000, train/total_loss/avg: 0.0569, max mem: 9226.0, experiment: run, epoch: 57, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 096ms, time_since_start: 04h 37m 02s 261ms, eta: 01h 25m 01s 857ms\n",
      "\u001b[32m2021-04-28T11:17:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0565, train/total_loss: 0.0000, train/total_loss/avg: 0.0565, max mem: 9226.0, experiment: run, epoch: 57, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 019ms, time_since_start: 04h 38m 32s 281ms, eta: 01h 25m 21s 766ms\n",
      "\u001b[32m2021-04-28T11:19:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0562, train/total_loss: 0.0000, train/total_loss/avg: 0.0562, max mem: 9226.0, experiment: run, epoch: 58, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 1.09, time: 01m 32s 578ms, time_since_start: 04h 40m 04s 860ms, eta: 01h 26m 13s 299ms\n",
      "\u001b[32m2021-04-28T11:20:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0558, train/total_loss: 0.0000, train/total_loss/avg: 0.0558, max mem: 9226.0, experiment: run, epoch: 58, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 509ms, time_since_start: 04h 41m 33s 369ms, eta: 01h 20m 55s 962ms\n",
      "\u001b[32m2021-04-28T11:22:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0555, train/total_loss: 0.0000, train/total_loss/avg: 0.0555, max mem: 9226.0, experiment: run, epoch: 58, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 086ms, time_since_start: 04h 43m 03s 456ms, eta: 01h 20m 51s 002ms\n",
      "\u001b[32m2021-04-28T11:23:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0552, train/total_loss: 0.0000, train/total_loss/avg: 0.0552, max mem: 9226.0, experiment: run, epoch: 59, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 1.09, time: 01m 32s 171ms, time_since_start: 04h 44m 35s 627ms, eta: 01h 21m 09s 611ms\n",
      "\u001b[32m2021-04-28T11:25:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0549, train/total_loss: 0.0000, train/total_loss/avg: 0.0549, max mem: 9226.0, experiment: run, epoch: 59, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 399ms, time_since_start: 04h 46m 05s 027ms, eta: 01h 17m 12s 326ms\n",
      "\u001b[32m2021-04-28T11:26:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T11:26:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T11:26:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T11:27:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T11:27:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0545, train/total_loss: 0.0000, train/total_loss/avg: 0.0545, max mem: 9226.0, experiment: run, epoch: 59, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 106ms, time_since_start: 04h 47m 59s 134ms, eta: 01h 36m 36s 614ms\n",
      "\u001b[32m2021-04-28T11:27:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T11:27:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:27:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:27:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:27:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 3.2180, val/total_loss: 3.2180, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5280, val/hateful_memes/roc_auc: 0.7047, num_updates: 17000, epoch: 59, iterations: 17000, max_updates: 22000, val_time: 21s 516ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:28:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:28:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:29:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0542, train/total_loss: 0.0000, train/total_loss/avg: 0.0542, max mem: 9226.0, experiment: run, epoch: 60, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 008ms, time_since_start: 04h 49m 51s 666ms, eta: 01h 15m 30s 770ms\n",
      "\u001b[32m2021-04-28T11:30:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0539, train/total_loss: 0.0000, train/total_loss/avg: 0.0539, max mem: 9226.0, experiment: run, epoch: 60, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 356ms, time_since_start: 04h 51m 21s 022ms, eta: 01h 12m 37s 719ms\n",
      "\u001b[32m2021-04-28T11:31:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0536, train/total_loss: 0.0000, train/total_loss/avg: 0.0536, max mem: 9226.0, experiment: run, epoch: 60, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 085ms, time_since_start: 04h 52m 50s 108ms, eta: 01h 10m 54s 030ms\n",
      "\u001b[32m2021-04-28T11:33:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0533, train/total_loss: 0.0000, train/total_loss/avg: 0.0533, max mem: 9226.0, experiment: run, epoch: 61, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 1.09, time: 01m 32s 021ms, time_since_start: 04h 54m 22s 129ms, eta: 01h 11m 40s 709ms\n",
      "\u001b[32m2021-04-28T11:35:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0530, train/total_loss: 0.0000, train/total_loss/avg: 0.0530, max mem: 9226.0, experiment: run, epoch: 61, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 526ms, time_since_start: 04h 55m 51s 656ms, eta: 01h 08m 13s 145ms\n",
      "\u001b[32m2021-04-28T11:36:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0527, train/total_loss: 0.0000, train/total_loss/avg: 0.0527, max mem: 9226.0, experiment: run, epoch: 61, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 441ms, time_since_start: 04h 57m 21s 097ms, eta: 01h 06m 38s 397ms\n",
      "\u001b[32m2021-04-28T11:38:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0524, train/total_loss: 0.0000, train/total_loss/avg: 0.0524, max mem: 9226.0, experiment: run, epoch: 62, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 606ms, time_since_start: 04h 58m 52s 704ms, eta: 01h 06m 42s 121ms\n",
      "\u001b[32m2021-04-28T11:39:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0521, train/total_loss: 0.0000, train/total_loss/avg: 0.0521, max mem: 9226.0, experiment: run, epoch: 62, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 651ms, time_since_start: 05h 22s 356ms, eta: 01h 03m 45s 617ms\n",
      "\u001b[32m2021-04-28T11:41:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0518, train/total_loss: 0.0000, train/total_loss/avg: 0.0518, max mem: 9226.0, experiment: run, epoch: 62, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 347ms, time_since_start: 05h 01m 52s 703ms, eta: 01h 02m 43s 504ms\n",
      "\u001b[32m2021-04-28T11:42:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T11:42:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T11:42:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T11:42:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T11:42:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0515, train/total_loss: 0.0000, train/total_loss/avg: 0.0515, max mem: 9226.0, experiment: run, epoch: 63, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 0.86, time: 01m 56s 793ms, time_since_start: 05h 03m 49s 497ms, eta: 01h 19m 06s 488ms\n",
      "\u001b[32m2021-04-28T11:42:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T11:42:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:42:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:42:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:43:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 3.4350, val/total_loss: 3.4350, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5330, val/hateful_memes/roc_auc: 0.7053, num_updates: 18000, epoch: 63, iterations: 18000, max_updates: 22000, val_time: 21s 592ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T11:44:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0512, train/total_loss: 0.0000, train/total_loss/avg: 0.0512, max mem: 9226.0, experiment: run, epoch: 63, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 252ms, time_since_start: 05h 05m 41s 347ms, eta: 59m 36s 147ms\n",
      "\u001b[32m2021-04-28T11:46:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0509, train/total_loss: 0.0000, train/total_loss/avg: 0.0509, max mem: 9226.0, experiment: run, epoch: 63, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 341ms, time_since_start: 05h 07m 12s 688ms, eta: 58m 46s 518ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:46:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:46:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:47:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0507, train/total_loss: 0.0000, train/total_loss/avg: 0.0507, max mem: 9226.0, experiment: run, epoch: 64, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 002ms, time_since_start: 05h 08m 46s 691ms, eta: 58m 53s 755ms\n",
      "\u001b[32m2021-04-28T11:49:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0504, train/total_loss: 0.0000, train/total_loss/avg: 0.0504, max mem: 9226.0, experiment: run, epoch: 64, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 030ms, time_since_start: 05h 10m 17s 722ms, eta: 55m 29s 542ms\n",
      "\u001b[32m2021-04-28T11:51:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0501, train/total_loss: 0.0000, train/total_loss/avg: 0.0501, max mem: 9226.0, experiment: run, epoch: 65, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 412ms, time_since_start: 05h 11m 51s 135ms, eta: 55m 21s 763ms\n",
      "\u001b[32m2021-04-28T11:52:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0498, train/total_loss: 0.0000, train/total_loss/avg: 0.0498, max mem: 9226.0, experiment: run, epoch: 65, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 445ms, time_since_start: 05h 13m 19s 580ms, eta: 50m 55s 246ms\n",
      "\u001b[32m2021-04-28T11:53:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0496, train/total_loss: 0.0000, train/total_loss/avg: 0.0496, max mem: 9226.0, experiment: run, epoch: 65, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 898ms, time_since_start: 05h 14m 50s 479ms, eta: 50m 47s 662ms\n",
      "\u001b[32m2021-04-28T11:55:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0493, train/total_loss: 0.0000, train/total_loss/avg: 0.0493, max mem: 9226.0, experiment: run, epoch: 66, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 246ms, time_since_start: 05h 16m 23s 726ms, eta: 50m 31s 633ms\n",
      "\u001b[32m2021-04-28T11:57:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0491, train/total_loss: 0.0000, train/total_loss/avg: 0.0491, max mem: 9226.0, experiment: run, epoch: 66, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 372ms, time_since_start: 05h 17m 53s 098ms, eta: 46m 54s 866ms\n",
      "\u001b[32m2021-04-28T11:58:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T11:58:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T11:58:41 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T11:58:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T11:58:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0488, train/total_loss: 0.0000, train/total_loss/avg: 0.0488, max mem: 9226.0, experiment: run, epoch: 66, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 0.90, time: 01m 51s 543ms, time_since_start: 05h 19m 44s 642ms, eta: 56m 39s 857ms\n",
      "\u001b[32m2021-04-28T11:58:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T11:58:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:58:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:58:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:59:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.7300, val/total_loss: 3.7300, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5203, val/hateful_memes/roc_auc: 0.7040, num_updates: 19000, epoch: 66, iterations: 19000, max_updates: 22000, val_time: 21s 575ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:00:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:00:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:00:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0485, train/total_loss: 0.0000, train/total_loss/avg: 0.0485, max mem: 9226.0, experiment: run, epoch: 67, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 809ms, time_since_start: 05h 21m 37s 030ms, eta: 44m 35s 620ms\n",
      "\u001b[32m2021-04-28T12:02:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0483, train/total_loss: 0.0000, train/total_loss/avg: 0.0483, max mem: 9226.0, experiment: run, epoch: 67, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 243ms, time_since_start: 05h 23m 07s 273ms, eta: 42m 47s 248ms\n",
      "\u001b[32m2021-04-28T12:03:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0480, train/total_loss: 0.0000, train/total_loss/avg: 0.0480, max mem: 9226.0, experiment: run, epoch: 67, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 375ms, time_since_start: 05h 24m 38s 649ms, eta: 41m 46s 626ms\n",
      "\u001b[32m2021-04-28T12:05:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0478, train/total_loss: 0.0000, train/total_loss/avg: 0.0478, max mem: 9226.0, experiment: run, epoch: 68, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 050ms, time_since_start: 05h 26m 11s 700ms, eta: 40m 58s 027ms\n",
      "\u001b[32m2021-04-28T12:06:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0475, train/total_loss: 0.0000, train/total_loss/avg: 0.0475, max mem: 9226.0, experiment: run, epoch: 68, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 387ms, time_since_start: 05h 27m 42s 087ms, eta: 38m 15s 834ms\n",
      "\u001b[32m2021-04-28T12:08:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0473, train/total_loss: 0.0000, train/total_loss/avg: 0.0473, max mem: 9226.0, experiment: run, epoch: 68, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 190ms, time_since_start: 05h 29m 12s 278ms, eta: 36m 39s 203ms\n",
      "\u001b[32m2021-04-28T12:09:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0471, train/total_loss: 0.0000, train/total_loss/avg: 0.0471, max mem: 9226.0, experiment: run, epoch: 69, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 1.09, time: 01m 32s 987ms, time_since_start: 05h 30m 45s 265ms, eta: 36m 12s 930ms\n",
      "\u001b[32m2021-04-28T12:11:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0468, train/total_loss: 0.0000, train/total_loss/avg: 0.0468, max mem: 9226.0, experiment: run, epoch: 69, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 112ms, time_since_start: 05h 32m 16s 378ms, eta: 33m 56s 541ms\n",
      "\u001b[32m2021-04-28T12:12:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0466, train/total_loss: 0.0000, train/total_loss/avg: 0.0466, max mem: 9226.0, experiment: run, epoch: 69, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 913ms, time_since_start: 05h 33m 47s 291ms, eta: 32m 19s 730ms\n",
      "\u001b[32m2021-04-28T12:14:28 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T12:14:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T12:14:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T12:14:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T12:14:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0464, train/total_loss: 0.0000, train/total_loss/avg: 0.0464, max mem: 9226.0, experiment: run, epoch: 70, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 569ms, time_since_start: 05h 35m 42s 861ms, eta: 39m 08s 379ms\n",
      "\u001b[32m2021-04-28T12:14:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T12:14:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:14:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:14:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:15:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 3.8089, val/total_loss: 3.8089, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.5383, val/hateful_memes/roc_auc: 0.7051, num_updates: 20000, epoch: 70, iterations: 20000, max_updates: 22000, val_time: 21s 092ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T12:16:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0461, train/total_loss: 0.0000, train/total_loss/avg: 0.0461, max mem: 9226.0, experiment: run, epoch: 70, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 676ms, time_since_start: 05h 37m 33s 632ms, eta: 28m 51s 116ms\n",
      "\u001b[32m2021-04-28T12:18:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0459, train/total_loss: 0.0000, train/total_loss/avg: 0.0459, max mem: 9226.0, experiment: run, epoch: 70, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.11, time: 01m 30s 720ms, time_since_start: 05h 39m 04s 353ms, eta: 27m 39s 105ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:18:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:18:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:19:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0457, train/total_loss: 0.0000, train/total_loss/avg: 0.0457, max mem: 9226.0, experiment: run, epoch: 71, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.09, time: 01m 32s 503ms, time_since_start: 05h 40m 36s 857ms, eta: 26m 37s 713ms\n",
      "\u001b[32m2021-04-28T12:21:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0454, train/total_loss: 0.0000, train/total_loss/avg: 0.0454, max mem: 9226.0, experiment: run, epoch: 71, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.11, time: 01m 30s 000ms, time_since_start: 05h 42m 06s 857ms, eta: 24m 23s 044ms\n",
      "\u001b[32m2021-04-28T12:22:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0452, train/total_loss: 0.0000, train/total_loss/avg: 0.0452, max mem: 9226.0, experiment: run, epoch: 71, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.11, time: 01m 30s 589ms, time_since_start: 05h 43m 37s 447ms, eta: 23m 590ms\n",
      "\u001b[32m2021-04-28T12:24:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0450, train/total_loss: 0.0000, train/total_loss/avg: 0.0450, max mem: 9226.0, experiment: run, epoch: 72, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 047ms, time_since_start: 05h 45m 10s 494ms, eta: 22m 03s 503ms\n",
      "\u001b[32m2021-04-28T12:25:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0448, train/total_loss: 0.0000, train/total_loss/avg: 0.0448, max mem: 9226.0, experiment: run, epoch: 72, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.11, time: 01m 30s 773ms, time_since_start: 05h 46m 41s 267ms, eta: 19m 58s 933ms\n",
      "\u001b[32m2021-04-28T12:27:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0446, train/total_loss: 0.0000, train/total_loss/avg: 0.0446, max mem: 9226.0, experiment: run, epoch: 72, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.10, time: 01m 31s 494ms, time_since_start: 05h 48m 12s 762ms, eta: 18m 35s 497ms\n",
      "\u001b[32m2021-04-28T12:28:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0444, train/total_loss: 0.0000, train/total_loss/avg: 0.0444, max mem: 9226.0, experiment: run, epoch: 73, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.10, time: 01m 31s 599ms, time_since_start: 05h 49m 44s 361ms, eta: 17m 03s 714ms\n",
      "\u001b[32m2021-04-28T12:30:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T12:30:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T12:30:35 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T12:30:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T12:30:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0441, train/total_loss: 0.0000, train/total_loss/avg: 0.0441, max mem: 9226.0, experiment: run, epoch: 73, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 54s 993ms, time_since_start: 05h 51m 39s 355ms, eta: 19m 28s 334ms\n",
      "\u001b[32m2021-04-28T12:30:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T12:30:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:30:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:30:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:31:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 3.7693, val/total_loss: 3.7693, val/hateful_memes/accuracy: 0.6340, val/hateful_memes/binary_f1: 0.5271, val/hateful_memes/roc_auc: 0.7013, num_updates: 21000, epoch: 73, iterations: 21000, max_updates: 22000, val_time: 21s 115ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:32:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:32:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:32:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0439, train/total_loss: 0.0000, train/total_loss/avg: 0.0439, max mem: 9226.0, experiment: run, epoch: 74, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 090ms, time_since_start: 05h 53m 33s 563ms, eta: 14m 11s 215ms\n",
      "\u001b[32m2021-04-28T12:34:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0437, train/total_loss: 0.0000, train/total_loss/avg: 0.0437, max mem: 9226.0, experiment: run, epoch: 74, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 366ms, time_since_start: 05h 55m 02s 930ms, eta: 12m 06s 374ms\n",
      "\u001b[32m2021-04-28T12:35:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0435, train/total_loss: 0.0000, train/total_loss/avg: 0.0435, max mem: 9226.0, experiment: run, epoch: 74, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.11, time: 01m 30s 946ms, time_since_start: 05h 56m 33s 876ms, eta: 10m 46s 811ms\n",
      "\u001b[32m2021-04-28T12:37:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0433, train/total_loss: 0.0000, train/total_loss/avg: 0.0433, max mem: 9226.0, experiment: run, epoch: 75, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.09, time: 01m 32s 584ms, time_since_start: 05h 58m 06s 461ms, eta: 09m 24s 398ms\n",
      "\u001b[32m2021-04-28T12:38:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0431, train/total_loss: 0.0000, train/total_loss/avg: 0.0431, max mem: 9226.0, experiment: run, epoch: 75, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 200ms, time_since_start: 05h 59m 34s 662ms, eta: 07m 28s 056ms\n",
      "\u001b[32m2021-04-28T12:40:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0429, train/total_loss: 0.0000, train/total_loss/avg: 0.0429, max mem: 9226.0, experiment: run, epoch: 75, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 942ms, time_since_start: 06h 01m 04s 604ms, eta: 06m 05s 527ms\n",
      "\u001b[32m2021-04-28T12:41:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0427, train/total_loss: 0.0000, train/total_loss/avg: 0.0427, max mem: 9226.0, experiment: run, epoch: 76, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.09, time: 01m 32s 184ms, time_since_start: 06h 02m 36s 789ms, eta: 04m 40s 978ms\n",
      "\u001b[32m2021-04-28T12:43:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0425, train/total_loss: 0.0000, train/total_loss/avg: 0.0425, max mem: 9226.0, experiment: run, epoch: 76, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 794ms, time_since_start: 06h 04m 05s 584ms, eta: 03m 430ms\n",
      "\u001b[32m2021-04-28T12:44:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0423, train/total_loss: 0.0000, train/total_loss/avg: 0.0423, max mem: 9226.0, experiment: run, epoch: 76, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 544ms, time_since_start: 06h 05m 34s 129ms, eta: 01m 29s 961ms\n",
      "\u001b[32m2021-04-28T12:46:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T12:46:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T12:46:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T12:46:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T12:46:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0421, train/total_loss: 0.0000, train/total_loss/avg: 0.0421, max mem: 9226.0, experiment: run, epoch: 77, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 0.86, time: 01m 56s 651ms, time_since_start: 06h 07m 30s 780ms, eta: 0ms\n",
      "\u001b[32m2021-04-28T12:46:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T12:46:39 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:46:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:46:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:47:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 3.7505, val/total_loss: 3.7505, val/hateful_memes/accuracy: 0.6380, val/hateful_memes/binary_f1: 0.5274, val/hateful_memes/roc_auc: 0.7015, num_updates: 22000, epoch: 77, iterations: 22000, max_updates: 22000, val_time: 22s 689ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.735318\n",
      "\u001b[32m2021-04-28T12:47:03 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2021-04-28T12:47:03 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2021-04-28T12:47:03 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2021-04-28T12:47:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-04-28T12:47:16 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 2000\n",
      "\u001b[32m2021-04-28T12:47:16 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 2000\n",
      "\u001b[32m2021-04-28T12:47:16 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 7\n",
      "\u001b[32m2021-04-28T12:47:18 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-04-28T12:47:18 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:47:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:47:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "100% 16/16 [00:17<00:00,  1.11s/it]\n",
      "\u001b[32m2021-04-28T12:47:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.1368, val/total_loss: 1.1368, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5714, val/hateful_memes/roc_auc: 0.7353\n",
      "\u001b[32m2021-04-28T12:47:36 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 06h 08m 27s 576ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/from_coco.yaml \\\n",
    "  model=visual_bert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=train_val \\\n",
    "  training.batch_size=32 \\\n",
    "  env.save_dir=/content/gdrive/MyDrive/colab/pretrained_visualbertcoco_election_memes/ \\\n",
    "  checkpoint.resume_zoo=visual_bert.pretrained.coco \\\n",
    "  checkpoint.resume_pretrained=True \\\n",
    "  dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_hateful_and_election.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 23256263,
     "status": "ok",
     "timestamp": 1619614060707,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "tk6Qo9Z9rp54"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN/G0w5Wwbu5jNIpQ1pbSBg",
   "collapsed_sections": [],
   "name": "PretrainedVisualBert COCO (Hateful and Election Memes).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
