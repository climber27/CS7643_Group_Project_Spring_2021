{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22446,
     "status": "ok",
     "timestamp": 1620078748100,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "jSPOfKQom8AF",
    "outputId": "7460ae68-13a6-4f94-d89f-88c576677724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1620078748866,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "SlKoJSAknjuq",
    "outputId": "90488d9e-37c1-47f2-84d9-ad18c988c14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/colab\n"
     ]
    }
   ],
   "source": [
    "%cd gdrive/MyDrive/colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1077,
     "status": "ok",
     "timestamp": 1620078749187,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "o-zreWCXnpGw",
    "outputId": "e6724e65-33b1-40b4-9aaa-5a3999e669e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf\n"
     ]
    }
   ],
   "source": [
    "%cd mmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 237062,
     "status": "ok",
     "timestamp": 1620078985176,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "6hM3BET1nrBO",
    "outputId": "429cc8ce-11a3-4d70-9d6e-1cee05ddfedf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting torchvision<=0.9.1,>=0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/8a/82062a33b5eb7f696bf23f8ccf04bf6fc81d1a4972740fb21c2569ada0a6/torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4MB)\n",
      "\u001b[K     |████████████████████████████████| 17.4MB 304kB/s \n",
      "\u001b[?25hCollecting ftfy==5.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.2MB/s \n",
      "\u001b[?25hCollecting torchtext==0.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
      "\u001b[?25hCollecting lmdb==0.98\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/5c/d56dbc2532ecf14fa004c543927500c0f645eaca8bd7ec39420c7546396a/lmdb-0.98.tar.gz (869kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 54.2MB/s \n",
      "\u001b[?25hCollecting transformers==3.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 47.0MB/s \n",
      "\u001b[?25hCollecting matplotlib==3.3.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/3d/db9a6b3c83c9511301152dbb64a029c3a4313c86eaef12c237b13ecf91d6/matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6MB 51.7MB/s \n",
      "\u001b[?25hCollecting omegaconf==2.0.6\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.0.2)\n",
      "Collecting datasets==1.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 58.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n",
      "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n",
      "Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.19.5)\n",
      "Collecting nltk==3.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 47.2MB/s \n",
      "\u001b[?25hCollecting tqdm<4.50.0,>=4.43.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.0)\n",
      "Collecting pytorch-lightning==1.2.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/13/fb401b8f9d9c5e2aa08769d230bb401bf11dee0bc93e069d7337a4201ec8/pytorch_lightning-1.2.7-py3-none-any.whl (830kB)\n",
      "\u001b[K     |████████████████████████████████| 839kB 48.8MB/s \n",
      "\u001b[?25hCollecting fasttext==0.9.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n",
      "\u001b[?25hCollecting iopath==0.1.7\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/d5/1c70fea7632640e8a9fb5a176676e555238119b3e7ee8b6dc49980ec5769/iopath-0.1.7-py3-none-any.whl\n",
      "Collecting torch<=1.8.1,>=1.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n",
      "\u001b[K     |████████████████████████████████| 804.1MB 19kB/s \n",
      "\u001b[?25hCollecting GitPython==3.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 51.2MB/s \n",
      "\u001b[?25hCollecting demjson==2.2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/67/6db789e2533158963d4af689f961b644ddd9200615b8ce92d6cad695c65a/demjson-2.2.4.tar.gz (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 58.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<=0.9.1,>=0.7.0->mmf==1.0.0rc12) (7.1.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->mmf==1.0.0rc12) (1.15.0)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 41.0MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 46.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (2019.12.20)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.12.4)\n",
      "Collecting tokenizers==0.9.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/e7/edf655ae34925aeaefb7b7fcc3dd0887d2a1203ee6b0df4d1170d1a19d4f/tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 45.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.0.12)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (20.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.4.7)\n",
      "Collecting PyYAML>=5.1.*\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 35.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.0.6->mmf==1.0.0rc12) (3.7.4.3)\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (56.0.0)\n",
      "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (0.29.22)\n",
      "Collecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 54.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (1.1.5)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.0.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.3.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.10.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.70.11.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2020.12.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (0.22.2.post1)\n",
      "Collecting torchmetrics>=0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/99/dc59248df9a50349d537ffb3403c1bdc1fa69077109d46feaa0843488001/torchmetrics-0.3.1-py3-none-any.whl (271kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 57.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.7->mmf==1.0.0rc12) (2.4.1)\n",
      "Collecting fsspec[http]>=0.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 58.3MB/s \n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 43.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (2.6.2)\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (1.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1->mmf==1.0.0rc12) (2018.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.2.1->mmf==1.0.0rc12) (3.4.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (3.3.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.12.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.28.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.4.4)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.8.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.32.0)\n",
      "Collecting aiohttp; extra == \"http\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 46.0MB/s \n",
      "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (20.3.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 59.4MB/s \n",
      "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Collecting multidict<7.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 60.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (3.1.0)\n",
      "Building wheels for collected packages: ftfy, lmdb, nltk, fasttext, demjson, future\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-5.8-cp37-none-any.whl size=45613 sha256=5220e92accbbd4625c5488b0fb54af29cca14092ce76113fbc16c7b245f93e91\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
      "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=219684 sha256=a0c172acd17d65dbe4bd07b419a5b433ac5026567489c6b904d0e55221b6cf5f\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/97/8c/7721e4b6b0ac723c6cc45ecca60599a80f75e2367330647390\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449906 sha256=638294bd54afebaee72dcfe7185167c2f5b87cb553a37ede3bbfa4b5e75724b6\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2463143 sha256=1641ed534e3a2f363fbf99911bd7d8019de4277f4028a66c2451bf8cd7d8b2ce\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
      "  Building wheel for demjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for demjson: filename=demjson-2.2.4-cp37-none-any.whl size=73546 sha256=0028eaff2b035cb19308701c9abbb967ef7fee79c8e3722ffb681c97c0458349\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/d2/ab/a54fb5ea53ac3badba098160e8452fa126a51febda80440ded\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=2d9510a0666b66b86c03f36548655f03f9a25b513ababf938700d2c9acbf5a10\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built ftfy lmdb nltk fasttext demjson future\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pytorch-lightning 1.2.7 has requirement PyYAML!=5.4.*,>=5.1, but you'll have pyyaml 5.4.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, torchvision, ftfy, tqdm, sentencepiece, torchtext, lmdb, sacremoses, tokenizers, transformers, matplotlib, PyYAML, omegaconf, xxhash, datasets, nltk, torchmetrics, multidict, yarl, async-timeout, aiohttp, fsspec, future, pytorch-lightning, fasttext, portalocker, iopath, smmap, gitdb, GitPython, demjson, mmf\n",
      "  Found existing installation: torch 1.8.1+cu101\n",
      "    Uninstalling torch-1.8.1+cu101:\n",
      "      Successfully uninstalled torch-1.8.1+cu101\n",
      "  Found existing installation: torchvision 0.9.1+cu101\n",
      "    Uninstalling torchvision-0.9.1+cu101:\n",
      "      Successfully uninstalled torchvision-0.9.1+cu101\n",
      "  Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "  Found existing installation: torchtext 0.9.1\n",
      "    Uninstalling torchtext-0.9.1:\n",
      "      Successfully uninstalled torchtext-0.9.1\n",
      "  Found existing installation: lmdb 0.99\n",
      "    Uninstalling lmdb-0.99:\n",
      "      Successfully uninstalled lmdb-0.99\n",
      "  Found existing installation: matplotlib 3.2.2\n",
      "    Uninstalling matplotlib-3.2.2:\n",
      "      Successfully uninstalled matplotlib-3.2.2\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Running setup.py develop for mmf\n",
      "Successfully installed GitPython-3.1.0 PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.2.1 demjson-2.2.4 fasttext-0.9.1 fsspec-2021.4.0 ftfy-5.8 future-0.18.2 gitdb-4.0.7 iopath-0.1.7 lmdb-0.98 matplotlib-3.3.4 mmf multidict-5.1.0 nltk-3.4.5 omegaconf-2.0.6 portalocker-2.3.0 pytorch-lightning-1.2.7 sacremoses-0.0.45 sentencepiece-0.1.95 smmap-4.0.0 tokenizers-0.9.2 torch-1.8.1 torchmetrics-0.3.1 torchtext-0.5.0 torchvision-0.9.1 tqdm-4.49.0 transformers-3.4.0 xxhash-2.0.2 yarl-1.6.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --editable ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243160,
     "status": "ok",
     "timestamp": 1620078991278,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "x4PQkXOxnsOA",
    "outputId": "3d13b251-3110-470a-8830-1607664b1a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyYAML==5.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 8.4MB/s \n",
      "\u001b[?25hCollecting imgaug==0.2.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
      "\u001b[K     |████████████████████████████████| 634kB 14.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.4.1)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (0.16.2)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.15.0)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (7.1.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.4.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (3.3.4)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.5.1)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.4.7)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.6) (4.4.2)\n",
      "Building wheels for collected packages: PyYAML, imgaug\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=ee8887d138e5b66f3b182861eae5bdc62a166048d843ab05a24272600c5c2d17\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for imgaug: filename=imgaug-0.2.6-cp37-none-any.whl size=654019 sha256=603e048726871d43f887f0533ac08346347ee9f765584d0f2864c9c3871f31c2\n",
      "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
      "Successfully built PyYAML imgaug\n",
      "Installing collected packages: PyYAML, imgaug\n",
      "  Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Found existing installation: imgaug 0.2.9\n",
      "    Uninstalling imgaug-0.2.9:\n",
      "      Successfully uninstalled imgaug-0.2.9\n",
      "Successfully installed PyYAML-5.3.1 imgaug-0.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install PyYAML==5.3.1 imgaug==0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243157,
     "status": "ok",
     "timestamp": 1620078991279,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "TSrAzqf9nvUO",
    "outputId": "b2175b76-3a76-4fe8-f1c9-1ccb0562ba85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/colab\n"
     ]
    }
   ],
   "source": [
    "%cd /content/gdrive/MyDrive/colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 468159,
     "status": "ok",
     "timestamp": 1620079216284,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "4qRna4BrnxaZ",
    "outputId": "f19053af-8889-4e24-8191-08784c9c5510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-03 21:56:45.808565: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Data folder is /root/.cache/torch/mmf/data\n",
      "Zip path is ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Starting checksum for XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Checksum successful\n",
      "Copying ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Unzipping ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Extracting the zip can take time. Sit back and relax.\n",
      "Moving train.jsonl\n",
      "Moving dev_seen.jsonl\n",
      "Moving test_seen.jsonl\n",
      "Moving dev_unseen.jsonl\n",
      "Moving test_unseen.jsonl\n",
      "Moving img\n"
     ]
    }
   ],
   "source": [
    "!mmf_convert_hm --zip_file=\"./XjiOc5ycDBRRNwbhRlgH.zip\" --password=REDACTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1152121,
     "status": "ok",
     "timestamp": 1620010206224,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "UlF9T15Wt92g",
    "outputId": "cbea384a-a943-45ae-ac37-4c6ee01f01bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-03 02:38:45.991593: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-03T02:39:19 | matplotlib.font_manager: \u001b[0mGenerating new fontManager, this may take some time...\n",
      "\u001b[32m2021-05-03T02:39:37 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/from_cc.yaml\n",
      "\u001b[32m2021-05-03T02:39:37 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
      "\u001b[32m2021-05-03T02:39:37 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-03T02:39:37 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2021-05-03T02:39:37 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.from_cc_original\n",
      "\u001b[32m2021-05-03T02:39:37 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
      "\u001b[32m2021-05-03T02:39:38 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2021-05-03T02:39:38 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original', 'checkpoint.resume_pretrained=False'])\n",
      "\u001b[32m2021-05-03T02:39:38 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-03T02:39:38 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-03T02:39:38 | mmf_cli.run: \u001b[0mUsing seed 37978410\n",
      "\u001b[32m2021-05-03T02:39:38 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100% 10.3G/10.3G [02:55<00:00, 58.7MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n",
      "Downloading extras.tar.gz: 100% 211k/211k [00:00<00:00, 565kB/s] \n",
      "[ Starting checksum for extras.tar.gz]\n",
      "[ Checksum successful for extras.tar.gz]\n",
      "Unpacking extras.tar.gz\n",
      "\u001b[32m2021-05-03T02:48:27 | filelock: \u001b[0mLock 140011451745872 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "Downloading: 100% 433/433 [00:00<00:00, 439kB/s]\n",
      "\u001b[32m2021-05-03T02:48:27 | filelock: \u001b[0mLock 140011451745872 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "\u001b[32m2021-05-03T02:48:28 | filelock: \u001b[0mLock 140011486551376 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 1.19MB/s]\n",
      "\u001b[32m2021-05-03T02:48:28 | filelock: \u001b[0mLock 140011486551376 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:48:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:48:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T02:48:28 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T02:48:28 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T02:48:28 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T02:48:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2021-05-03T02:48:29 | filelock: \u001b[0mLock 140011445610128 acquired on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Downloading: 100% 440M/440M [00:13<00:00, 31.7MB/s]\n",
      "\u001b[32m2021-05-03T02:48:43 | filelock: \u001b[0mLock 140011445610128 released on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-03T02:48:53 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-03T02:48:53 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:48:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:48:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-03T02:48:53 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_from_cc.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.finetuned.hateful_memes.from_cc_original/vilbert.finetuned.hateful_memes_from_cc.tar.gz ]\n",
      "Downloading vilbert.finetuned.hateful_memes_from_cc.tar.gz: 100% 918M/918M [00:12<00:00, 72.6MB/s]\n",
      "[ Starting checksum for vilbert.finetuned.hateful_memes_from_cc.tar.gz]\n",
      "[ Checksum successful for vilbert.finetuned.hateful_memes_from_cc.tar.gz]\n",
      "Unpacking vilbert.finetuned.hateful_memes_from_cc.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:49:19 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:49:19 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:49:19 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:49:19 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:49:20 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
      "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
      "Unexpected keys if any: []\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:49:20 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:49:20 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:49:20 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:49:20 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2021-05-03T02:49:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-03T02:49:20 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-03T02:49:20 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-03T02:49:20 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-03T02:49:20 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-03T02:49:20 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
      "  (model): ViLBERTForClassification(\n",
      "    (bert): ViLBERTBase(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (v_embeddings): BertImageFeatureEmbeddings(\n",
      "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (v_layer): ModuleList(\n",
      "          (0): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (c_layer): ModuleList(\n",
      "          (0): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (t_pooler): BertTextPooler(\n",
      "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (v_pooler): BertImagePooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-03T02:49:20 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
      "\u001b[32m2021-05-03T02:49:20 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-03T02:49:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100% 17/17 [00:43<00:00,  2.54s/it]\n",
      "\u001b[32m2021-05-03T02:50:04 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 3.1337, val/total_loss: 3.1337, val/hateful_memes/accuracy: 0.6796, val/hateful_memes/binary_f1: 0.3887, val/hateful_memes/roc_auc: 0.6600\n",
      "\u001b[32m2021-05-03T02:50:04 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01m 10s 396ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/vilbert/from_cc.yaml \\\n",
    "  model=vilbert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=val \\\n",
    "  checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original \\\n",
    "  checkpoint.resume_pretrained=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwAfY1esqz6N"
   },
   "source": [
    "Using pretrained ViLBERT on Conceptual Caption (CC). Fine-tuning on all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16183480,
     "status": "ok",
     "timestamp": 1620039330594,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "j2Lhcy3fn7M0",
    "outputId": "54ea4e5c-c862-43c2-f8f7-b3418c5feac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-03 02:50:09.948501: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/from_cc.yaml\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/gdrive/MyDrive/colab/pretrained_vilbertcc_election_memes/\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.pretrained.cc.original\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to /content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_seen.jsonl\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.train[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T02:50:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.val[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T02:50:16 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.test[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T02:50:16 | mmf: \u001b[0mLogging to: /content/gdrive/MyDrive/colab/pretrained_vilbertcc_election_memes/train.log\n",
      "\u001b[32m2021-05-03T02:50:16 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=train_val', 'training.batch_size=32', 'env.save_dir=/content/gdrive/MyDrive/colab/pretrained_vilbertcc_election_memes/', 'checkpoint.resume_zoo=vilbert.pretrained.cc.original', 'checkpoint.resume_pretrained=True', 'dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl', 'dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb'])\n",
      "\u001b[32m2021-05-03T02:50:16 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-03T02:50:16 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-03T02:50:16 | mmf_cli.run: \u001b[0mUsing seed 16054874\n",
      "\u001b[32m2021-05-03T02:50:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:50:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:50:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T02:50:18 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T02:50:18 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T02:50:18 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T02:50:18 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-03T02:50:31 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-03T02:50:31 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:50:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:50:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-03T02:50:31 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.pretrained.cc_original.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.pretrained.cc.original/vilbert.pretrained.cc_original.tar.gz ]\n",
      "Downloading vilbert.pretrained.cc_original.tar.gz: 100% 918M/918M [00:12<00:00, 71.2MB/s]\n",
      "[ Starting checksum for vilbert.pretrained.cc_original.tar.gz]\n",
      "[ Checksum successful for vilbert.pretrained.cc_original.tar.gz]\n",
      "Unpacking vilbert.pretrained.cc_original.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:50:56 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:50:56 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:50:56 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T02:50:56 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:56 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
      "  (model): ViLBERTForClassification(\n",
      "    (bert): ViLBERTBase(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (v_embeddings): BertImageFeatureEmbeddings(\n",
      "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (v_layer): ModuleList(\n",
      "          (0): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (c_layer): ModuleList(\n",
      "          (0): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (t_pooler): BertTextPooler(\n",
      "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (v_pooler): BertImagePooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
      "\u001b[32m2021-05-03T02:50:57 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2021-05-03T02:56:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.6966, train/hateful_memes/cross_entropy/avg: 0.6966, train/total_loss: 0.6966, train/total_loss/avg: 0.6966, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 0.30, time: 05m 30s 914ms, time_since_start: 05m 57s 246ms, eta: 20h 27m 09s 820ms\n",
      "\u001b[32m2021-05-03T02:58:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.6856, train/hateful_memes/cross_entropy/avg: 0.6911, train/total_loss: 0.6856, train/total_loss/avg: 0.6911, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 443ms, time_since_start: 07m 39s 689ms, eta: 06h 18m 09s 974ms\n",
      "\u001b[32m2021-05-03T02:59:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.6966, train/hateful_memes/cross_entropy/avg: 0.7062, train/total_loss: 0.6966, train/total_loss/avg: 0.7062, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 841ms, time_since_start: 09m 25s 530ms, eta: 06h 28m 55s 048ms\n",
      "\u001b[32m2021-05-03T03:01:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.6856, train/hateful_memes/cross_entropy/avg: 0.6455, train/total_loss: 0.6856, train/total_loss/avg: 0.6455, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 1.00, time: 01m 40s 784ms, time_since_start: 11m 06s 315ms, eta: 06h 08m 37s 854ms\n",
      "\u001b[32m2021-05-03T03:03:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.6856, train/hateful_memes/cross_entropy/avg: 0.6436, train/total_loss: 0.6856, train/total_loss/avg: 0.6436, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 024ms, time_since_start: 12m 49s 340ms, eta: 06h 15m 04s 604ms\n",
      "\u001b[32m2021-05-03T03:05:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.6360, train/hateful_memes/cross_entropy/avg: 0.6287, train/total_loss: 0.6360, train/total_loss/avg: 0.6287, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 991ms, time_since_start: 14m 34s 331ms, eta: 06h 20m 27s 592ms\n",
      "\u001b[32m2021-05-03T03:06:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.6360, train/hateful_memes/cross_entropy/avg: 0.6043, train/total_loss: 0.6360, train/total_loss/avg: 0.6043, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 176ms, time_since_start: 16m 17s 507ms, eta: 06h 12m 08s 224ms\n",
      "\u001b[32m2021-05-03T03:08:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.5546, train/hateful_memes/cross_entropy/avg: 0.5861, train/total_loss: 0.5546, train/total_loss/avg: 0.5861, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 020ms, time_since_start: 18m 528ms, eta: 06h 09m 49s 808ms\n",
      "\u001b[32m2021-05-03T03:10:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.6360, train/hateful_memes/cross_entropy/avg: 0.5950, train/total_loss: 0.6360, train/total_loss/avg: 0.5950, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 790ms, time_since_start: 19m 45s 319ms, eta: 06h 14m 24s 629ms\n",
      "\u001b[32m2021-05-03T03:12:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T03:12:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T03:16:00 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T03:16:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T03:16:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.5546, train/hateful_memes/cross_entropy/avg: 0.5741, train/total_loss: 0.5546, train/total_loss/avg: 0.5741, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 0.26, time: 06m 20s 123ms, time_since_start: 26m 05s 442ms, eta: 22h 31m 43s 152ms\n",
      "\u001b[32m2021-05-03T03:16:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T03:16:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:16:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:16:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T03:17:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T03:18:16 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T03:18:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T03:19:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T03:19:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.8593, val/total_loss: 0.8593, val/hateful_memes/accuracy: 0.5740, val/hateful_memes/binary_f1: 0.3932, val/hateful_memes/roc_auc: 0.6433, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 02m 58s 030ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.643277\n",
      "\u001b[32m2021-05-03T03:22:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.5546, train/hateful_memes/cross_entropy/avg: 0.5572, train/total_loss: 0.5546, train/total_loss/avg: 0.5572, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 0.66, time: 02m 32s 812ms, time_since_start: 31m 36s 291ms, eta: 09h 48s 745ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:23:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:23:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T03:23:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.4631, train/hateful_memes/cross_entropy/avg: 0.5332, train/total_loss: 0.4631, train/total_loss/avg: 0.5332, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 009ms, time_since_start: 33m 21s 300ms, eta: 06h 09m 51s 480ms\n",
      "\u001b[32m2021-05-03T03:25:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.4631, train/hateful_memes/cross_entropy/avg: 0.5186, train/total_loss: 0.4631, train/total_loss/avg: 0.5186, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 885ms, time_since_start: 35m 05s 186ms, eta: 06h 04m 08s 366ms\n",
      "\u001b[32m2021-05-03T03:27:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.4589, train/hateful_memes/cross_entropy/avg: 0.5074, train/total_loss: 0.4589, train/total_loss/avg: 0.5074, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 914ms, time_since_start: 36m 49s 101ms, eta: 06h 02m 28s 983ms\n",
      "\u001b[32m2021-05-03T03:29:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.4589, train/hateful_memes/cross_entropy/avg: 0.4941, train/total_loss: 0.4589, train/total_loss/avg: 0.4941, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 048ms, time_since_start: 38m 34s 150ms, eta: 06h 04m 39s 603ms\n",
      "\u001b[32m2021-05-03T03:30:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.4575, train/hateful_memes/cross_entropy/avg: 0.4797, train/total_loss: 0.4575, train/total_loss/avg: 0.4797, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 776ms, time_since_start: 40m 17s 926ms, eta: 05h 58m 29s 112ms\n",
      "\u001b[32m2021-05-03T03:32:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.4575, train/hateful_memes/cross_entropy/avg: 0.4609, train/total_loss: 0.4575, train/total_loss/avg: 0.4609, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 593ms, time_since_start: 42m 01s 520ms, eta: 05h 56m 06s 047ms\n",
      "\u001b[32m2021-05-03T03:34:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.3882, train/hateful_memes/cross_entropy/avg: 0.4436, train/total_loss: 0.3882, train/total_loss/avg: 0.4436, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 715ms, time_since_start: 43m 47s 236ms, eta: 06h 01m 36s 261ms\n",
      "\u001b[32m2021-05-03T03:36:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.3882, train/hateful_memes/cross_entropy/avg: 0.4261, train/total_loss: 0.3882, train/total_loss/avg: 0.4261, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 486ms, time_since_start: 45m 30s 723ms, eta: 05h 52m 13s 621ms\n",
      "\u001b[32m2021-05-03T03:37:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T03:37:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T03:38:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T03:38:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T03:38:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.3860, train/hateful_memes/cross_entropy/avg: 0.4198, train/total_loss: 0.3860, train/total_loss/avg: 0.4198, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 43s 159ms, time_since_start: 48m 13s 883ms, eta: 09h 12m 34s 109ms\n",
      "\u001b[32m2021-05-03T03:38:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T03:38:45 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:38:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:38:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T03:39:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T03:40:15 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T03:40:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T03:41:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T03:41:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 0.9701, val/total_loss: 0.9701, val/hateful_memes/accuracy: 0.6380, val/hateful_memes/binary_f1: 0.5987, val/hateful_memes/roc_auc: 0.6885, num_updates: 2000, epoch: 7, iterations: 2000, max_updates: 22000, val_time: 02m 44s 356ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.688467\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:42:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T03:42:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T03:44:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.3611, train/hateful_memes/cross_entropy/avg: 0.4037, train/total_loss: 0.3611, train/total_loss/avg: 0.4037, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 46s 205ms, time_since_start: 53m 44s 450ms, eta: 09h 20m 04s 024ms\n",
      "\u001b[32m2021-05-03T03:45:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.3434, train/hateful_memes/cross_entropy/avg: 0.3988, train/total_loss: 0.3434, train/total_loss/avg: 0.3988, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 829ms, time_since_start: 55m 28s 279ms, eta: 05h 48m 07s 095ms\n",
      "\u001b[32m2021-05-03T03:47:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.3084, train/hateful_memes/cross_entropy/avg: 0.3854, train/total_loss: 0.3084, train/total_loss/avg: 0.3854, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 875ms, time_since_start: 57m 12s 154ms, eta: 05h 46m 30s 844ms\n",
      "\u001b[32m2021-05-03T03:49:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.3007, train/hateful_memes/cross_entropy/avg: 0.3708, train/total_loss: 0.3007, train/total_loss/avg: 0.3708, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 518ms, time_since_start: 58m 57s 673ms, eta: 05h 50m 12s 531ms\n",
      "\u001b[32m2021-05-03T03:51:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.2947, train/hateful_memes/cross_entropy/avg: 0.3641, train/total_loss: 0.2947, train/total_loss/avg: 0.3641, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 857ms, time_since_start: 01h 41s 531ms, eta: 05h 42m 56s 344ms\n",
      "\u001b[32m2021-05-03T03:52:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.2691, train/hateful_memes/cross_entropy/avg: 0.3548, train/total_loss: 0.2691, train/total_loss/avg: 0.3548, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 993ms, time_since_start: 01h 02m 24s 525ms, eta: 05h 38m 20s 527ms\n",
      "\u001b[32m2021-05-03T03:54:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.2639, train/hateful_memes/cross_entropy/avg: 0.3426, train/total_loss: 0.2639, train/total_loss/avg: 0.3426, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 453ms, time_since_start: 01h 04m 09s 978ms, eta: 05h 44m 38s 238ms\n",
      "\u001b[32m2021-05-03T03:56:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.2028, train/hateful_memes/cross_entropy/avg: 0.3327, train/total_loss: 0.2028, train/total_loss/avg: 0.3327, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 923ms, time_since_start: 01h 05m 53s 902ms, eta: 05h 37m 52s 610ms\n",
      "\u001b[32m2021-05-03T03:58:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.1604, train/hateful_memes/cross_entropy/avg: 0.3219, train/total_loss: 0.1604, train/total_loss/avg: 0.3219, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 434ms, time_since_start: 01h 07m 40s 337ms, eta: 05h 44m 14s 215ms\n",
      "\u001b[32m2021-05-03T03:59:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T03:59:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T04:00:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T04:00:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T04:00:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.1492, train/hateful_memes/cross_entropy/avg: 0.3117, train/total_loss: 0.1492, train/total_loss/avg: 0.3117, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 0.62, time: 02m 41s 229ms, time_since_start: 01h 10m 21s 566ms, eta: 08h 38m 43s 808ms\n",
      "\u001b[32m2021-05-03T04:00:52 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T04:00:52 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:00:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:00:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T04:01:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T04:02:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T04:02:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T04:02:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 1.6070, val/total_loss: 1.6070, val/hateful_memes/accuracy: 0.6320, val/hateful_memes/binary_f1: 0.5354, val/hateful_memes/roc_auc: 0.6876, num_updates: 3000, epoch: 11, iterations: 3000, max_updates: 22000, val_time: 02m 04s 773ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.688467\n",
      "\u001b[32m2021-05-03T04:05:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.1223, train/hateful_memes/cross_entropy/avg: 0.3023, train/total_loss: 0.1223, train/total_loss/avg: 0.3023, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 0.66, time: 02m 31s 331ms, time_since_start: 01h 14m 57s 673ms, eta: 08h 04m 19s 200ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:06:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:06:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T04:07:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.1107, train/hateful_memes/cross_entropy/avg: 0.2933, train/total_loss: 0.1107, train/total_loss/avg: 0.2933, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 450ms, time_since_start: 01h 16m 43s 123ms, eta: 05h 35m 41s 842ms\n",
      "\u001b[32m2021-05-03T04:08:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0910, train/hateful_memes/cross_entropy/avg: 0.2853, train/total_loss: 0.0910, train/total_loss/avg: 0.2853, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 209ms, time_since_start: 01h 18m 26s 333ms, eta: 05h 26m 49s 078ms\n",
      "\u001b[32m2021-05-03T04:10:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0910, train/hateful_memes/cross_entropy/avg: 0.2798, train/total_loss: 0.0910, train/total_loss/avg: 0.2798, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 902ms, time_since_start: 01h 20m 10s 236ms, eta: 05h 27m 15s 118ms\n",
      "\u001b[32m2021-05-03T04:12:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0822, train/hateful_memes/cross_entropy/avg: 0.2720, train/total_loss: 0.0822, train/total_loss/avg: 0.2720, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 379ms, time_since_start: 01h 21m 55s 615ms, eta: 05h 30m 07s 061ms\n",
      "\u001b[32m2021-05-03T04:14:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0657, train/hateful_memes/cross_entropy/avg: 0.2646, train/total_loss: 0.0657, train/total_loss/avg: 0.2646, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 071ms, time_since_start: 01h 23m 39s 687ms, eta: 05h 24m 15s 572ms\n",
      "\u001b[32m2021-05-03T04:15:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0360, train/hateful_memes/cross_entropy/avg: 0.2577, train/total_loss: 0.0360, train/total_loss/avg: 0.2577, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 535ms, time_since_start: 01h 25m 23s 223ms, eta: 05h 20m 50s 223ms\n",
      "\u001b[32m2021-05-03T04:17:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0303, train/hateful_memes/cross_entropy/avg: 0.2511, train/total_loss: 0.0303, train/total_loss/avg: 0.2511, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 598ms, time_since_start: 01h 27m 08s 821ms, eta: 05h 25m 26s 381ms\n",
      "\u001b[32m2021-05-03T04:19:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0243, train/hateful_memes/cross_entropy/avg: 0.2449, train/total_loss: 0.0243, train/total_loss/avg: 0.2449, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 558ms, time_since_start: 01h 28m 52s 379ms, eta: 05h 17m 23s 942ms\n",
      "\u001b[32m2021-05-03T04:21:07 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T04:21:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T04:21:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T04:22:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T04:22:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0243, train/hateful_memes/cross_entropy/avg: 0.2394, train/total_loss: 0.0243, train/total_loss/avg: 0.2394, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 0.62, time: 02m 42s 254ms, time_since_start: 01h 31m 34s 634ms, eta: 08h 14m 33s 183ms\n",
      "\u001b[32m2021-05-03T04:22:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T04:22:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:22:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:22:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T04:23:10 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T04:26:30 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T04:27:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T04:27:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T04:27:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.7562, val/total_loss: 1.7562, val/hateful_memes/accuracy: 0.6180, val/hateful_memes/binary_f1: 0.5090, val/hateful_memes/roc_auc: 0.6965, num_updates: 4000, epoch: 14, iterations: 4000, max_updates: 22000, val_time: 05m 37s 705ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.696468\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:29:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:29:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T04:30:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0243, train/hateful_memes/cross_entropy/avg: 0.2345, train/total_loss: 0.0243, train/total_loss/avg: 0.2345, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 48s 126ms, time_since_start: 01h 40m 477ms, eta: 08h 29m 36s 103ms\n",
      "\u001b[32m2021-05-03T04:32:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0225, train/hateful_memes/cross_entropy/avg: 0.2290, train/total_loss: 0.0225, train/total_loss/avg: 0.2290, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 330ms, time_since_start: 01h 41m 43s 808ms, eta: 05h 11m 27s 096ms\n",
      "\u001b[32m2021-05-03T04:33:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0196, train/hateful_memes/cross_entropy/avg: 0.2238, train/total_loss: 0.0196, train/total_loss/avg: 0.2238, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 300ms, time_since_start: 01h 43m 27s 108ms, eta: 05h 09m 36s 760ms\n",
      "\u001b[32m2021-05-03T04:35:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0155, train/hateful_memes/cross_entropy/avg: 0.2190, train/total_loss: 0.0155, train/total_loss/avg: 0.2190, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 956ms, time_since_start: 01h 45m 13s 065ms, eta: 05h 15m 46s 791ms\n",
      "\u001b[32m2021-05-03T04:37:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0150, train/hateful_memes/cross_entropy/avg: 0.2142, train/total_loss: 0.0150, train/total_loss/avg: 0.2142, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 731ms, time_since_start: 01h 46m 56s 797ms, eta: 05h 07m 23s 481ms\n",
      "\u001b[32m2021-05-03T04:39:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0150, train/hateful_memes/cross_entropy/avg: 0.2102, train/total_loss: 0.0150, train/total_loss/avg: 0.2102, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 645ms, time_since_start: 01h 48m 40s 443ms, eta: 05h 05m 22s 913ms\n",
      "\u001b[32m2021-05-03T04:40:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0124, train/hateful_memes/cross_entropy/avg: 0.2060, train/total_loss: 0.0124, train/total_loss/avg: 0.2060, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 769ms, time_since_start: 01h 50m 25s 212ms, eta: 05h 06m 55s 061ms\n",
      "\u001b[32m2021-05-03T04:42:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0119, train/hateful_memes/cross_entropy/avg: 0.2017, train/total_loss: 0.0119, train/total_loss/avg: 0.2017, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 618ms, time_since_start: 01h 52m 08s 830ms, eta: 05h 01m 47s 523ms\n",
      "\u001b[32m2021-05-03T04:44:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0119, train/hateful_memes/cross_entropy/avg: 0.2010, train/total_loss: 0.0119, train/total_loss/avg: 0.2010, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 504ms, time_since_start: 01h 53m 52s 335ms, eta: 04h 59m 42s 452ms\n",
      "\u001b[32m2021-05-03T04:46:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T04:46:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T04:46:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T04:47:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T04:47:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0087, train/hateful_memes/cross_entropy/avg: 0.1970, train/total_loss: 0.0087, train/total_loss/avg: 0.1970, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 45s 566ms, time_since_start: 01h 56m 37s 901ms, eta: 07h 56m 36s 602ms\n",
      "\u001b[32m2021-05-03T04:47:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T04:47:09 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:47:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:47:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T04:48:17 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T04:48:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T04:49:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T04:49:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.9743, val/total_loss: 1.9743, val/hateful_memes/accuracy: 0.6120, val/hateful_memes/binary_f1: 0.4948, val/hateful_memes/roc_auc: 0.6939, num_updates: 5000, epoch: 18, iterations: 5000, max_updates: 22000, val_time: 02m 07s 381ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.696468\n",
      "\u001b[32m2021-05-03T04:51:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0087, train/hateful_memes/cross_entropy/avg: 0.1934, train/total_loss: 0.0087, train/total_loss/avg: 0.1934, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 0.67, time: 02m 30s 133ms, time_since_start: 02h 01m 15s 422ms, eta: 07h 09m 38s 559ms\n",
      "\u001b[32m2021-05-03T04:53:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0086, train/hateful_memes/cross_entropy/avg: 0.1898, train/total_loss: 0.0086, train/total_loss/avg: 0.1898, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 342ms, time_since_start: 02h 02m 58s 765ms, eta: 04h 53m 59s 318ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:53:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T04:53:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T04:55:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0083, train/hateful_memes/cross_entropy/avg: 0.1863, train/total_loss: 0.0083, train/total_loss/avg: 0.1863, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 646ms, time_since_start: 02h 04m 45s 411ms, eta: 05h 01m 34s 903ms\n",
      "\u001b[32m2021-05-03T04:57:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1830, train/total_loss: 0.0073, train/total_loss/avg: 0.1830, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 535ms, time_since_start: 02h 06m 28s 946ms, eta: 04h 51m 01s 854ms\n",
      "\u001b[32m2021-05-03T04:58:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1797, train/total_loss: 0.0073, train/total_loss/avg: 0.1797, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 126ms, time_since_start: 02h 08m 15s 073ms, eta: 04h 56m 31s 060ms\n",
      "\u001b[32m2021-05-03T05:00:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1765, train/total_loss: 0.0073, train/total_loss/avg: 0.1765, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 865ms, time_since_start: 02h 09m 57s 939ms, eta: 04h 45m 39s 869ms\n",
      "\u001b[32m2021-05-03T05:02:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1735, train/total_loss: 0.0062, train/total_loss/avg: 0.1735, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 709ms, time_since_start: 02h 11m 41s 649ms, eta: 04h 46m 15s 201ms\n",
      "\u001b[32m2021-05-03T05:03:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0062, train/hateful_memes/cross_entropy/avg: 0.1705, train/total_loss: 0.0062, train/total_loss/avg: 0.1705, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 144ms, time_since_start: 02h 13m 27s 794ms, eta: 04h 51m 10s 603ms\n",
      "\u001b[32m2021-05-03T05:05:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0059, train/hateful_memes/cross_entropy/avg: 0.1676, train/total_loss: 0.0059, train/total_loss/avg: 0.1676, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 714ms, time_since_start: 02h 15m 11s 508ms, eta: 04h 42m 45s 242ms\n",
      "\u001b[32m2021-05-03T05:07:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T05:07:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:07:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:08:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:08:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1649, train/total_loss: 0.0035, train/total_loss/avg: 0.1649, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 45s 101ms, time_since_start: 02h 17m 56s 610ms, eta: 07h 27m 18s 946ms\n",
      "\u001b[32m2021-05-03T05:08:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T05:08:27 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:08:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:08:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:09:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:09:59 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:10:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:10:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.4975, val/total_loss: 2.4975, val/hateful_memes/accuracy: 0.5980, val/hateful_memes/binary_f1: 0.4306, val/hateful_memes/roc_auc: 0.6961, num_updates: 6000, epoch: 21, iterations: 6000, max_updates: 22000, val_time: 02m 10s 601ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.696468\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:12:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:12:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:13:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1655, train/total_loss: 0.0035, train/total_loss/avg: 0.1655, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 0.67, time: 02m 30s 832ms, time_since_start: 02h 22m 38s 048ms, eta: 06h 46m 06s 168ms\n",
      "\u001b[32m2021-05-03T05:14:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0059, train/hateful_memes/cross_entropy/avg: 0.1630, train/total_loss: 0.0059, train/total_loss/avg: 0.1630, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 467ms, time_since_start: 02h 24m 21s 515ms, eta: 04h 36m 49s 408ms\n",
      "\u001b[32m2021-05-03T05:16:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1605, train/total_loss: 0.0035, train/total_loss/avg: 0.1605, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 844ms, time_since_start: 02h 26m 05s 360ms, eta: 04h 36m 04s 409ms\n",
      "\u001b[32m2021-05-03T05:18:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1581, train/total_loss: 0.0035, train/total_loss/avg: 0.1581, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 148ms, time_since_start: 02h 27m 50s 508ms, eta: 04h 37m 45s 605ms\n",
      "\u001b[32m2021-05-03T05:20:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1557, train/total_loss: 0.0035, train/total_loss/avg: 0.1557, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 549ms, time_since_start: 02h 29m 34s 058ms, eta: 04h 31m 46s 994ms\n",
      "\u001b[32m2021-05-03T05:21:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1537, train/total_loss: 0.0035, train/total_loss/avg: 0.1537, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 248ms, time_since_start: 02h 31m 17s 307ms, eta: 04h 29m 14s 721ms\n",
      "\u001b[32m2021-05-03T05:23:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1514, train/total_loss: 0.0024, train/total_loss/avg: 0.1514, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 720ms, time_since_start: 02h 33m 03s 027ms, eta: 04h 33m 54s 023ms\n",
      "\u001b[32m2021-05-03T05:25:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1492, train/total_loss: 0.0032, train/total_loss/avg: 0.1492, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 196ms, time_since_start: 02h 34m 46s 224ms, eta: 04h 25m 36s 887ms\n",
      "\u001b[32m2021-05-03T05:27:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1472, train/total_loss: 0.0032, train/total_loss/avg: 0.1472, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 400ms, time_since_start: 02h 36m 29s 624ms, eta: 04h 24m 23s 262ms\n",
      "\u001b[32m2021-05-03T05:28:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T05:28:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:29:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:29:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:29:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1451, train/total_loss: 0.0032, train/total_loss/avg: 0.1451, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 47s 708ms, time_since_start: 02h 39m 17s 333ms, eta: 07h 05m 58s 751ms\n",
      "\u001b[32m2021-05-03T05:29:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T05:29:48 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:29:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:29:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:30:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:31:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:32:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:32:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.2911, val/total_loss: 2.2911, val/hateful_memes/accuracy: 0.6260, val/hateful_memes/binary_f1: 0.5290, val/hateful_memes/roc_auc: 0.6960, num_updates: 7000, epoch: 25, iterations: 7000, max_updates: 22000, val_time: 02m 12s 346ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.696468\n",
      "\u001b[32m2021-05-03T05:34:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1433, train/total_loss: 0.0032, train/total_loss/avg: 0.1433, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 0.67, time: 02m 29s 640ms, time_since_start: 02h 43m 59s 326ms, eta: 06h 17m 33s 221ms\n",
      "\u001b[32m2021-05-03T05:36:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1417, train/total_loss: 0.0032, train/total_loss/avg: 0.1417, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 926ms, time_since_start: 02h 45m 42s 253ms, eta: 04h 17m 56s 805ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:36:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:36:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:37:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1397, train/total_loss: 0.0032, train/total_loss/avg: 0.1397, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 042ms, time_since_start: 02h 47m 26s 295ms, eta: 04h 18m 58s 967ms\n",
      "\u001b[32m2021-05-03T05:39:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1385, train/total_loss: 0.0032, train/total_loss/avg: 0.1385, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 165ms, time_since_start: 02h 49m 10s 461ms, eta: 04h 17m 31s 527ms\n",
      "\u001b[32m2021-05-03T05:41:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1367, train/total_loss: 0.0032, train/total_loss/avg: 0.1367, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 635ms, time_since_start: 02h 50m 54s 096ms, eta: 04h 14m 27s 537ms\n",
      "\u001b[32m2021-05-03T05:43:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1349, train/total_loss: 0.0032, train/total_loss/avg: 0.1349, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 559ms, time_since_start: 02h 52m 39s 656ms, eta: 04h 17m 23s 815ms\n",
      "\u001b[32m2021-05-03T05:44:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1331, train/total_loss: 0.0024, train/total_loss/avg: 0.1331, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 684ms, time_since_start: 02h 54m 23s 340ms, eta: 04h 11m 04s 052ms\n",
      "\u001b[32m2021-05-03T05:46:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1314, train/total_loss: 0.0024, train/total_loss/avg: 0.1314, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 184ms, time_since_start: 02h 56m 06s 525ms, eta: 04h 08m 06s 706ms\n",
      "\u001b[32m2021-05-03T05:48:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1298, train/total_loss: 0.0024, train/total_loss/avg: 0.1298, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 606ms, time_since_start: 02h 57m 52s 132ms, eta: 04h 12m 08s 796ms\n",
      "\u001b[32m2021-05-03T05:50:07 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T05:50:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:50:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:51:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:51:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0024, train/hateful_memes/cross_entropy/avg: 0.1284, train/total_loss: 0.0024, train/total_loss/avg: 0.1284, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 44s 032ms, time_since_start: 03h 36s 164ms, eta: 06h 28m 52s 005ms\n",
      "\u001b[32m2021-05-03T05:51:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T05:51:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:51:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:51:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:52:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T05:52:45 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T05:53:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T05:54:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T05:54:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.7291, val/total_loss: 2.7291, val/hateful_memes/accuracy: 0.6160, val/hateful_memes/binary_f1: 0.4811, val/hateful_memes/roc_auc: 0.6983, num_updates: 8000, epoch: 28, iterations: 8000, max_updates: 22000, val_time: 03m 25s 459ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.698301\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:57:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T05:57:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T05:57:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0021, train/hateful_memes/cross_entropy/avg: 0.1268, train/total_loss: 0.0021, train/total_loss/avg: 0.1268, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00001, ups: 0.62, time: 02m 41s 749ms, time_since_start: 03h 06m 43s 377ms, eta: 06h 20m 42s 922ms\n",
      "\u001b[32m2021-05-03T05:58:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1252, train/total_loss: 0.0015, train/total_loss/avg: 0.1252, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 687ms, time_since_start: 03h 08m 26s 065ms, eta: 03h 59m 57s 630ms\n",
      "\u001b[32m2021-05-03T06:00:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1237, train/total_loss: 0.0015, train/total_loss/avg: 0.1237, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 437ms, time_since_start: 03h 10m 09s 502ms, eta: 03h 59m 57s 661ms\n",
      "\u001b[32m2021-05-03T06:02:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1223, train/total_loss: 0.0015, train/total_loss/avg: 0.1223, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 878ms, time_since_start: 03h 11m 56s 381ms, eta: 04h 06m 08s 112ms\n",
      "\u001b[32m2021-05-03T06:04:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1212, train/total_loss: 0.0015, train/total_loss/avg: 0.1212, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 352ms, time_since_start: 03h 13m 39s 734ms, eta: 03h 56m 15s 782ms\n",
      "\u001b[32m2021-05-03T06:05:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1198, train/total_loss: 0.0015, train/total_loss/avg: 0.1198, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 834ms, time_since_start: 03h 15m 23s 568ms, eta: 03h 55m 36s 478ms\n",
      "\u001b[32m2021-05-03T06:07:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1184, train/total_loss: 0.0015, train/total_loss/avg: 0.1184, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 480ms, time_since_start: 03h 17m 08s 050ms, eta: 03h 55m 18s 313ms\n",
      "\u001b[32m2021-05-03T06:09:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1171, train/total_loss: 0.0015, train/total_loss/avg: 0.1171, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 926ms, time_since_start: 03h 18m 50s 976ms, eta: 03h 50m 03s 683ms\n",
      "\u001b[32m2021-05-03T06:11:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1159, train/total_loss: 0.0015, train/total_loss/avg: 0.1159, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 323ms, time_since_start: 03h 20m 34s 300ms, eta: 03h 49m 11s 947ms\n",
      "\u001b[32m2021-05-03T06:12:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T06:12:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:13:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:13:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:13:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1146, train/total_loss: 0.0015, train/total_loss/avg: 0.1146, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 45s 846ms, time_since_start: 03h 23m 20s 146ms, eta: 06h 05m 05s 014ms\n",
      "\u001b[32m2021-05-03T06:13:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T06:13:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:13:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:13:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T06:14:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:15:27 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T06:16:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:16:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:16:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.5275, val/total_loss: 2.5275, val/hateful_memes/accuracy: 0.6320, val/hateful_memes/binary_f1: 0.5208, val/hateful_memes/roc_auc: 0.7062, num_updates: 9000, epoch: 32, iterations: 9000, max_updates: 22000, val_time: 02m 49s 802ms, best_update: 9000, best_iteration: 9000, best_val/hateful_memes/roc_auc: 0.706190\n",
      "\u001b[32m2021-05-03T06:19:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.1133, train/total_loss: 0.0011, train/total_loss/avg: 0.1133, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00001, ups: 0.62, time: 02m 42s 295ms, time_since_start: 03h 28m 52s 247ms, eta: 05h 54m 31s 104ms\n",
      "\u001b[32m2021-05-03T06:21:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.1121, train/total_loss: 0.0011, train/total_loss/avg: 0.1121, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00001, ups: 0.99, time: 01m 41s 567ms, time_since_start: 03h 30m 33s 815ms, eta: 03h 40m 08s 636ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:21:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:21:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T06:22:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.1109, train/total_loss: 0.0011, train/total_loss/avg: 0.1109, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 569ms, time_since_start: 03h 32m 18s 385ms, eta: 03h 44m 52s 840ms\n",
      "\u001b[32m2021-05-03T06:24:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1097, train/total_loss: 0.0009, train/total_loss/avg: 0.1097, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 797ms, time_since_start: 03h 34m 02s 182ms, eta: 03h 41m 27s 726ms\n",
      "\u001b[32m2021-05-03T06:26:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1086, train/total_loss: 0.0009, train/total_loss/avg: 0.1086, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 005ms, time_since_start: 03h 35m 46s 187ms, eta: 03h 40m 08s 675ms\n",
      "\u001b[32m2021-05-03T06:28:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.1074, train/total_loss: 0.0011, train/total_loss/avg: 0.1074, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 551ms, time_since_start: 03h 37m 30s 739ms, eta: 03h 39m 31s 878ms\n",
      "\u001b[32m2021-05-03T06:29:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1063, train/total_loss: 0.0009, train/total_loss/avg: 0.1063, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 140ms, time_since_start: 03h 39m 14s 880ms, eta: 03h 36m 54s 226ms\n",
      "\u001b[32m2021-05-03T06:31:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1053, train/total_loss: 0.0007, train/total_loss/avg: 0.1053, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 716ms, time_since_start: 03h 40m 58s 596ms, eta: 03h 34m 15s 879ms\n",
      "\u001b[32m2021-05-03T06:33:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1043, train/total_loss: 0.0007, train/total_loss/avg: 0.1043, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 985ms, time_since_start: 03h 42m 44s 582ms, eta: 03h 37m 09s 446ms\n",
      "\u001b[32m2021-05-03T06:34:59 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T06:34:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:35:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:36:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:36:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1033, train/total_loss: 0.0006, train/total_loss/avg: 0.1033, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 45s 124ms, time_since_start: 03h 45m 29s 707ms, eta: 05h 35m 32s 011ms\n",
      "\u001b[32m2021-05-03T06:36:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T06:36:00 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:36:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:36:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T06:37:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:37:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:38:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:38:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.8544, val/total_loss: 2.8544, val/hateful_memes/accuracy: 0.6300, val/hateful_memes/binary_f1: 0.5119, val/hateful_memes/roc_auc: 0.6967, num_updates: 10000, epoch: 35, iterations: 10000, max_updates: 22000, val_time: 02m 03s 506ms, best_update: 9000, best_iteration: 9000, best_val/hateful_memes/roc_auc: 0.706190\n",
      "\u001b[32m2021-05-03T06:40:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1023, train/total_loss: 0.0006, train/total_loss/avg: 0.1023, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00001, ups: 0.67, time: 02m 29s 588ms, time_since_start: 03h 50m 02s 812ms, eta: 05h 01m 25s 894ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:40:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:40:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T06:42:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1013, train/total_loss: 0.0007, train/total_loss/avg: 0.1013, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 686ms, time_since_start: 03h 51m 48s 499ms, eta: 03h 31m 10s 571ms\n",
      "\u001b[32m2021-05-03T06:44:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1004, train/total_loss: 0.0007, train/total_loss/avg: 0.1004, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 331ms, time_since_start: 03h 53m 32s 830ms, eta: 03h 26m 42s 046ms\n",
      "\u001b[32m2021-05-03T06:45:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0994, train/total_loss: 0.0006, train/total_loss/avg: 0.0994, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 937ms, time_since_start: 03h 55m 16s 768ms, eta: 03h 24m 09s 656ms\n",
      "\u001b[32m2021-05-03T06:47:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0985, train/total_loss: 0.0006, train/total_loss/avg: 0.0985, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 129ms, time_since_start: 03h 57m 02s 897ms, eta: 03h 26m 40s 141ms\n",
      "\u001b[32m2021-05-03T06:49:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0975, train/total_loss: 0.0005, train/total_loss/avg: 0.0975, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 407ms, time_since_start: 03h 58m 47s 305ms, eta: 03h 21m 32s 935ms\n",
      "\u001b[32m2021-05-03T06:51:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0968, train/total_loss: 0.0005, train/total_loss/avg: 0.0968, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 936ms, time_since_start: 04h 34s 241ms, eta: 03h 24m 37s 155ms\n",
      "\u001b[32m2021-05-03T06:52:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0959, train/total_loss: 0.0005, train/total_loss/avg: 0.0959, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 830ms, time_since_start: 04h 02m 17s 072ms, eta: 03h 15m 01s 263ms\n",
      "\u001b[32m2021-05-03T06:54:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0951, train/total_loss: 0.0005, train/total_loss/avg: 0.0951, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 580ms, time_since_start: 04h 04m 652ms, eta: 03h 14m 41s 396ms\n",
      "\u001b[32m2021-05-03T06:56:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T06:56:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:56:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:57:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:57:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0942, train/total_loss: 0.0005, train/total_loss/avg: 0.0942, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 48s 484ms, time_since_start: 04h 06m 49s 137ms, eta: 05h 13m 49s 861ms\n",
      "\u001b[32m2021-05-03T06:57:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T06:57:20 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:57:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T06:57:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T06:58:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T06:58:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T06:59:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T06:59:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.9099, val/total_loss: 2.9099, val/hateful_memes/accuracy: 0.6360, val/hateful_memes/binary_f1: 0.5054, val/hateful_memes/roc_auc: 0.7020, num_updates: 11000, epoch: 39, iterations: 11000, max_updates: 22000, val_time: 02m 07s 266ms, best_update: 9000, best_iteration: 9000, best_val/hateful_memes/roc_auc: 0.706190\n",
      "\u001b[32m2021-05-03T07:01:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0934, train/total_loss: 0.0005, train/total_loss/avg: 0.0934, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00001, ups: 0.66, time: 02m 31s 559ms, time_since_start: 04h 11m 27s 966ms, eta: 04h 39m 44s 270ms\n",
      "\u001b[32m2021-05-03T07:03:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0925, train/total_loss: 0.0005, train/total_loss/avg: 0.0925, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 360ms, time_since_start: 04h 13m 12s 327ms, eta: 03h 10m 51s 294ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:04:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:04:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T07:05:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0917, train/total_loss: 0.0005, train/total_loss/avg: 0.0917, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 066ms, time_since_start: 04h 14m 58s 394ms, eta: 03h 12m 10s 717ms\n",
      "\u001b[32m2021-05-03T07:07:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0909, train/total_loss: 0.0005, train/total_loss/avg: 0.0909, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 115ms, time_since_start: 04h 16m 42s 509ms, eta: 03h 06m 52s 773ms\n",
      "\u001b[32m2021-05-03T07:08:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0901, train/total_loss: 0.0005, train/total_loss/avg: 0.0901, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 402ms, time_since_start: 04h 18m 26s 911ms, eta: 03h 05m 37s 656ms\n",
      "\u001b[32m2021-05-03T07:10:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0893, train/total_loss: 0.0004, train/total_loss/avg: 0.0893, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 258ms, time_since_start: 04h 20m 12s 170ms, eta: 03h 05m 22s 078ms\n",
      "\u001b[32m2021-05-03T07:12:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0886, train/total_loss: 0.0003, train/total_loss/avg: 0.0886, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 398ms, time_since_start: 04h 21m 56s 569ms, eta: 03h 02m 05s 083ms\n",
      "\u001b[32m2021-05-03T07:14:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0878, train/total_loss: 0.0003, train/total_loss/avg: 0.0878, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 237ms, time_since_start: 04h 23m 40s 806ms, eta: 03h 02s 370ms\n",
      "\u001b[32m2021-05-03T07:15:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0871, train/total_loss: 0.0003, train/total_loss/avg: 0.0871, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 026ms, time_since_start: 04h 25m 26s 833ms, eta: 03h 01m 20s 006ms\n",
      "\u001b[32m2021-05-03T07:17:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T07:17:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T07:18:04 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T07:18:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T07:18:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0864, train/total_loss: 0.0003, train/total_loss/avg: 0.0864, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 45s 114ms, time_since_start: 04h 28m 11s 948ms, eta: 04h 39m 35s 650ms\n",
      "\u001b[32m2021-05-03T07:18:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T07:18:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:18:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:18:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T07:19:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T07:20:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T07:20:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T07:20:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.7994, val/total_loss: 2.7994, val/hateful_memes/accuracy: 0.6300, val/hateful_memes/binary_f1: 0.5093, val/hateful_memes/roc_auc: 0.6944, num_updates: 12000, epoch: 42, iterations: 12000, max_updates: 22000, val_time: 02m 04s 859ms, best_update: 9000, best_iteration: 9000, best_val/hateful_memes/roc_auc: 0.706190\n",
      "\u001b[32m2021-05-03T07:23:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0857, train/total_loss: 0.0003, train/total_loss/avg: 0.0857, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0., ups: 0.65, time: 02m 35s 542ms, time_since_start: 04h 32m 52s 353ms, eta: 04h 20m 45s 040ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:24:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:24:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T07:25:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0850, train/total_loss: 0.0003, train/total_loss/avg: 0.0850, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 695ms, time_since_start: 04h 34m 36s 048ms, eta: 02h 52m 04s 728ms\n",
      "\u001b[32m2021-05-03T07:26:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0843, train/total_loss: 0.0003, train/total_loss/avg: 0.0843, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 928ms, time_since_start: 04h 36m 19s 977ms, eta: 02h 50m 42s 367ms\n",
      "\u001b[32m2021-05-03T07:28:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0836, train/total_loss: 0.0002, train/total_loss/avg: 0.0836, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 714ms, time_since_start: 04h 38m 03s 692ms, eta: 02h 48m 35s 915ms\n",
      "\u001b[32m2021-05-03T07:30:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0829, train/total_loss: 0.0003, train/total_loss/avg: 0.0829, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 957ms, time_since_start: 04h 39m 48s 649ms, eta: 02h 48m 50s 497ms\n",
      "\u001b[32m2021-05-03T07:32:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0823, train/total_loss: 0.0002, train/total_loss/avg: 0.0823, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 355ms, time_since_start: 04h 41m 32s 005ms, eta: 02h 44m 30s 870ms\n",
      "\u001b[32m2021-05-03T07:33:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0816, train/total_loss: 0.0002, train/total_loss/avg: 0.0816, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 570ms, time_since_start: 04h 43m 15s 575ms, eta: 02h 43m 06s 173ms\n",
      "\u001b[32m2021-05-03T07:35:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0810, train/total_loss: 0.0003, train/total_loss/avg: 0.0810, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 796ms, time_since_start: 04h 45m 02s 372ms, eta: 02h 46m 22s 460ms\n",
      "\u001b[32m2021-05-03T07:37:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0804, train/total_loss: 0.0003, train/total_loss/avg: 0.0804, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 186ms, time_since_start: 04h 46m 46s 558ms, eta: 02h 40m 32s 676ms\n",
      "\u001b[32m2021-05-03T07:39:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T07:39:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T07:39:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T07:40:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T07:40:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0798, train/total_loss: 0.0003, train/total_loss/avg: 0.0798, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 45s 407ms, time_since_start: 04h 49m 31s 965ms, eta: 04h 12m 04s 826ms\n",
      "\u001b[32m2021-05-03T07:40:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T07:40:03 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:40:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:40:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T07:41:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T07:41:31 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T07:42:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T07:43:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T07:43:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.6959, val/total_loss: 2.6959, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5403, val/hateful_memes/roc_auc: 0.7068, num_updates: 13000, epoch: 45, iterations: 13000, max_updates: 22000, val_time: 03m 15s 893ms, best_update: 13000, best_iteration: 13000, best_val/hateful_memes/roc_auc: 0.706834\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:43:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T07:43:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T07:46:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0791, train/total_loss: 0.0003, train/total_loss/avg: 0.0791, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0., ups: 0.62, time: 02m 42s 564ms, time_since_start: 04h 55m 30s 426ms, eta: 04h 04m 59s 760ms\n",
      "\u001b[32m2021-05-03T07:47:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0785, train/total_loss: 0.0003, train/total_loss/avg: 0.0785, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 811ms, time_since_start: 04h 57m 14s 238ms, eta: 02h 34m 41s 607ms\n",
      "\u001b[32m2021-05-03T07:49:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0780, train/total_loss: 0.0003, train/total_loss/avg: 0.0780, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 891ms, time_since_start: 04h 59m 01s 129ms, eta: 02h 37m 28s 345ms\n",
      "\u001b[32m2021-05-03T07:51:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0774, train/total_loss: 0.0002, train/total_loss/avg: 0.0774, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 549ms, time_since_start: 05h 43s 679ms, eta: 02h 29m 20s 386ms\n",
      "\u001b[32m2021-05-03T07:52:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0768, train/total_loss: 0.0002, train/total_loss/avg: 0.0768, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 486ms, time_since_start: 05h 02m 27s 166ms, eta: 02h 28m 57s 137ms\n",
      "\u001b[32m2021-05-03T07:54:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0763, train/total_loss: 0.0003, train/total_loss/avg: 0.0763, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 299ms, time_since_start: 05h 04m 13s 466ms, eta: 02h 31m 12s 001ms\n",
      "\u001b[32m2021-05-03T07:56:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0757, train/total_loss: 0.0003, train/total_loss/avg: 0.0757, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 169ms, time_since_start: 05h 05m 55s 635ms, eta: 02h 23m 35s 718ms\n",
      "\u001b[32m2021-05-03T07:58:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0752, train/total_loss: 0.0003, train/total_loss/avg: 0.0752, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 839ms, time_since_start: 05h 07m 39s 475ms, eta: 02h 24m 11s 111ms\n",
      "\u001b[32m2021-05-03T07:59:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0746, train/total_loss: 0.0003, train/total_loss/avg: 0.0746, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 748ms, time_since_start: 05h 09m 25s 223ms, eta: 02h 25m 02s 665ms\n",
      "\u001b[32m2021-05-03T08:01:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T08:01:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T08:02:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T08:02:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T08:02:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0741, train/total_loss: 0.0002, train/total_loss/avg: 0.0741, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 44s 855ms, time_since_start: 05h 12m 10s 078ms, eta: 03h 43m 19s 424ms\n",
      "\u001b[32m2021-05-03T08:02:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T08:02:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:02:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:02:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:03:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T08:04:06 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T08:04:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T08:04:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 3.1057, val/total_loss: 3.1057, val/hateful_memes/accuracy: 0.6280, val/hateful_memes/binary_f1: 0.4946, val/hateful_memes/roc_auc: 0.6935, num_updates: 14000, epoch: 49, iterations: 14000, max_updates: 22000, val_time: 02m 02s 283ms, best_update: 13000, best_iteration: 13000, best_val/hateful_memes/roc_auc: 0.706834\n",
      "\u001b[32m2021-05-03T08:07:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0736, train/total_loss: 0.0002, train/total_loss/avg: 0.0736, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0., ups: 0.68, time: 02m 27s 511ms, time_since_start: 05h 16m 39s 876ms, eta: 03h 17m 19s 852ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:08:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:08:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:08:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0731, train/total_loss: 0.0002, train/total_loss/avg: 0.0731, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 589ms, time_since_start: 05h 18m 21s 465ms, eta: 02h 14m 10s 779ms\n",
      "\u001b[32m2021-05-03T08:10:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0725, train/total_loss: 0.0003, train/total_loss/avg: 0.0725, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 020ms, time_since_start: 05h 20m 05s 486ms, eta: 02h 15m 37s 746ms\n",
      "\u001b[32m2021-05-03T08:12:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0720, train/total_loss: 0.0003, train/total_loss/avg: 0.0720, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 025ms, time_since_start: 05h 21m 49s 512ms, eta: 02h 13m 52s 423ms\n",
      "\u001b[32m2021-05-03T08:14:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0715, train/total_loss: 0.0003, train/total_loss/avg: 0.0715, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 981ms, time_since_start: 05h 23m 35s 493ms, eta: 02h 14m 35s 780ms\n",
      "\u001b[32m2021-05-03T08:15:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0711, train/total_loss: 0.0003, train/total_loss/avg: 0.0711, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 819ms, time_since_start: 05h 25m 19s 313ms, eta: 02h 10m 05s 595ms\n",
      "\u001b[32m2021-05-03T08:17:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0706, train/total_loss: 0.0002, train/total_loss/avg: 0.0706, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 253ms, time_since_start: 05h 27m 03s 566ms, eta: 02h 08m 52s 281ms\n",
      "\u001b[32m2021-05-03T08:19:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0701, train/total_loss: 0.0002, train/total_loss/avg: 0.0701, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 254ms, time_since_start: 05h 28m 49s 822ms, eta: 02h 09m 32s 767ms\n",
      "\u001b[32m2021-05-03T08:21:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0696, train/total_loss: 0.0002, train/total_loss/avg: 0.0696, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 529ms, time_since_start: 05h 30m 34s 351ms, eta: 02h 05m 40s 336ms\n",
      "\u001b[32m2021-05-03T08:22:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T08:22:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T08:23:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T08:23:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T08:23:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0692, train/total_loss: 0.0002, train/total_loss/avg: 0.0692, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 44s 854ms, time_since_start: 05h 33m 19s 205ms, eta: 03h 15m 24s 438ms\n",
      "\u001b[32m2021-05-03T08:23:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T08:23:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:23:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:23:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:24:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T08:25:19 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T08:25:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T08:25:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 3.3005, val/total_loss: 3.3005, val/hateful_memes/accuracy: 0.6120, val/hateful_memes/binary_f1: 0.4699, val/hateful_memes/roc_auc: 0.6911, num_updates: 15000, epoch: 52, iterations: 15000, max_updates: 22000, val_time: 02m 05s 432ms, best_update: 13000, best_iteration: 13000, best_val/hateful_memes/roc_auc: 0.706834\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:27:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:27:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:28:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0687, train/total_loss: 0.0002, train/total_loss/avg: 0.0687, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0., ups: 0.64, time: 02m 36s 138ms, time_since_start: 05h 38m 779ms, eta: 03h 02m 25s 920ms\n",
      "\u001b[32m2021-05-03T08:30:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0683, train/total_loss: 0.0002, train/total_loss/avg: 0.0683, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 388ms, time_since_start: 05h 39m 45s 167ms, eta: 02h 11s 974ms\n",
      "\u001b[32m2021-05-03T08:32:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0678, train/total_loss: 0.0002, train/total_loss/avg: 0.0678, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 905ms, time_since_start: 05h 41m 29s 072ms, eta: 01h 57m 53s 041ms\n",
      "\u001b[32m2021-05-03T08:33:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0674, train/total_loss: 0.0002, train/total_loss/avg: 0.0674, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 836ms, time_since_start: 05h 43m 13s 908ms, eta: 01h 57m 09s 887ms\n",
      "\u001b[32m2021-05-03T08:35:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0670, train/total_loss: 0.0002, train/total_loss/avg: 0.0670, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 163ms, time_since_start: 05h 44m 58s 071ms, eta: 01h 54m 38s 935ms\n",
      "\u001b[32m2021-05-03T08:37:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0665, train/total_loss: 0.0001, train/total_loss/avg: 0.0665, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 442ms, time_since_start: 05h 46m 42s 514ms, eta: 01h 53m 11s 248ms\n",
      "\u001b[32m2021-05-03T08:39:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0661, train/total_loss: 0.0001, train/total_loss/avg: 0.0661, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 442ms, time_since_start: 05h 48m 28s 956ms, eta: 01h 53m 33s 151ms\n",
      "\u001b[32m2021-05-03T08:40:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0657, train/total_loss: 0.0001, train/total_loss/avg: 0.0657, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 750ms, time_since_start: 05h 50m 12s 707ms, eta: 01h 48m 55s 465ms\n",
      "\u001b[32m2021-05-03T08:42:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0653, train/total_loss: 0.0001, train/total_loss/avg: 0.0653, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0., ups: 0.93, time: 01m 47s 484ms, time_since_start: 05h 52m 191ms, eta: 01h 51m 01s 457ms\n",
      "\u001b[32m2021-05-03T08:44:12 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T08:44:12 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T08:44:36 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T08:45:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T08:45:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0649, train/total_loss: 0.0001, train/total_loss/avg: 0.0649, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0., ups: 0.62, time: 02m 42s 394ms, time_since_start: 05h 54m 42s 586ms, eta: 02h 44m 59s 565ms\n",
      "\u001b[32m2021-05-03T08:45:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T08:45:13 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:45:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:45:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:46:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T08:46:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T08:47:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T08:47:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 3.1617, val/total_loss: 3.1617, val/hateful_memes/accuracy: 0.6200, val/hateful_memes/binary_f1: 0.4865, val/hateful_memes/roc_auc: 0.7015, num_updates: 16000, epoch: 56, iterations: 16000, max_updates: 22000, val_time: 02m 07s 119ms, best_update: 13000, best_iteration: 13000, best_val/hateful_memes/roc_auc: 0.706834\n",
      "\u001b[32m2021-05-03T08:49:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0645, train/total_loss: 0.0001, train/total_loss/avg: 0.0645, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0., ups: 0.67, time: 02m 30s 969ms, time_since_start: 05h 59m 20s 680ms, eta: 02h 30m 49s 746ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:51:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T08:51:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T08:51:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0641, train/total_loss: 0.0001, train/total_loss/avg: 0.0641, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 950ms, time_since_start: 06h 01m 06s 631ms, eta: 01h 44m 03s 463ms\n",
      "\u001b[32m2021-05-03T08:53:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0637, train/total_loss: 0.0001, train/total_loss/avg: 0.0637, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 955ms, time_since_start: 06h 02m 49s 586ms, eta: 01h 39m 22s 363ms\n",
      "\u001b[32m2021-05-03T08:55:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0633, train/total_loss: 0.0001, train/total_loss/avg: 0.0633, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 547ms, time_since_start: 06h 04m 34s 134ms, eta: 01h 39m 08s 327ms\n",
      "\u001b[32m2021-05-03T08:56:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0629, train/total_loss: 0.0001, train/total_loss/avg: 0.0629, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 374ms, time_since_start: 06h 06m 20s 509ms, eta: 01h 39m 04s 235ms\n",
      "\u001b[32m2021-05-03T08:58:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0625, train/total_loss: 0.0001, train/total_loss/avg: 0.0625, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 996ms, time_since_start: 06h 08m 04s 505ms, eta: 01h 35m 05s 680ms\n",
      "\u001b[32m2021-05-03T09:00:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0622, train/total_loss: 0.0001, train/total_loss/avg: 0.0622, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 375ms, time_since_start: 06h 09m 48s 881ms, eta: 01h 33m 40s 432ms\n",
      "\u001b[32m2021-05-03T09:02:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0618, train/total_loss: 0.0001, train/total_loss/avg: 0.0618, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 999ms, time_since_start: 06h 11m 33s 880ms, eta: 01h 32m 27s 311ms\n",
      "\u001b[32m2021-05-03T09:03:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0614, train/total_loss: 0.0001, train/total_loss/avg: 0.0614, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 178ms, time_since_start: 06h 13m 17s 059ms, eta: 01h 29m 06s 286ms\n",
      "\u001b[32m2021-05-03T09:05:32 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T09:05:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T09:05:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T09:06:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T09:06:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0611, train/total_loss: 0.0001, train/total_loss/avg: 0.0611, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 45s 653ms, time_since_start: 06h 16m 02s 712ms, eta: 02h 20m 15s 186ms\n",
      "\u001b[32m2021-05-03T09:06:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T09:06:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:06:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:06:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T09:07:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T09:07:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T09:08:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T09:08:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 3.0364, val/total_loss: 3.0364, val/hateful_memes/accuracy: 0.6240, val/hateful_memes/binary_f1: 0.5000, val/hateful_memes/roc_auc: 0.7022, num_updates: 17000, epoch: 59, iterations: 17000, max_updates: 22000, val_time: 01m 54s 602ms, best_update: 13000, best_iteration: 13000, best_val/hateful_memes/roc_auc: 0.706834\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:10:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:10:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T09:11:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0607, train/total_loss: 0.0001, train/total_loss/avg: 0.0607, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0., ups: 0.65, time: 02m 34s 578ms, time_since_start: 06h 20m 31s 895ms, eta: 02h 08m 15s 525ms\n",
      "\u001b[32m2021-05-03T09:12:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0604, train/total_loss: 0.0000, train/total_loss/avg: 0.0604, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 866ms, time_since_start: 06h 22m 15s 762ms, eta: 01h 24m 25s 368ms\n",
      "\u001b[32m2021-05-03T09:14:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0600, train/total_loss: 0.0000, train/total_loss/avg: 0.0600, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 466ms, time_since_start: 06h 24m 229ms, eta: 01h 23m 08s 479ms\n",
      "\u001b[32m2021-05-03T09:16:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0597, train/total_loss: 0.0000, train/total_loss/avg: 0.0597, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 928ms, time_since_start: 06h 25m 46s 157ms, eta: 01h 22m 30s 693ms\n",
      "\u001b[32m2021-05-03T09:18:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0593, train/total_loss: 0.0000, train/total_loss/avg: 0.0593, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 819ms, time_since_start: 06h 27m 29s 977ms, eta: 01h 19m 06s 644ms\n",
      "\u001b[32m2021-05-03T09:19:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0590, train/total_loss: 0.0000, train/total_loss/avg: 0.0590, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 729ms, time_since_start: 06h 29m 13s 707ms, eta: 01h 17m 17s 115ms\n",
      "\u001b[32m2021-05-03T09:25:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0587, train/total_loss: 0.0000, train/total_loss/avg: 0.0587, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0., ups: 0.30, time: 05m 32s 933ms, time_since_start: 06h 34m 46s 640ms, eta: 04h 02m 25s 201ms\n",
      "\u001b[32m2021-05-03T09:27:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0583, train/total_loss: 0.0000, train/total_loss/avg: 0.0583, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 351ms, time_since_start: 06h 36m 29s 992ms, eta: 01h 13m 30s 205ms\n",
      "\u001b[32m2021-05-03T09:28:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0580, train/total_loss: 0.0000, train/total_loss/avg: 0.0580, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 202ms, time_since_start: 06h 38m 14s 194ms, eta: 01h 12m 20s 665ms\n",
      "\u001b[32m2021-05-03T09:30:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T09:30:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T09:34:12 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T09:35:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T09:35:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0577, train/total_loss: 0.0000, train/total_loss/avg: 0.0577, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0., ups: 0.25, time: 06m 39s 964ms, time_since_start: 06h 44m 54s 159ms, eta: 04h 30m 54s 548ms\n",
      "\u001b[32m2021-05-03T09:35:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T09:35:25 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:35:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:35:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T09:36:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 3.2452, val/total_loss: 3.2452, val/hateful_memes/accuracy: 0.6320, val/hateful_memes/binary_f1: 0.5080, val/hateful_memes/roc_auc: 0.6988, num_updates: 18000, epoch: 63, iterations: 18000, max_updates: 22000, val_time: 01m 07s 630ms, best_update: 13000, best_iteration: 13000, best_val/hateful_memes/roc_auc: 0.706834\n",
      "\u001b[32m2021-05-03T09:38:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0574, train/total_loss: 0.0000, train/total_loss/avg: 0.0574, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 514ms, time_since_start: 06h 47m 46s 307ms, eta: 01h 09m 01s 293ms\n",
      "\u001b[32m2021-05-03T09:40:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0570, train/total_loss: 0.0000, train/total_loss/avg: 0.0570, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 553ms, time_since_start: 06h 49m 29s 861ms, eta: 01h 06m 38s 003ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:40:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:40:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T09:41:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0567, train/total_loss: 0.0000, train/total_loss/avg: 0.0567, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 451ms, time_since_start: 06h 51m 14s 313ms, eta: 01h 05m 26s 558ms\n",
      "\u001b[32m2021-05-03T09:43:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0564, train/total_loss: 0.0000, train/total_loss/avg: 0.0564, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 115ms, time_since_start: 06h 52m 58s 429ms, eta: 01h 03m 28s 142ms\n",
      "\u001b[32m2021-05-03T09:45:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0561, train/total_loss: 0.0000, train/total_loss/avg: 0.0561, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0., ups: 0.93, time: 01m 47s 363ms, time_since_start: 06h 54m 45s 792ms, eta: 01h 03m 37s 845ms\n",
      "\u001b[32m2021-05-03T09:46:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0558, train/total_loss: 0.0000, train/total_loss/avg: 0.0558, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 502ms, time_since_start: 06h 56m 28s 295ms, eta: 59m 848ms\n",
      "\u001b[32m2021-05-03T09:48:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0555, train/total_loss: 0.0000, train/total_loss/avg: 0.0555, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 578ms, time_since_start: 06h 58m 11s 873ms, eta: 57m 52s 772ms\n",
      "\u001b[32m2021-05-03T09:50:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0552, train/total_loss: 0.0000, train/total_loss/avg: 0.0552, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 496ms, time_since_start: 06h 59m 58s 370ms, eta: 57m 42s 411ms\n",
      "\u001b[32m2021-05-03T09:52:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0549, train/total_loss: 0.0000, train/total_loss/avg: 0.0549, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 717ms, time_since_start: 07h 01m 40s 087ms, eta: 53m 23s 699ms\n",
      "\u001b[32m2021-05-03T09:53:55 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T09:53:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T09:54:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T09:54:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T09:54:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0546, train/total_loss: 0.0000, train/total_loss/avg: 0.0546, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 45s 272ms, time_since_start: 07h 04m 25s 360ms, eta: 01h 23m 57s 512ms\n",
      "\u001b[32m2021-05-03T09:54:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T09:54:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:54:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:54:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T09:56:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.2540, val/total_loss: 3.2540, val/hateful_memes/accuracy: 0.6300, val/hateful_memes/binary_f1: 0.5093, val/hateful_memes/roc_auc: 0.7001, num_updates: 19000, epoch: 66, iterations: 19000, max_updates: 22000, val_time: 01m 07s 286ms, best_update: 13000, best_iteration: 13000, best_val/hateful_memes/roc_auc: 0.706834\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:57:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T09:57:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T09:57:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0544, train/total_loss: 0.0000, train/total_loss/avg: 0.0544, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 229ms, time_since_start: 07h 07m 18s 879ms, eta: 52m 09s 951ms\n",
      "\u001b[32m2021-05-03T09:59:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0541, train/total_loss: 0.0000, train/total_loss/avg: 0.0541, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 099ms, time_since_start: 07h 09m 02s 978ms, eta: 49m 21s 415ms\n",
      "\u001b[32m2021-05-03T10:01:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0538, train/total_loss: 0.0000, train/total_loss/avg: 0.0538, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 286ms, time_since_start: 07h 10m 47s 264ms, eta: 47m 40s 776ms\n",
      "\u001b[32m2021-05-03T10:03:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0535, train/total_loss: 0.0000, train/total_loss/avg: 0.0535, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 938ms, time_since_start: 07h 12m 32s 203ms, eta: 46m 12s 065ms\n",
      "\u001b[32m2021-05-03T10:04:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0532, train/total_loss: 0.0000, train/total_loss/avg: 0.0532, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 078ms, time_since_start: 07h 14m 15s 281ms, eta: 43m 38s 193ms\n",
      "\u001b[32m2021-05-03T10:06:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0530, train/total_loss: 0.0000, train/total_loss/avg: 0.0530, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 875ms, time_since_start: 07h 15m 59s 157ms, eta: 42m 12s 893ms\n",
      "\u001b[32m2021-05-03T10:08:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0527, train/total_loss: 0.0000, train/total_loss/avg: 0.0527, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 873ms, time_since_start: 07h 17m 44s 030ms, eta: 40m 50s 687ms\n",
      "\u001b[32m2021-05-03T10:09:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0524, train/total_loss: 0.0000, train/total_loss/avg: 0.0524, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 134ms, time_since_start: 07h 19m 28s 165ms, eta: 38m 47s 618ms\n",
      "\u001b[32m2021-05-03T10:11:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0522, train/total_loss: 0.0000, train/total_loss/avg: 0.0522, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 017ms, time_since_start: 07h 21m 12s 183ms, eta: 36m 59s 319ms\n",
      "\u001b[32m2021-05-03T10:13:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T10:13:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T10:13:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T10:14:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T10:14:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0519, train/total_loss: 0.0000, train/total_loss/avg: 0.0519, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0., ups: 0.60, time: 02m 46s 979ms, time_since_start: 07h 23m 59s 162ms, eta: 56m 33s 028ms\n",
      "\u001b[32m2021-05-03T10:14:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T10:14:30 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:14:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:14:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T10:15:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 3.2779, val/total_loss: 3.2779, val/hateful_memes/accuracy: 0.6240, val/hateful_memes/binary_f1: 0.5079, val/hateful_memes/roc_auc: 0.6970, num_updates: 20000, epoch: 70, iterations: 20000, max_updates: 22000, val_time: 01m 05s 644ms, best_update: 13000, best_iteration: 13000, best_val/hateful_memes/roc_auc: 0.706834\n",
      "\u001b[32m2021-05-03T10:17:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0517, train/total_loss: 0.0000, train/total_loss/avg: 0.0517, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.01, time: 01m 39s 065ms, time_since_start: 07h 26m 43s 878ms, eta: 31m 52s 356ms\n",
      "\u001b[32m2021-05-03T10:18:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0514, train/total_loss: 0.0000, train/total_loss/avg: 0.0514, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.00, time: 01m 40s 970ms, time_since_start: 07h 28m 24s 848ms, eta: 30m 46s 555ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:19:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:19:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T10:20:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0511, train/total_loss: 0.0000, train/total_loss/avg: 0.0511, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 068ms, time_since_start: 07h 30m 09s 917ms, eta: 30m 14s 745ms\n",
      "\u001b[32m2021-05-03T10:22:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0509, train/total_loss: 0.0000, train/total_loss/avg: 0.0509, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 864ms, time_since_start: 07h 31m 53s 781ms, eta: 28m 08s 416ms\n",
      "\u001b[32m2021-05-03T10:24:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0506, train/total_loss: 0.0000, train/total_loss/avg: 0.0506, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 494ms, time_since_start: 07h 33m 37s 276ms, eta: 26m 17s 254ms\n",
      "\u001b[32m2021-05-03T10:25:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0504, train/total_loss: 0.0000, train/total_loss/avg: 0.0504, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 738ms, time_since_start: 07h 35m 22s 014ms, eta: 24m 49s 798ms\n",
      "\u001b[32m2021-05-03T10:27:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0502, train/total_loss: 0.0001, train/total_loss/avg: 0.0502, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 320ms, time_since_start: 07h 37m 06s 334ms, eta: 22m 57s 859ms\n",
      "\u001b[32m2021-05-03T10:29:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0499, train/total_loss: 0.0001, train/total_loss/avg: 0.0499, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 986ms, time_since_start: 07h 38m 50s 321ms, eta: 21m 07s 801ms\n",
      "\u001b[32m2021-05-03T10:31:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0497, train/total_loss: 0.0001, train/total_loss/avg: 0.0497, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 615ms, time_since_start: 07h 40m 34s 936ms, eta: 19m 29s 178ms\n",
      "\u001b[32m2021-05-03T10:32:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T10:32:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T10:33:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T10:33:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T10:33:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0494, train/total_loss: 0.0001, train/total_loss/avg: 0.0494, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 45s 012ms, time_since_start: 07h 43m 19s 948ms, eta: 27m 56s 523ms\n",
      "\u001b[32m2021-05-03T10:33:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T10:33:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:33:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:33:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T10:34:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 3.4450, val/total_loss: 3.4450, val/hateful_memes/accuracy: 0.6180, val/hateful_memes/binary_f1: 0.4796, val/hateful_memes/roc_auc: 0.6959, num_updates: 21000, epoch: 73, iterations: 21000, max_updates: 22000, val_time: 01m 06s 861ms, best_update: 13000, best_iteration: 13000, best_val/hateful_memes/roc_auc: 0.706834\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:36:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:36:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T10:36:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0492, train/total_loss: 0.0001, train/total_loss/avg: 0.0492, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 0.93, time: 01m 48s 380ms, time_since_start: 07h 46m 15s 195ms, eta: 16m 31s 032ms\n",
      "\u001b[32m2021-05-03T10:38:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0490, train/total_loss: 0.0001, train/total_loss/avg: 0.0490, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 446ms, time_since_start: 07h 47m 57s 642ms, eta: 13m 52s 686ms\n",
      "\u001b[32m2021-05-03T10:40:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0487, train/total_loss: 0.0001, train/total_loss/avg: 0.0487, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 504ms, time_since_start: 07h 49m 42s 147ms, eta: 12m 23s 239ms\n",
      "\u001b[32m2021-05-03T10:42:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0485, train/total_loss: 0.0001, train/total_loss/avg: 0.0485, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 0.93, time: 01m 47s 028ms, time_since_start: 07h 51m 29s 176ms, eta: 10m 52s 445ms\n",
      "\u001b[32m2021-05-03T10:43:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0483, train/total_loss: 0.0000, train/total_loss/avg: 0.0483, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 926ms, time_since_start: 07h 53m 12s 102ms, eta: 08m 42s 865ms\n",
      "\u001b[32m2021-05-03T10:45:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0481, train/total_loss: 0.0000, train/total_loss/avg: 0.0481, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 829ms, time_since_start: 07h 54m 55s 931ms, eta: 07m 01s 963ms\n",
      "\u001b[32m2021-05-03T10:47:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0479, train/total_loss: 0.0000, train/total_loss/avg: 0.0479, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 618ms, time_since_start: 07h 56m 42s 550ms, eta: 05m 24s 972ms\n",
      "\u001b[32m2021-05-03T10:48:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0476, train/total_loss: 0.0000, train/total_loss/avg: 0.0476, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 106ms, time_since_start: 07h 58m 25s 656ms, eta: 03m 29s 512ms\n",
      "\u001b[32m2021-05-03T10:50:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0474, train/total_loss: 0.0000, train/total_loss/avg: 0.0474, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 428ms, time_since_start: 08h 10s 085ms, eta: 01m 46s 099ms\n",
      "\u001b[32m2021-05-03T10:52:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T10:52:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T10:52:50 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T10:53:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T10:53:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0472, train/total_loss: 0.0000, train/total_loss/avg: 0.0472, max mem: 10794.0, experiment: run, epoch: 77, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 0.60, time: 02m 47s 739ms, time_since_start: 08h 02m 57s 825ms, eta: 0ms\n",
      "\u001b[32m2021-05-03T10:53:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T10:53:28 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:53:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:53:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T10:54:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 3.4492, val/total_loss: 3.4492, val/hateful_memes/accuracy: 0.6220, val/hateful_memes/binary_f1: 0.4878, val/hateful_memes/roc_auc: 0.6967, num_updates: 22000, epoch: 77, iterations: 22000, max_updates: 22000, val_time: 01m 03s 771ms, best_update: 13000, best_iteration: 13000, best_val/hateful_memes/roc_auc: 0.706834\n",
      "\u001b[32m2021-05-03T10:54:33 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2021-05-03T10:54:33 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2021-05-03T10:54:33 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2021-05-03T10:55:03 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-03T10:55:03 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 13000\n",
      "\u001b[32m2021-05-03T10:55:03 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 13000\n",
      "\u001b[32m2021-05-03T10:55:03 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 45\n",
      "\u001b[32m2021-05-03T10:55:08 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-03T10:55:08 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:55:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T10:55:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "100% 16/16 [00:17<00:00,  1.10s/it]\n",
      "\u001b[32m2021-05-03T10:55:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 2.6959, val/total_loss: 2.6959, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5403, val/hateful_memes/roc_auc: 0.7068\n",
      "\u001b[32m2021-05-03T10:55:26 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 08h 04m 55s 105ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/vilbert/from_cc.yaml \\\n",
    "  model=vilbert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=train_val \\\n",
    "  training.batch_size=32 \\\n",
    "  env.save_dir=/content/gdrive/MyDrive/colab/pretrained_vilbertcc_election_memes/ \\\n",
    "  checkpoint.resume_zoo=vilbert.pretrained.cc.original \\\n",
    "  checkpoint.resume_pretrained=True \\\n",
    "  dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfWuZ-qSgmCB"
   },
   "source": [
    "Training on finetuned VilBERT CC (hateful + election memes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 930141,
     "status": "ok",
     "timestamp": 1620108780470,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "zAI7oZLchkmJ",
    "outputId": "4ec56074-4475-4660-bb28-45452ce4056c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-03 22:00:23.376068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-03T22:00:58 | matplotlib.font_manager: \u001b[0mGenerating new fontManager, this may take some time...\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/from_cc.yaml\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/gdrive/MyDrive/colab/finetuned_vilbertcc_election_memes/\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.from_cc_original\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to /content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_seen.jsonl\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.train[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.val[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.test[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf: \u001b[0mLogging to: /content/gdrive/MyDrive/colab/finetuned_vilbertcc_election_memes/train.log\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=train_val', 'training.batch_size=32', 'env.save_dir=/content/gdrive/MyDrive/colab/finetuned_vilbertcc_election_memes/', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original', 'checkpoint.resume_pretrained=True', 'dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl', 'dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb'])\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf_cli.run: \u001b[0mUsing seed 18142900\n",
      "\u001b[32m2021-05-03T22:01:18 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100% 10.3G/10.3G [02:52<00:00, 59.5MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n",
      "Downloading extras.tar.gz: 100% 211k/211k [00:00<00:00, 362kB/s]  \n",
      "[ Starting checksum for extras.tar.gz]\n",
      "[ Checksum successful for extras.tar.gz]\n",
      "Unpacking extras.tar.gz\n",
      "\u001b[32m2021-05-03T22:10:06 | filelock: \u001b[0mLock 140015340071760 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "Downloading: 100% 433/433 [00:00<00:00, 250kB/s]\n",
      "\u001b[32m2021-05-03T22:10:06 | filelock: \u001b[0mLock 140015340071760 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "\u001b[32m2021-05-03T22:10:07 | filelock: \u001b[0mLock 140015369505424 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 863kB/s]\n",
      "\u001b[32m2021-05-03T22:10:07 | filelock: \u001b[0mLock 140015369505424 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:10:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:10:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T22:10:07 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T22:10:07 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T22:10:07 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-03T22:10:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2021-05-03T22:10:07 | filelock: \u001b[0mLock 140015334297296 acquired on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Downloading: 100% 440M/440M [00:07<00:00, 62.1MB/s]\n",
      "\u001b[32m2021-05-03T22:10:15 | filelock: \u001b[0mLock 140015334297296 released on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-03T22:10:25 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-03T22:10:25 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:10:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:10:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-03T22:10:25 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_from_cc.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.finetuned.hateful_memes.from_cc_original/vilbert.finetuned.hateful_memes_from_cc.tar.gz ]\n",
      "Downloading vilbert.finetuned.hateful_memes_from_cc.tar.gz: 100% 918M/918M [00:18<00:00, 48.6MB/s]\n",
      "[ Starting checksum for vilbert.finetuned.hateful_memes_from_cc.tar.gz]\n",
      "[ Checksum successful for vilbert.finetuned.hateful_memes_from_cc.tar.gz]\n",
      "Unpacking vilbert.finetuned.hateful_memes_from_cc.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:10:57 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:10:57 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:10:57 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:10:57 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:57 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
      "  (model): ViLBERTForClassification(\n",
      "    (bert): ViLBERTBase(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (v_embeddings): BertImageFeatureEmbeddings(\n",
      "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (v_layer): ModuleList(\n",
      "          (0): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (c_layer): ModuleList(\n",
      "          (0): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (t_pooler): BertTextPooler(\n",
      "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (v_pooler): BertImagePooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
      "\u001b[32m2021-05-03T22:10:58 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2021-05-03T22:16:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.5326, train/hateful_memes/cross_entropy/avg: 0.5326, train/total_loss: 0.5326, train/total_loss/avg: 0.5326, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 0.31, time: 05m 27s 726ms, time_since_start: 06m 481ms, eta: 20h 15m 20s 453ms\n",
      "\u001b[32m2021-05-03T22:18:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.2554, train/hateful_memes/cross_entropy/avg: 0.3940, train/total_loss: 0.2554, train/total_loss/avg: 0.3940, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 806ms, time_since_start: 07m 37s 288ms, eta: 05h 57m 21s 586ms\n",
      "\u001b[32m2021-05-03T22:19:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.4971, train/hateful_memes/cross_entropy/avg: 0.4284, train/total_loss: 0.4971, train/total_loss/avg: 0.4284, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 741ms, time_since_start: 09m 22s 029ms, eta: 06h 24m 52s 499ms\n",
      "\u001b[32m2021-05-03T22:21:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.3485, train/hateful_memes/cross_entropy/avg: 0.4084, train/total_loss: 0.3485, train/total_loss/avg: 0.4084, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 358ms, time_since_start: 11m 387ms, eta: 05h 59m 45s 291ms\n",
      "\u001b[32m2021-05-03T22:23:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.3485, train/hateful_memes/cross_entropy/avg: 0.3946, train/total_loss: 0.3485, train/total_loss/avg: 0.3946, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 394ms, time_since_start: 12m 44s 782ms, eta: 06h 20m 03s 945ms\n",
      "\u001b[32m2021-05-03T22:24:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.3392, train/hateful_memes/cross_entropy/avg: 0.3738, train/total_loss: 0.3392, train/total_loss/avg: 0.3738, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 286ms, time_since_start: 14m 31s 069ms, eta: 06h 25m 09s 335ms\n",
      "\u001b[32m2021-05-03T22:26:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.3392, train/hateful_memes/cross_entropy/avg: 0.3521, train/total_loss: 0.3392, train/total_loss/avg: 0.3521, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 365ms, time_since_start: 16m 14s 434ms, eta: 06h 12m 49s 063ms\n",
      "\u001b[32m2021-05-03T22:28:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.3392, train/hateful_memes/cross_entropy/avg: 0.3584, train/total_loss: 0.3392, train/total_loss/avg: 0.3584, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 756ms, time_since_start: 17m 58s 191ms, eta: 06h 12m 28s 244ms\n",
      "\u001b[32m2021-05-03T22:30:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.3392, train/hateful_memes/cross_entropy/avg: 0.3412, train/total_loss: 0.3392, train/total_loss/avg: 0.3412, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 054ms, time_since_start: 19m 44s 245ms, eta: 06h 18m 55s 594ms\n",
      "\u001b[32m2021-05-03T22:31:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T22:31:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T22:36:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T22:37:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T22:37:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.2700, train/hateful_memes/cross_entropy/avg: 0.3309, train/total_loss: 0.2700, train/total_loss/avg: 0.3309, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 0.23, time: 07m 21s 137ms, time_since_start: 27m 05s 383ms, eta: 26h 08m 41s 076ms\n",
      "\u001b[32m2021-05-03T22:37:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T22:37:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:37:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:37:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T22:38:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T22:38:57 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T22:40:09 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T22:40:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T22:40:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 0.9980, val/total_loss: 0.9980, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.5882, val/hateful_memes/roc_auc: 0.7067, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 03m 14s 820ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.706742\n",
      "\u001b[32m2021-05-03T22:43:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.3063, train/hateful_memes/cross_entropy/avg: 0.3286, train/total_loss: 0.3063, train/total_loss/avg: 0.3286, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 0.64, time: 02m 37s 154ms, time_since_start: 32m 57s 363ms, eta: 09h 16m 10s 785ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:44:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T22:44:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T22:45:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.2700, train/hateful_memes/cross_entropy/avg: 0.3137, train/total_loss: 0.2700, train/total_loss/avg: 0.3137, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 473ms, time_since_start: 34m 42s 837ms, eta: 06h 11m 29s 506ms\n",
      "\u001b[32m2021-05-03T22:46:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.2700, train/hateful_memes/cross_entropy/avg: 0.2971, train/total_loss: 0.2700, train/total_loss/avg: 0.2971, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 752ms, time_since_start: 36m 26s 589ms, eta: 06h 03m 40s 324ms\n",
      "\u001b[32m2021-05-03T22:48:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.2554, train/hateful_memes/cross_entropy/avg: 0.2825, train/total_loss: 0.2554, train/total_loss/avg: 0.2825, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 636ms, time_since_start: 38m 10s 225ms, eta: 06h 01m 30s 621ms\n",
      "\u001b[32m2021-05-03T22:50:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.2698, train/hateful_memes/cross_entropy/avg: 0.2817, train/total_loss: 0.2698, train/total_loss/avg: 0.2817, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 377ms, time_since_start: 39m 54s 603ms, eta: 06h 02m 19s 743ms\n",
      "\u001b[32m2021-05-03T22:52:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.2554, train/hateful_memes/cross_entropy/avg: 0.2736, train/total_loss: 0.2554, train/total_loss/avg: 0.2736, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 791ms, time_since_start: 41m 38s 395ms, eta: 05h 58m 32s 352ms\n",
      "\u001b[32m2021-05-03T22:53:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.2554, train/hateful_memes/cross_entropy/avg: 0.2713, train/total_loss: 0.2554, train/total_loss/avg: 0.2713, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 110ms, time_since_start: 43m 22s 505ms, eta: 05h 57m 52s 519ms\n",
      "\u001b[32m2021-05-03T22:55:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.2382, train/hateful_memes/cross_entropy/avg: 0.2659, train/total_loss: 0.2382, train/total_loss/avg: 0.2659, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 777ms, time_since_start: 45m 08s 283ms, eta: 06h 01m 49s 008ms\n",
      "\u001b[32m2021-05-03T22:57:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.2382, train/hateful_memes/cross_entropy/avg: 0.2562, train/total_loss: 0.2382, train/total_loss/avg: 0.2562, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 036ms, time_since_start: 46m 52s 319ms, eta: 05h 54m 05s 843ms\n",
      "\u001b[32m2021-05-03T22:59:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T22:59:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:00:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:00:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:00:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.2353, train/hateful_memes/cross_entropy/avg: 0.2515, train/total_loss: 0.2353, train/total_loss/avg: 0.2515, max mem: 10794.0, experiment: run, epoch: 7, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 0.50, time: 03m 19s 368ms, time_since_start: 50m 11s 687ms, eta: 11h 15m 11s 629ms\n",
      "\u001b[32m2021-05-03T23:00:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T23:00:37 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:00:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:00:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:01:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:02:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:02:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:02:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 2.0026, val/total_loss: 2.0026, val/hateful_memes/accuracy: 0.6020, val/hateful_memes/binary_f1: 0.4363, val/hateful_memes/roc_auc: 0.7055, num_updates: 2000, epoch: 7, iterations: 2000, max_updates: 22000, val_time: 02m 04s 335ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.706742\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:03:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:03:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:05:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.2217, train/hateful_memes/cross_entropy/avg: 0.2460, train/total_loss: 0.2217, train/total_loss/avg: 0.2460, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 0.67, time: 02m 30s 116ms, time_since_start: 54m 46s 143ms, eta: 08h 25m 51s 203ms\n",
      "\u001b[32m2021-05-03T23:06:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.2030, train/hateful_memes/cross_entropy/avg: 0.2365, train/total_loss: 0.2030, train/total_loss/avg: 0.2365, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 759ms, time_since_start: 56m 29s 903ms, eta: 05h 47m 53s 149ms\n",
      "\u001b[32m2021-05-03T23:08:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.1732, train/hateful_memes/cross_entropy/avg: 0.2285, train/total_loss: 0.1732, train/total_loss/avg: 0.2285, max mem: 10794.0, experiment: run, epoch: 8, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 002ms, time_since_start: 58m 13s 905ms, eta: 05h 46m 56s 337ms\n",
      "\u001b[32m2021-05-03T23:10:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.1626, train/hateful_memes/cross_entropy/avg: 0.2206, train/total_loss: 0.1626, train/total_loss/avg: 0.2206, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 227ms, time_since_start: 59m 58s 133ms, eta: 05h 45m 55s 414ms\n",
      "\u001b[32m2021-05-03T23:12:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.1515, train/hateful_memes/cross_entropy/avg: 0.2123, train/total_loss: 0.1515, train/total_loss/avg: 0.2123, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 881ms, time_since_start: 01h 01m 42s 014ms, eta: 05h 43m 01s 035ms\n",
      "\u001b[32m2021-05-03T23:13:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.1490, train/hateful_memes/cross_entropy/avg: 0.2081, train/total_loss: 0.1490, train/total_loss/avg: 0.2081, max mem: 10794.0, experiment: run, epoch: 9, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 051ms, time_since_start: 01h 03m 25s 066ms, eta: 05h 38m 31s 778ms\n",
      "\u001b[32m2021-05-03T23:15:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.1353, train/hateful_memes/cross_entropy/avg: 0.2007, train/total_loss: 0.1353, train/total_loss/avg: 0.2007, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 113ms, time_since_start: 01h 05m 11s 179ms, eta: 05h 46m 47s 652ms\n",
      "\u001b[32m2021-05-03T23:17:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.1038, train/hateful_memes/cross_entropy/avg: 0.1966, train/total_loss: 0.1038, train/total_loss/avg: 0.1966, max mem: 10794.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 997ms, time_since_start: 01h 06m 55s 177ms, eta: 05h 38m 07s 026ms\n",
      "\u001b[32m2021-05-03T23:19:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.0982, train/hateful_memes/cross_entropy/avg: 0.1913, train/total_loss: 0.0982, train/total_loss/avg: 0.1913, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 957ms, time_since_start: 01h 08m 42s 134ms, eta: 05h 45m 55s 663ms\n",
      "\u001b[32m2021-05-03T23:20:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T23:20:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:25:07 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:25:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:25:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.0936, train/hateful_memes/cross_entropy/avg: 0.1852, train/total_loss: 0.0936, train/total_loss/avg: 0.1852, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 0.25, time: 06m 35s 251ms, time_since_start: 01h 15m 17s 386ms, eta: 21h 11m 39s 345ms\n",
      "\u001b[32m2021-05-03T23:25:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T23:25:43 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:25:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:25:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:26:47 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:27:12 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T23:27:48 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:28:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:28:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 2.1478, val/total_loss: 2.1478, val/hateful_memes/accuracy: 0.6480, val/hateful_memes/binary_f1: 0.5319, val/hateful_memes/roc_auc: 0.7082, num_updates: 3000, epoch: 11, iterations: 3000, max_updates: 22000, val_time: 02m 40s 955ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.708214\n",
      "\u001b[32m2021-05-03T23:31:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.0879, train/hateful_memes/cross_entropy/avg: 0.1794, train/total_loss: 0.0879, train/total_loss/avg: 0.1794, max mem: 10794.0, experiment: run, epoch: 11, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 0.59, time: 02m 49s 157ms, time_since_start: 01h 20m 47s 503ms, eta: 09h 01m 22s 375ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:32:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:32:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:32:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.0827, train/hateful_memes/cross_entropy/avg: 0.1752, train/total_loss: 0.0827, train/total_loss/avg: 0.1752, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 0.99, time: 01m 41s 793ms, time_since_start: 01h 22m 29s 297ms, eta: 05h 24m 03s 464ms\n",
      "\u001b[32m2021-05-03T23:34:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0519, train/hateful_memes/cross_entropy/avg: 0.1706, train/total_loss: 0.0519, train/total_loss/avg: 0.1706, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 260ms, time_since_start: 01h 24m 11s 557ms, eta: 05h 23m 48s 658ms\n",
      "\u001b[32m2021-05-03T23:36:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0447, train/hateful_memes/cross_entropy/avg: 0.1668, train/total_loss: 0.0447, train/total_loss/avg: 0.1668, max mem: 10794.0, experiment: run, epoch: 12, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 893ms, time_since_start: 01h 25m 56s 451ms, eta: 05h 30m 22s 447ms\n",
      "\u001b[32m2021-05-03T23:38:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0418, train/hateful_memes/cross_entropy/avg: 0.1622, train/total_loss: 0.0418, train/total_loss/avg: 0.1622, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 467ms, time_since_start: 01h 27m 41s 918ms, eta: 05h 30m 23s 631ms\n",
      "\u001b[32m2021-05-03T23:39:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0415, train/hateful_memes/cross_entropy/avg: 0.1589, train/total_loss: 0.0415, train/total_loss/avg: 0.1589, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 800ms, time_since_start: 01h 29m 24s 719ms, eta: 05h 20m 17s 889ms\n",
      "\u001b[32m2021-05-03T23:41:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0410, train/hateful_memes/cross_entropy/avg: 0.1549, train/total_loss: 0.0410, train/total_loss/avg: 0.1549, max mem: 10794.0, experiment: run, epoch: 13, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 464ms, time_since_start: 01h 31m 09s 184ms, eta: 05h 23m 42s 965ms\n",
      "\u001b[32m2021-05-03T23:43:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0391, train/hateful_memes/cross_entropy/avg: 0.1508, train/total_loss: 0.0391, train/total_loss/avg: 0.1508, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 976ms, time_since_start: 01h 32m 54s 160ms, eta: 05h 23m 31s 414ms\n",
      "\u001b[32m2021-05-03T23:45:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0378, train/hateful_memes/cross_entropy/avg: 0.1471, train/total_loss: 0.0378, train/total_loss/avg: 0.1471, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 047ms, time_since_start: 01h 34m 38s 208ms, eta: 05h 18m 53s 973ms\n",
      "\u001b[32m2021-05-03T23:46:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-03T23:46:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:47:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:47:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:47:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0224, train/hateful_memes/cross_entropy/avg: 0.1435, train/total_loss: 0.0224, train/total_loss/avg: 0.1435, max mem: 10794.0, experiment: run, epoch: 14, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 46s 334ms, time_since_start: 01h 37m 24s 543ms, eta: 08h 26m 59s 326ms\n",
      "\u001b[32m2021-05-03T23:47:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-03T23:47:50 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:47:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:47:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:48:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-03T23:49:23 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-03T23:49:59 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-03T23:50:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-03T23:50:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 1.9899, val/total_loss: 1.9899, val/hateful_memes/accuracy: 0.6620, val/hateful_memes/binary_f1: 0.5947, val/hateful_memes/roc_auc: 0.7270, num_updates: 4000, epoch: 14, iterations: 4000, max_updates: 22000, val_time: 02m 45s 029ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.727013\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:52:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-03T23:52:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-03T23:53:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0169, train/hateful_memes/cross_entropy/avg: 0.1404, train/total_loss: 0.0169, train/total_loss/avg: 0.1404, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 0.59, time: 02m 50s 923ms, time_since_start: 01h 43m 500ms, eta: 08h 38m 04s 752ms\n",
      "\u001b[32m2021-05-03T23:55:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0125, train/hateful_memes/cross_entropy/avg: 0.1373, train/total_loss: 0.0125, train/total_loss/avg: 0.1373, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 358ms, time_since_start: 01h 44m 44s 859ms, eta: 05h 14m 33s 043ms\n",
      "\u001b[32m2021-05-03T23:56:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0119, train/hateful_memes/cross_entropy/avg: 0.1342, train/total_loss: 0.0119, train/total_loss/avg: 0.1342, max mem: 10794.0, experiment: run, epoch: 15, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 182ms, time_since_start: 01h 46m 29s 041ms, eta: 05h 12m 15s 334ms\n",
      "\u001b[32m2021-05-03T23:58:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0119, train/hateful_memes/cross_entropy/avg: 0.1321, train/total_loss: 0.0119, train/total_loss/avg: 0.1321, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 203ms, time_since_start: 01h 48m 12s 244ms, eta: 05h 07m 34s 359ms\n",
      "\u001b[32m2021-05-04T00:00:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0119, train/hateful_memes/cross_entropy/avg: 0.1295, train/total_loss: 0.0119, train/total_loss/avg: 0.1295, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 700ms, time_since_start: 01h 49m 55s 945ms, eta: 05h 07m 17s 996ms\n",
      "\u001b[32m2021-05-04T00:02:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0119, train/hateful_memes/cross_entropy/avg: 0.1271, train/total_loss: 0.0119, train/total_loss/avg: 0.1271, max mem: 10794.0, experiment: run, epoch: 16, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 896ms, time_since_start: 01h 51m 39s 842ms, eta: 05h 06m 07s 228ms\n",
      "\u001b[32m2021-05-04T00:03:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0119, train/hateful_memes/cross_entropy/avg: 0.1244, train/total_loss: 0.0119, train/total_loss/avg: 0.1244, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 799ms, time_since_start: 01h 53m 24s 642ms, eta: 05h 07m 489ms\n",
      "\u001b[32m2021-05-04T00:05:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.1218, train/total_loss: 0.0095, train/total_loss/avg: 0.1218, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 503ms, time_since_start: 01h 55m 09s 145ms, eta: 05h 04m 22s 235ms\n",
      "\u001b[32m2021-05-04T00:07:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0091, train/hateful_memes/cross_entropy/avg: 0.1194, train/total_loss: 0.0091, train/total_loss/avg: 0.1194, max mem: 10794.0, experiment: run, epoch: 17, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 970ms, time_since_start: 01h 56m 53s 116ms, eta: 05h 01m 03s 485ms\n",
      "\u001b[32m2021-05-04T00:09:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T00:09:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:09:27 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:10:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:10:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.1179, train/total_loss: 0.0095, train/total_loss/avg: 0.1179, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 46s 390ms, time_since_start: 01h 59m 39s 507ms, eta: 07h 58m 58s 975ms\n",
      "\u001b[32m2021-05-04T00:10:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T00:10:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:10:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:10:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:11:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:11:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:12:11 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:12:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 2.5281, val/total_loss: 2.5281, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5627, val/hateful_memes/roc_auc: 0.7207, num_updates: 5000, epoch: 18, iterations: 5000, max_updates: 22000, val_time: 02m 06s 691ms, best_update: 4000, best_iteration: 4000, best_val/hateful_memes/roc_auc: 0.727013\n",
      "\u001b[32m2021-05-04T00:14:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.1156, train/total_loss: 0.0095, train/total_loss/avg: 0.1156, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 0.67, time: 02m 29s 699ms, time_since_start: 02h 04m 15s 905ms, eta: 07h 08m 23s 978ms\n",
      "\u001b[32m2021-05-04T00:16:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.1148, train/total_loss: 0.0095, train/total_loss/avg: 0.1148, max mem: 10794.0, experiment: run, epoch: 18, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 1.03, time: 01m 37s 014ms, time_since_start: 02h 05m 52s 919ms, eta: 04h 35m 59s 178ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:16:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:16:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:18:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0095, train/hateful_memes/cross_entropy/avg: 0.1131, train/total_loss: 0.0095, train/total_loss/avg: 0.1131, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 167ms, time_since_start: 02h 07m 37s 086ms, eta: 04h 54m 34s 249ms\n",
      "\u001b[32m2021-05-04T00:19:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0081, train/hateful_memes/cross_entropy/avg: 0.1111, train/total_loss: 0.0081, train/total_loss/avg: 0.1111, max mem: 10794.0, experiment: run, epoch: 19, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 236ms, time_since_start: 02h 09m 21s 323ms, eta: 04h 53m 144ms\n",
      "\u001b[32m2021-05-04T00:21:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1091, train/total_loss: 0.0043, train/total_loss/avg: 0.1091, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 0.93, time: 01m 47s 299ms, time_since_start: 02h 11m 08s 622ms, eta: 04h 59m 47s 668ms\n",
      "\u001b[32m2021-05-04T00:23:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0038, train/hateful_memes/cross_entropy/avg: 0.1072, train/total_loss: 0.0038, train/total_loss/avg: 0.1072, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 0.99, time: 01m 41s 466ms, time_since_start: 02h 12m 50s 089ms, eta: 04h 41m 46s 745ms\n",
      "\u001b[32m2021-05-04T00:25:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0038, train/hateful_memes/cross_entropy/avg: 0.1054, train/total_loss: 0.0038, train/total_loss/avg: 0.1054, max mem: 10794.0, experiment: run, epoch: 20, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 222ms, time_since_start: 02h 14m 34s 311ms, eta: 04h 47m 40s 051ms\n",
      "\u001b[32m2021-05-04T00:26:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1080, train/total_loss: 0.0043, train/total_loss/avg: 0.1080, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 347ms, time_since_start: 02h 16m 20s 659ms, eta: 04h 51m 43s 991ms\n",
      "\u001b[32m2021-05-04T00:28:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1062, train/total_loss: 0.0043, train/total_loss/avg: 0.1062, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 084ms, time_since_start: 02h 18m 03s 744ms, eta: 04h 41m 02s 203ms\n",
      "\u001b[32m2021-05-04T00:30:13 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T00:30:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:30:39 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:31:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:31:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1045, train/total_loss: 0.0032, train/total_loss/avg: 0.1045, max mem: 10794.0, experiment: run, epoch: 21, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 46s 092ms, time_since_start: 02h 20m 49s 836ms, eta: 07h 29m 59s 990ms\n",
      "\u001b[32m2021-05-04T00:31:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T00:31:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:31:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:31:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:32:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:32:51 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-04T00:33:28 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:34:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:34:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 2.3982, val/total_loss: 2.3982, val/hateful_memes/accuracy: 0.6600, val/hateful_memes/binary_f1: 0.5707, val/hateful_memes/roc_auc: 0.7283, num_updates: 6000, epoch: 21, iterations: 6000, max_updates: 22000, val_time: 02m 49s 865ms, best_update: 6000, best_iteration: 6000, best_val/hateful_memes/roc_auc: 0.728297\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:36:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:36:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:36:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1031, train/total_loss: 0.0032, train/total_loss/avg: 0.1031, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 48s 202ms, time_since_start: 02h 26m 27s 908ms, eta: 07h 32m 52s 157ms\n",
      "\u001b[32m2021-05-04T00:38:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.1015, train/total_loss: 0.0032, train/total_loss/avg: 0.1015, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 430ms, time_since_start: 02h 28m 12s 338ms, eta: 04h 39m 24s 001ms\n",
      "\u001b[32m2021-05-04T00:40:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1000, train/total_loss: 0.0036, train/total_loss/avg: 0.1000, max mem: 10794.0, experiment: run, epoch: 22, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 438ms, time_since_start: 02h 29m 55s 777ms, eta: 04h 34m 59s 717ms\n",
      "\u001b[32m2021-05-04T00:42:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0032, train/hateful_memes/cross_entropy/avg: 0.0985, train/total_loss: 0.0032, train/total_loss/avg: 0.0985, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 588ms, time_since_start: 02h 31m 41s 365ms, eta: 04h 38m 55s 289ms\n",
      "\u001b[32m2021-05-04T00:43:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0027, train/hateful_memes/cross_entropy/avg: 0.0969, train/total_loss: 0.0027, train/total_loss/avg: 0.0969, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 930ms, time_since_start: 02h 33m 25s 296ms, eta: 04h 32m 46s 949ms\n",
      "\u001b[32m2021-05-04T00:45:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0027, train/hateful_memes/cross_entropy/avg: 0.0959, train/total_loss: 0.0027, train/total_loss/avg: 0.0959, max mem: 10794.0, experiment: run, epoch: 23, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 507ms, time_since_start: 02h 35m 08s 803ms, eta: 04h 29m 55s 137ms\n",
      "\u001b[32m2021-05-04T00:47:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.0945, train/total_loss: 0.0017, train/total_loss/avg: 0.0945, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 147ms, time_since_start: 02h 36m 53s 951ms, eta: 04h 32m 25s 041ms\n",
      "\u001b[32m2021-05-04T00:49:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.0931, train/total_loss: 0.0017, train/total_loss/avg: 0.0931, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 512ms, time_since_start: 02h 38m 37s 463ms, eta: 04h 26m 25s 628ms\n",
      "\u001b[32m2021-05-04T00:50:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0918, train/total_loss: 0.0022, train/total_loss/avg: 0.0918, max mem: 10794.0, experiment: run, epoch: 24, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 639ms, time_since_start: 02h 40m 21s 103ms, eta: 04h 25m 028ms\n",
      "\u001b[32m2021-05-04T00:52:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T00:52:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:52:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:53:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:53:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.0905, train/total_loss: 0.0017, train/total_loss/avg: 0.0905, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 46s 568ms, time_since_start: 02h 43m 07s 672ms, eta: 07h 03m 05s 093ms\n",
      "\u001b[32m2021-05-04T00:53:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T00:53:33 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:53:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T00:53:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T00:54:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T00:54:48 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-04T00:55:24 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T00:56:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T00:56:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.5872, val/total_loss: 2.5872, val/hateful_memes/accuracy: 0.6440, val/hateful_memes/binary_f1: 0.5241, val/hateful_memes/roc_auc: 0.7287, num_updates: 7000, epoch: 25, iterations: 7000, max_updates: 22000, val_time: 02m 26s 646ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.728669\n",
      "\u001b[32m2021-05-04T00:58:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.0893, train/total_loss: 0.0017, train/total_loss/avg: 0.0893, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 48s 088ms, time_since_start: 02h 48m 22s 412ms, eta: 07h 04m 05s 850ms\n",
      "\u001b[32m2021-05-04T01:00:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.0881, train/total_loss: 0.0017, train/total_loss/avg: 0.0881, max mem: 10794.0, experiment: run, epoch: 25, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 1.01, time: 01m 39s 772ms, time_since_start: 02h 50m 02s 185ms, eta: 04h 10m 02s 587ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:00:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:00:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:02:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.0871, train/total_loss: 0.0017, train/total_loss/avg: 0.0871, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 388ms, time_since_start: 02h 51m 47s 573ms, eta: 04h 22m 19s 977ms\n",
      "\u001b[32m2021-05-04T01:03:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0862, train/total_loss: 0.0022, train/total_loss/avg: 0.0862, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 297ms, time_since_start: 02h 53m 31s 871ms, eta: 04h 17m 51s 060ms\n",
      "\u001b[32m2021-05-04T01:05:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0854, train/total_loss: 0.0022, train/total_loss/avg: 0.0854, max mem: 10794.0, experiment: run, epoch: 26, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 960ms, time_since_start: 02h 55m 15s 832ms, eta: 04h 15m 15s 499ms\n",
      "\u001b[32m2021-05-04T01:07:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0843, train/total_loss: 0.0022, train/total_loss/avg: 0.0843, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 238ms, time_since_start: 02h 57m 01s 071ms, eta: 04h 16m 36s 888ms\n",
      "\u001b[32m2021-05-04T01:09:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0833, train/total_loss: 0.0022, train/total_loss/avg: 0.0833, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 077ms, time_since_start: 02h 58m 45s 148ms, eta: 04h 12m 01s 209ms\n",
      "\u001b[32m2021-05-04T01:10:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0825, train/total_loss: 0.0022, train/total_loss/avg: 0.0825, max mem: 10794.0, experiment: run, epoch: 27, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 372ms, time_since_start: 03h 28s 521ms, eta: 04h 08m 33s 794ms\n",
      "\u001b[32m2021-05-04T01:12:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0815, train/total_loss: 0.0022, train/total_loss/avg: 0.0815, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 657ms, time_since_start: 03h 02m 13s 178ms, eta: 04h 09m 52s 793ms\n",
      "\u001b[32m2021-05-04T01:14:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T01:14:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:14:46 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:15:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:15:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0804, train/total_loss: 0.0022, train/total_loss/avg: 0.0804, max mem: 10794.0, experiment: run, epoch: 28, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 45s 331ms, time_since_start: 03h 04m 58s 510ms, eta: 06h 31m 56s 790ms\n",
      "\u001b[32m2021-05-04T01:15:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T01:15:24 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:15:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:15:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:16:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:16:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:17:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:17:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.6365, val/total_loss: 2.6365, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5464, val/hateful_memes/roc_auc: 0.7209, num_updates: 8000, epoch: 28, iterations: 8000, max_updates: 22000, val_time: 02m 04s 639ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.728669\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:19:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:19:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:20:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0795, train/total_loss: 0.0022, train/total_loss/avg: 0.0795, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00001, ups: 0.66, time: 02m 32s 866ms, time_since_start: 03h 09m 36s 020ms, eta: 05h 59m 48s 453ms\n",
      "\u001b[32m2021-05-04T01:21:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0786, train/total_loss: 0.0022, train/total_loss/avg: 0.0786, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00001, ups: 0.99, time: 01m 41s 836ms, time_since_start: 03h 11m 17s 857ms, eta: 03h 57m 58s 286ms\n",
      "\u001b[32m2021-05-04T01:23:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0776, train/total_loss: 0.0022, train/total_loss/avg: 0.0776, max mem: 10794.0, experiment: run, epoch: 29, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 686ms, time_since_start: 03h 13m 01s 543ms, eta: 04h 32s 268ms\n",
      "\u001b[32m2021-05-04T01:25:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0767, train/total_loss: 0.0022, train/total_loss/avg: 0.0767, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00001, ups: 0.94, time: 01m 46s 122ms, time_since_start: 03h 14m 47s 665ms, eta: 04h 04m 23s 614ms\n",
      "\u001b[32m2021-05-04T01:26:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0764, train/total_loss: 0.0022, train/total_loss/avg: 0.0764, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00001, ups: 0.99, time: 01m 41s 887ms, time_since_start: 03h 16m 29s 553ms, eta: 03h 52m 54s 888ms\n",
      "\u001b[32m2021-05-04T01:28:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0755, train/total_loss: 0.0022, train/total_loss/avg: 0.0755, max mem: 10794.0, experiment: run, epoch: 30, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 607ms, time_since_start: 03h 18m 13s 161ms, eta: 03h 55m 05s 590ms\n",
      "\u001b[32m2021-05-04T01:30:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0747, train/total_loss: 0.0022, train/total_loss/avg: 0.0747, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 049ms, time_since_start: 03h 19m 58s 211ms, eta: 03h 56m 35s 176ms\n",
      "\u001b[32m2021-05-04T01:32:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0738, train/total_loss: 0.0022, train/total_loss/avg: 0.0738, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 763ms, time_since_start: 03h 21m 40s 974ms, eta: 03h 49m 41s 782ms\n",
      "\u001b[32m2021-05-04T01:33:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0730, train/total_loss: 0.0022, train/total_loss/avg: 0.0730, max mem: 10794.0, experiment: run, epoch: 31, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 317ms, time_since_start: 03h 23m 24s 291ms, eta: 03h 49m 11s 129ms\n",
      "\u001b[32m2021-05-04T01:35:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T01:35:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:35:58 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:36:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:36:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.0723, train/total_loss: 0.0031, train/total_loss/avg: 0.0723, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 46s 328ms, time_since_start: 03h 26m 10s 620ms, eta: 06h 06m 08s 695ms\n",
      "\u001b[32m2021-05-04T01:36:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T01:36:36 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:36:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:36:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:37:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:38:11 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:38:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:38:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 3.3383, val/total_loss: 3.3383, val/hateful_memes/accuracy: 0.6280, val/hateful_memes/binary_f1: 0.4497, val/hateful_memes/roc_auc: 0.7191, num_updates: 9000, epoch: 32, iterations: 9000, max_updates: 22000, val_time: 02m 12s 700ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.728669\n",
      "\u001b[32m2021-05-04T01:41:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0031, train/hateful_memes/cross_entropy/avg: 0.0715, train/total_loss: 0.0031, train/total_loss/avg: 0.0715, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00001, ups: 0.68, time: 02m 28s 577ms, time_since_start: 03h 30m 51s 901ms, eta: 05h 24m 33s 139ms\n",
      "\u001b[32m2021-05-04T01:42:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0022, train/hateful_memes/cross_entropy/avg: 0.0707, train/total_loss: 0.0022, train/total_loss/avg: 0.0707, max mem: 10794.0, experiment: run, epoch: 32, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00001, ups: 1.03, time: 01m 37s 388ms, time_since_start: 03h 32m 29s 290ms, eta: 03h 31m 05s 208ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:43:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:43:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:44:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.0700, train/total_loss: 0.0015, train/total_loss/avg: 0.0700, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 715ms, time_since_start: 03h 34m 15s 006ms, eta: 03h 47m 20s 735ms\n",
      "\u001b[32m2021-05-04T01:46:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.0693, train/total_loss: 0.0011, train/total_loss/avg: 0.0693, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 975ms, time_since_start: 03h 35m 58s 982ms, eta: 03h 41m 50s 578ms\n",
      "\u001b[32m2021-05-04T01:48:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.0685, train/total_loss: 0.0011, train/total_loss/avg: 0.0685, max mem: 10794.0, experiment: run, epoch: 33, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 970ms, time_since_start: 03h 37m 42s 953ms, eta: 03h 40m 04s 243ms\n",
      "\u001b[32m2021-05-04T01:49:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0678, train/total_loss: 0.0009, train/total_loss/avg: 0.0678, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 207ms, time_since_start: 03h 39m 28s 160ms, eta: 03h 40m 54s 407ms\n",
      "\u001b[32m2021-05-04T01:51:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0671, train/total_loss: 0.0009, train/total_loss/avg: 0.0671, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 516ms, time_since_start: 03h 41m 11s 676ms, eta: 03h 35m 36s 201ms\n",
      "\u001b[32m2021-05-04T01:53:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0665, train/total_loss: 0.0009, train/total_loss/avg: 0.0665, max mem: 10794.0, experiment: run, epoch: 34, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 282ms, time_since_start: 03h 42m 55s 958ms, eta: 03h 35m 26s 001ms\n",
      "\u001b[32m2021-05-04T01:55:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0658, train/total_loss: 0.0010, train/total_loss/avg: 0.0658, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 258ms, time_since_start: 03h 44m 41s 217ms, eta: 03h 35m 40s 111ms\n",
      "\u001b[32m2021-05-04T01:56:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T01:56:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:57:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:57:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:57:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0652, train/total_loss: 0.0010, train/total_loss/avg: 0.0652, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 44s 758ms, time_since_start: 03h 47m 25s 976ms, eta: 05h 34m 47s 376ms\n",
      "\u001b[32m2021-05-04T01:57:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T01:57:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:57:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T01:57:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T01:58:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T01:59:13 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T01:59:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T01:59:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.4258, val/total_loss: 2.4258, val/hateful_memes/accuracy: 0.6620, val/hateful_memes/binary_f1: 0.5764, val/hateful_memes/roc_auc: 0.7259, num_updates: 10000, epoch: 35, iterations: 10000, max_updates: 22000, val_time: 01m 58s 309ms, best_update: 7000, best_iteration: 7000, best_val/hateful_memes/roc_auc: 0.728669\n",
      "\u001b[32m2021-05-04T02:02:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0645, train/total_loss: 0.0010, train/total_loss/avg: 0.0645, max mem: 10794.0, experiment: run, epoch: 35, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00001, ups: 0.68, time: 02m 28s 984ms, time_since_start: 03h 51m 53s 273ms, eta: 05h 12s 854ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:02:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:02:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:04:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0639, train/total_loss: 0.0010, train/total_loss/avg: 0.0639, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 686ms, time_since_start: 03h 53m 37s 960ms, eta: 03h 29m 10s 620ms\n",
      "\u001b[32m2021-05-04T02:05:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0633, train/total_loss: 0.0007, train/total_loss/avg: 0.0633, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 600ms, time_since_start: 03h 55m 21s 560ms, eta: 03h 25m 15s 153ms\n",
      "\u001b[32m2021-05-04T02:07:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0627, train/total_loss: 0.0007, train/total_loss/avg: 0.0627, max mem: 10794.0, experiment: run, epoch: 36, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 811ms, time_since_start: 03h 57m 05s 371ms, eta: 03h 23m 54s 763ms\n",
      "\u001b[32m2021-05-04T02:09:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.0621, train/total_loss: 0.0007, train/total_loss/avg: 0.0621, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 529ms, time_since_start: 03h 58m 50s 901ms, eta: 03h 25m 30s 090ms\n",
      "\u001b[32m2021-05-04T02:11:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0615, train/total_loss: 0.0004, train/total_loss/avg: 0.0615, max mem: 10794.0, experiment: run, epoch: 37, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 927ms, time_since_start: 04h 34s 829ms, eta: 03h 20m 37s 356ms\n",
      "\u001b[32m2021-05-04T02:12:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0609, train/total_loss: 0.0004, train/total_loss/avg: 0.0609, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00001, ups: 0.93, time: 01m 47s 324ms, time_since_start: 04h 02m 22s 153ms, eta: 03h 25m 21s 719ms\n",
      "\u001b[32m2021-05-04T02:14:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0604, train/total_loss: 0.0004, train/total_loss/avg: 0.0604, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00001, ups: 0.99, time: 01m 41s 886ms, time_since_start: 04h 04m 04s 039ms, eta: 03h 13m 13s 818ms\n",
      "\u001b[32m2021-05-04T02:16:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0598, train/total_loss: 0.0004, train/total_loss/avg: 0.0598, max mem: 10794.0, experiment: run, epoch: 38, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 291ms, time_since_start: 04h 05m 47s 331ms, eta: 03h 14m 08s 851ms\n",
      "\u001b[32m2021-05-04T02:17:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T02:17:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:18:23 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:19:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:19:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0593, train/total_loss: 0.0004, train/total_loss/avg: 0.0593, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00001, ups: 0.60, time: 02m 47s 892ms, time_since_start: 04h 08m 35s 223ms, eta: 05h 12m 43s 626ms\n",
      "\u001b[32m2021-05-04T02:19:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T02:19:01 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:19:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:19:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:20:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:20:30 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-05-04T02:21:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:22:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:22:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.9383, val/total_loss: 2.9383, val/hateful_memes/accuracy: 0.6540, val/hateful_memes/binary_f1: 0.5435, val/hateful_memes/roc_auc: 0.7322, num_updates: 11000, epoch: 39, iterations: 11000, max_updates: 22000, val_time: 03m 17s 249ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[32m2021-05-04T02:24:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0588, train/total_loss: 0.0003, train/total_loss/avg: 0.0588, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00001, ups: 0.63, time: 02m 38s 848ms, time_since_start: 04h 14m 31s 324ms, eta: 04h 53m 11s 493ms\n",
      "\u001b[32m2021-05-04T02:26:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0583, train/total_loss: 0.0003, train/total_loss/avg: 0.0583, max mem: 10794.0, experiment: run, epoch: 39, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00001, ups: 1.01, time: 01m 39s 128ms, time_since_start: 04h 16m 10s 452ms, eta: 03h 01m 17s 135ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:27:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:27:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:28:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0578, train/total_loss: 0.0003, train/total_loss/avg: 0.0578, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 728ms, time_since_start: 04h 17m 56s 181ms, eta: 03h 11m 33s 944ms\n",
      "\u001b[32m2021-05-04T02:30:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0575, train/total_loss: 0.0004, train/total_loss/avg: 0.0575, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 931ms, time_since_start: 04h 19m 40s 113ms, eta: 03h 06m 33s 019ms\n",
      "\u001b[32m2021-05-04T02:31:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0570, train/total_loss: 0.0004, train/total_loss/avg: 0.0570, max mem: 10794.0, experiment: run, epoch: 40, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 734ms, time_since_start: 04h 21m 23s 847ms, eta: 03h 04m 26s 428ms\n",
      "\u001b[32m2021-05-04T02:33:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0565, train/total_loss: 0.0005, train/total_loss/avg: 0.0565, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 189ms, time_since_start: 04h 23m 09s 037ms, eta: 03h 05m 14s 725ms\n",
      "\u001b[32m2021-05-04T02:35:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0560, train/total_loss: 0.0004, train/total_loss/avg: 0.0560, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00001, ups: 0.97, time: 01m 43s 953ms, time_since_start: 04h 24m 52s 991ms, eta: 03h 01m 18s 579ms\n",
      "\u001b[32m2021-05-04T02:37:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0556, train/total_loss: 0.0003, train/total_loss/avg: 0.0556, max mem: 10794.0, experiment: run, epoch: 41, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 133ms, time_since_start: 04h 26m 37s 125ms, eta: 02h 59m 51s 601ms\n",
      "\u001b[32m2021-05-04T02:38:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0551, train/total_loss: 0.0003, train/total_loss/avg: 0.0551, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00001, ups: 0.95, time: 01m 45s 643ms, time_since_start: 04h 28m 22s 768ms, eta: 03h 40s 675ms\n",
      "\u001b[32m2021-05-04T02:40:32 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T02:40:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:40:56 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:41:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:41:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0546, train/total_loss: 0.0003, train/total_loss/avg: 0.0546, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00001, ups: 0.61, time: 02m 45s 563ms, time_since_start: 04h 31m 08s 331ms, eta: 04h 40m 21s 268ms\n",
      "\u001b[32m2021-05-04T02:41:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T02:41:34 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:41:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:41:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:42:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T02:43:08 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T02:43:45 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T02:43:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.7526, val/total_loss: 2.7526, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5693, val/hateful_memes/roc_auc: 0.7281, num_updates: 12000, epoch: 42, iterations: 12000, max_updates: 22000, val_time: 02m 11s 583ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[32m2021-05-04T02:46:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0542, train/total_loss: 0.0003, train/total_loss/avg: 0.0542, max mem: 10794.0, experiment: run, epoch: 42, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0., ups: 0.67, time: 02m 29s 988ms, time_since_start: 04h 35m 49s 909ms, eta: 04h 11m 26s 427ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:46:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T02:46:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T02:47:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0538, train/total_loss: 0.0003, train/total_loss/avg: 0.0538, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 822ms, time_since_start: 04h 37m 32s 732ms, eta: 02h 50m 37s 872ms\n",
      "\u001b[32m2021-05-04T02:49:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0533, train/total_loss: 0.0003, train/total_loss/avg: 0.0533, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 344ms, time_since_start: 04h 39m 16s 077ms, eta: 02h 49m 44s 842ms\n",
      "\u001b[32m2021-05-04T02:51:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0529, train/total_loss: 0.0003, train/total_loss/avg: 0.0529, max mem: 10794.0, experiment: run, epoch: 43, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 021ms, time_since_start: 04h 41m 099ms, eta: 02h 49m 05s 857ms\n",
      "\u001b[32m2021-05-04T02:53:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0525, train/total_loss: 0.0003, train/total_loss/avg: 0.0525, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 373ms, time_since_start: 04h 42m 45s 472ms, eta: 02h 49m 30s 630ms\n",
      "\u001b[32m2021-05-04T02:54:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0521, train/total_loss: 0.0003, train/total_loss/avg: 0.0521, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 862ms, time_since_start: 04h 44m 29s 334ms, eta: 02h 45m 19s 252ms\n",
      "\u001b[32m2021-05-04T02:56:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0517, train/total_loss: 0.0005, train/total_loss/avg: 0.0517, max mem: 10794.0, experiment: run, epoch: 44, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 674ms, time_since_start: 04h 46m 13s 008ms, eta: 02h 43m 15s 964ms\n",
      "\u001b[32m2021-05-04T02:58:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0513, train/total_loss: 0.0003, train/total_loss/avg: 0.0513, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 256ms, time_since_start: 04h 47m 58s 264ms, eta: 02h 43m 58s 500ms\n",
      "\u001b[32m2021-05-04T03:00:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0509, train/total_loss: 0.0005, train/total_loss/avg: 0.0509, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 977ms, time_since_start: 04h 49m 42s 242ms, eta: 02h 40m 13s 312ms\n",
      "\u001b[32m2021-05-04T03:01:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T03:01:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:02:16 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:02:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:02:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0507, train/total_loss: 0.0005, train/total_loss/avg: 0.0507, max mem: 10794.0, experiment: run, epoch: 45, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 43s 255ms, time_since_start: 04h 52m 25s 498ms, eta: 04h 08m 48s 117ms\n",
      "\u001b[32m2021-05-04T03:02:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T03:02:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:02:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:02:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:03:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:04:17 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:04:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:04:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 3.1584, val/total_loss: 3.1584, val/hateful_memes/accuracy: 0.6360, val/hateful_memes/binary_f1: 0.5285, val/hateful_memes/roc_auc: 0.7163, num_updates: 13000, epoch: 45, iterations: 13000, max_updates: 22000, val_time: 02m 02s 158ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:04:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:04:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:07:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0503, train/total_loss: 0.0005, train/total_loss/avg: 0.0503, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0., ups: 0.66, time: 02m 32s 968ms, time_since_start: 04h 57m 628ms, eta: 03h 50m 32s 062ms\n",
      "\u001b[32m2021-05-04T03:09:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0499, train/total_loss: 0.0005, train/total_loss/avg: 0.0499, max mem: 10794.0, experiment: run, epoch: 46, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 513ms, time_since_start: 04h 58m 39s 142ms, eta: 02h 26m 47s 868ms\n",
      "\u001b[32m2021-05-04T03:10:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0495, train/total_loss: 0.0003, train/total_loss/avg: 0.0495, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 957ms, time_since_start: 05h 26s 099ms, eta: 02h 37m 34s 156ms\n",
      "\u001b[32m2021-05-04T03:12:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0492, train/total_loss: 0.0003, train/total_loss/avg: 0.0492, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 954ms, time_since_start: 05h 02m 08s 053ms, eta: 02h 28m 28s 373ms\n",
      "\u001b[32m2021-05-04T03:14:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0488, train/total_loss: 0.0002, train/total_loss/avg: 0.0488, max mem: 10794.0, experiment: run, epoch: 47, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 328ms, time_since_start: 05h 03m 51s 382ms, eta: 02h 28m 43s 443ms\n",
      "\u001b[32m2021-05-04T03:16:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0484, train/total_loss: 0.0002, train/total_loss/avg: 0.0484, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 345ms, time_since_start: 05h 05m 37s 727ms, eta: 02h 31m 15s 928ms\n",
      "\u001b[32m2021-05-04T03:17:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0481, train/total_loss: 0.0003, train/total_loss/avg: 0.0481, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 815ms, time_since_start: 05h 07m 19s 543ms, eta: 02h 23m 05s 930ms\n",
      "\u001b[32m2021-05-04T03:19:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0477, train/total_loss: 0.0003, train/total_loss/avg: 0.0477, max mem: 10794.0, experiment: run, epoch: 48, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 813ms, time_since_start: 05h 09m 03s 356ms, eta: 02h 24m 08s 907ms\n",
      "\u001b[32m2021-05-04T03:21:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0474, train/total_loss: 0.0003, train/total_loss/avg: 0.0474, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 484ms, time_since_start: 05h 10m 48s 841ms, eta: 02h 24m 40s 989ms\n",
      "\u001b[32m2021-05-04T03:22:57 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T03:22:57 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:23:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:23:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:23:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0471, train/total_loss: 0.0002, train/total_loss/avg: 0.0471, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 44s 213ms, time_since_start: 05h 13m 33s 055ms, eta: 03h 42m 27s 313ms\n",
      "\u001b[32m2021-05-04T03:23:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T03:23:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:23:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:23:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:25:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:25:30 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:26:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:26:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 3.0985, val/total_loss: 3.0985, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5450, val/hateful_memes/roc_auc: 0.7076, num_updates: 14000, epoch: 49, iterations: 14000, max_updates: 22000, val_time: 02m 08s 530ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[32m2021-05-04T03:28:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0467, train/total_loss: 0.0002, train/total_loss/avg: 0.0467, max mem: 10794.0, experiment: run, epoch: 49, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0., ups: 0.67, time: 02m 30s 013ms, time_since_start: 05h 18m 11s 604ms, eta: 03h 20m 40s 665ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:29:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:29:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:30:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0464, train/total_loss: 0.0002, train/total_loss/avg: 0.0464, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 299ms, time_since_start: 05h 19m 54s 904ms, eta: 02h 16m 26s 279ms\n",
      "\u001b[32m2021-05-04T03:32:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0461, train/total_loss: 0.0002, train/total_loss/avg: 0.0461, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 470ms, time_since_start: 05h 21m 38s 374ms, eta: 02h 14m 54s 718ms\n",
      "\u001b[32m2021-05-04T03:33:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0458, train/total_loss: 0.0002, train/total_loss/avg: 0.0458, max mem: 10794.0, experiment: run, epoch: 50, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 124ms, time_since_start: 05h 23m 22s 499ms, eta: 02h 14m 099ms\n",
      "\u001b[32m2021-05-04T03:35:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0454, train/total_loss: 0.0002, train/total_loss/avg: 0.0454, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 272ms, time_since_start: 05h 25m 07s 772ms, eta: 02h 13m 41s 788ms\n",
      "\u001b[32m2021-05-04T03:37:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0452, train/total_loss: 0.0002, train/total_loss/avg: 0.0452, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 287ms, time_since_start: 05h 26m 51s 060ms, eta: 02h 09m 25s 597ms\n",
      "\u001b[32m2021-05-04T03:39:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0449, train/total_loss: 0.0002, train/total_loss/avg: 0.0449, max mem: 10794.0, experiment: run, epoch: 51, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 185ms, time_since_start: 05h 28m 34s 245ms, eta: 02h 07m 33s 030ms\n",
      "\u001b[32m2021-05-04T03:40:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0446, train/total_loss: 0.0002, train/total_loss/avg: 0.0446, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 724ms, time_since_start: 05h 30m 20s 970ms, eta: 02h 10m 07s 107ms\n",
      "\u001b[32m2021-05-04T03:42:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0443, train/total_loss: 0.0002, train/total_loss/avg: 0.0443, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 698ms, time_since_start: 05h 32m 04s 669ms, eta: 02h 04m 40s 430ms\n",
      "\u001b[32m2021-05-04T03:44:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T03:44:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:44:38 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:45:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:45:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0441, train/total_loss: 0.0002, train/total_loss/avg: 0.0441, max mem: 10794.0, experiment: run, epoch: 52, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 45s 403ms, time_since_start: 05h 34m 50s 072ms, eta: 03h 16m 03s 494ms\n",
      "\u001b[32m2021-05-04T03:45:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T03:45:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:45:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:45:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:46:24 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T03:46:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T03:47:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T03:47:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 3.2940, val/total_loss: 3.2940, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5252, val/hateful_memes/roc_auc: 0.6975, num_updates: 15000, epoch: 52, iterations: 15000, max_updates: 22000, val_time: 02m 11s 787ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:48:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T03:48:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T03:50:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0438, train/total_loss: 0.0001, train/total_loss/avg: 0.0438, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0., ups: 0.65, time: 02m 33s 352ms, time_since_start: 05h 39m 35s 217ms, eta: 02h 59m 10s 654ms\n",
      "\u001b[32m2021-05-04T03:51:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0435, train/total_loss: 0.0002, train/total_loss/avg: 0.0435, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 022ms, time_since_start: 05h 41m 19s 240ms, eta: 01h 59m 46s 677ms\n",
      "\u001b[32m2021-05-04T03:53:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0432, train/total_loss: 0.0002, train/total_loss/avg: 0.0432, max mem: 10794.0, experiment: run, epoch: 53, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 693ms, time_since_start: 05h 43m 02s 933ms, eta: 01h 57m 38s 598ms\n",
      "\u001b[32m2021-05-04T03:55:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0429, train/total_loss: 0.0001, train/total_loss/avg: 0.0429, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 886ms, time_since_start: 05h 44m 48s 820ms, eta: 01h 58m 20s 366ms\n",
      "\u001b[32m2021-05-04T03:56:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0426, train/total_loss: 0.0001, train/total_loss/avg: 0.0426, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 125ms, time_since_start: 05h 46m 32s 946ms, eta: 01h 54m 36s 471ms\n",
      "\u001b[32m2021-05-04T03:58:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0424, train/total_loss: 0.0001, train/total_loss/avg: 0.0424, max mem: 10794.0, experiment: run, epoch: 54, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 114ms, time_since_start: 05h 48m 16s 060ms, eta: 01h 51m 44s 908ms\n",
      "\u001b[32m2021-05-04T04:00:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0421, train/total_loss: 0.0001, train/total_loss/avg: 0.0421, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 755ms, time_since_start: 05h 50m 01s 815ms, eta: 01h 52m 49s 183ms\n",
      "\u001b[32m2021-05-04T04:02:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0418, train/total_loss: 0.0001, train/total_loss/avg: 0.0418, max mem: 10794.0, experiment: run, epoch: 55, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 428ms, time_since_start: 05h 51m 45s 245ms, eta: 01h 48m 35s 200ms\n",
      "\u001b[32m2021-05-04T04:03:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0416, train/total_loss: 0.0001, train/total_loss/avg: 0.0416, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 896ms, time_since_start: 05h 53m 32s 141ms, eta: 01h 50m 24s 993ms\n",
      "\u001b[32m2021-05-04T04:05:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T04:05:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:06:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:06:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:06:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0413, train/total_loss: 0.0000, train/total_loss/avg: 0.0413, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 43s 948ms, time_since_start: 05h 56m 16s 089ms, eta: 02h 46m 34s 297ms\n",
      "\u001b[32m2021-05-04T04:06:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T04:06:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:06:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:06:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:07:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 3.7167, val/total_loss: 3.7167, val/hateful_memes/accuracy: 0.6380, val/hateful_memes/binary_f1: 0.5041, val/hateful_memes/roc_auc: 0.6955, num_updates: 16000, epoch: 56, iterations: 16000, max_updates: 22000, val_time: 01m 07s 406ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[32m2021-05-04T04:09:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0410, train/total_loss: 0.0000, train/total_loss/avg: 0.0410, max mem: 10794.0, experiment: run, epoch: 56, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0., ups: 1.01, time: 01m 39s 972ms, time_since_start: 05h 59m 03s 484ms, eta: 01h 39m 52s 782ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:10:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:10:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:11:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0408, train/total_loss: 0.0000, train/total_loss/avg: 0.0408, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 428ms, time_since_start: 06h 44s 913ms, eta: 01h 39m 36s 985ms\n",
      "\u001b[32m2021-05-04T04:12:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0405, train/total_loss: 0.0000, train/total_loss/avg: 0.0405, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 539ms, time_since_start: 06h 02m 26s 452ms, eta: 01h 38m 329ms\n",
      "\u001b[32m2021-05-04T04:14:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0403, train/total_loss: 0.0000, train/total_loss/avg: 0.0403, max mem: 10794.0, experiment: run, epoch: 57, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 831ms, time_since_start: 06h 04m 10s 283ms, eta: 01h 38m 27s 571ms\n",
      "\u001b[32m2021-05-04T04:16:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0401, train/total_loss: 0.0001, train/total_loss/avg: 0.0401, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 341ms, time_since_start: 06h 05m 55s 624ms, eta: 01h 38m 06s 460ms\n",
      "\u001b[32m2021-05-04T04:18:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0398, train/total_loss: 0.0001, train/total_loss/avg: 0.0398, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 549ms, time_since_start: 06h 07m 39s 173ms, eta: 01h 34m 41s 120ms\n",
      "\u001b[32m2021-05-04T04:19:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0396, train/total_loss: 0.0001, train/total_loss/avg: 0.0396, max mem: 10794.0, experiment: run, epoch: 58, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 422ms, time_since_start: 06h 09m 23s 595ms, eta: 01h 33m 42s 920ms\n",
      "\u001b[32m2021-05-04T04:21:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0394, train/total_loss: 0.0001, train/total_loss/avg: 0.0394, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 086ms, time_since_start: 06h 11m 08s 682ms, eta: 01h 32m 31s 956ms\n",
      "\u001b[32m2021-05-04T04:23:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0391, train/total_loss: 0.0001, train/total_loss/avg: 0.0391, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 779ms, time_since_start: 06h 12m 52s 462ms, eta: 01h 29m 37s 457ms\n",
      "\u001b[32m2021-05-04T04:25:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T04:25:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:25:25 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:26:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:26:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0389, train/total_loss: 0.0001, train/total_loss/avg: 0.0389, max mem: 10794.0, experiment: run, epoch: 59, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 44s 549ms, time_since_start: 06h 15m 37s 012ms, eta: 02h 19m 19s 135ms\n",
      "\u001b[32m2021-05-04T04:26:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T04:26:02 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:26:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:26:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:27:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 3.2690, val/total_loss: 3.2690, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5227, val/hateful_memes/roc_auc: 0.7188, num_updates: 17000, epoch: 59, iterations: 17000, max_updates: 22000, val_time: 01m 08s 381ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:28:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:28:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:28:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0387, train/total_loss: 0.0001, train/total_loss/avg: 0.0387, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 695ms, time_since_start: 06h 18m 28s 092ms, eta: 01h 25m 12s 608ms\n",
      "\u001b[32m2021-05-04T04:30:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0384, train/total_loss: 0.0001, train/total_loss/avg: 0.0384, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 972ms, time_since_start: 06h 20m 12s 065ms, eta: 01h 24m 30s 521ms\n",
      "\u001b[32m2021-05-04T04:32:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0382, train/total_loss: 0.0001, train/total_loss/avg: 0.0382, max mem: 10794.0, experiment: run, epoch: 60, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 142ms, time_since_start: 06h 21m 56s 207ms, eta: 01h 22m 53s 007ms\n",
      "\u001b[32m2021-05-04T04:34:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0380, train/total_loss: 0.0001, train/total_loss/avg: 0.0380, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 225ms, time_since_start: 06h 23m 40s 432ms, eta: 01h 21m 11s 076ms\n",
      "\u001b[32m2021-05-04T04:35:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0378, train/total_loss: 0.0001, train/total_loss/avg: 0.0378, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 268ms, time_since_start: 06h 25m 24s 701ms, eta: 01h 19m 27s 177ms\n",
      "\u001b[32m2021-05-04T04:37:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0376, train/total_loss: 0.0001, train/total_loss/avg: 0.0376, max mem: 10794.0, experiment: run, epoch: 61, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 489ms, time_since_start: 06h 27m 08s 191ms, eta: 01h 17m 06s 410ms\n",
      "\u001b[32m2021-05-04T04:39:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0374, train/total_loss: 0.0001, train/total_loss/avg: 0.0374, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 020ms, time_since_start: 06h 28m 53s 212ms, eta: 01h 16m 28s 125ms\n",
      "\u001b[32m2021-05-04T04:41:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0372, train/total_loss: 0.0001, train/total_loss/avg: 0.0372, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 455ms, time_since_start: 06h 30m 36s 667ms, eta: 01h 13m 34s 658ms\n",
      "\u001b[32m2021-05-04T04:42:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0370, train/total_loss: 0.0001, train/total_loss/avg: 0.0370, max mem: 10794.0, experiment: run, epoch: 62, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 526ms, time_since_start: 06h 32m 20s 194ms, eta: 01h 11m 52s 511ms\n",
      "\u001b[32m2021-05-04T04:44:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T04:44:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T04:44:54 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T04:45:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T04:45:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0368, train/total_loss: 0.0001, train/total_loss/avg: 0.0368, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 45s 577ms, time_since_start: 06h 35m 05s 772ms, eta: 01h 52m 09s 091ms\n",
      "\u001b[32m2021-05-04T04:45:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T04:45:31 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:45:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:45:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:46:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 3.4289, val/total_loss: 3.4289, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.5358, val/hateful_memes/roc_auc: 0.7124, num_updates: 18000, epoch: 63, iterations: 18000, max_updates: 22000, val_time: 01m 04s 677ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[32m2021-05-04T04:48:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0366, train/total_loss: 0.0001, train/total_loss/avg: 0.0366, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0., ups: 1.01, time: 01m 39s 880ms, time_since_start: 06h 37m 50s 337ms, eta: 01h 05m 57s 663ms\n",
      "\u001b[32m2021-05-04T04:49:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0364, train/total_loss: 0.0001, train/total_loss/avg: 0.0364, max mem: 10794.0, experiment: run, epoch: 63, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0., ups: 1.00, time: 01m 40s 533ms, time_since_start: 06h 39m 30s 871ms, eta: 01h 04m 41s 393ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:50:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T04:50:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T04:51:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0362, train/total_loss: 0.0001, train/total_loss/avg: 0.0362, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 597ms, time_since_start: 06h 41m 14s 469ms, eta: 01h 04m 54s 453ms\n",
      "\u001b[32m2021-05-04T04:53:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0360, train/total_loss: 0.0001, train/total_loss/avg: 0.0360, max mem: 10794.0, experiment: run, epoch: 64, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 684ms, time_since_start: 06h 42m 58s 154ms, eta: 01h 03m 12s 381ms\n",
      "\u001b[32m2021-05-04T04:55:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0358, train/total_loss: 0.0001, train/total_loss/avg: 0.0358, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 952ms, time_since_start: 06h 44m 45s 106ms, eta: 01h 03m 23s 215ms\n",
      "\u001b[32m2021-05-04T04:56:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0356, train/total_loss: 0.0001, train/total_loss/avg: 0.0356, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 512ms, time_since_start: 06h 46m 27s 618ms, eta: 59m 01s 175ms\n",
      "\u001b[32m2021-05-04T04:58:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0354, train/total_loss: 0.0001, train/total_loss/avg: 0.0354, max mem: 10794.0, experiment: run, epoch: 65, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 971ms, time_since_start: 06h 48m 11s 590ms, eta: 58m 05s 973ms\n",
      "\u001b[32m2021-05-04T05:00:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0352, train/total_loss: 0.0001, train/total_loss/avg: 0.0352, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 567ms, time_since_start: 06h 49m 58s 158ms, eta: 57m 44s 738ms\n",
      "\u001b[32m2021-05-04T05:02:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0350, train/total_loss: 0.0001, train/total_loss/avg: 0.0350, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0., ups: 1.00, time: 01m 40s 791ms, time_since_start: 06h 51m 38s 950ms, eta: 52m 54s 534ms\n",
      "\u001b[32m2021-05-04T05:03:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T05:03:48 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T05:07:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T05:08:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T05:08:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0348, train/total_loss: 0.0001, train/total_loss/avg: 0.0348, max mem: 10794.0, experiment: run, epoch: 66, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0., ups: 0.25, time: 06m 48s 735ms, time_since_start: 06h 58m 27s 685ms, eta: 03h 27m 38s 270ms\n",
      "\u001b[32m2021-05-04T05:08:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T05:08:53 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:08:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:08:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:10:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.3627, val/total_loss: 3.3627, val/hateful_memes/accuracy: 0.6520, val/hateful_memes/binary_f1: 0.5469, val/hateful_memes/roc_auc: 0.7123, num_updates: 19000, epoch: 66, iterations: 19000, max_updates: 22000, val_time: 01m 07s 269ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:11:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:11:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:15:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0347, train/total_loss: 0.0001, train/total_loss/avg: 0.0347, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0., ups: 0.30, time: 05m 29s 904ms, time_since_start: 07h 05m 04s 864ms, eta: 02h 42m 305ms\n",
      "\u001b[32m2021-05-04T05:17:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0345, train/total_loss: 0.0000, train/total_loss/avg: 0.0345, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0., ups: 1.01, time: 01m 39s 897ms, time_since_start: 07h 06m 44s 762ms, eta: 47m 21s 877ms\n",
      "\u001b[32m2021-05-04T05:18:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0343, train/total_loss: 0.0000, train/total_loss/avg: 0.0343, max mem: 10794.0, experiment: run, epoch: 67, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 063ms, time_since_start: 07h 08m 27s 825ms, eta: 47m 07s 231ms\n",
      "\u001b[32m2021-05-04T05:20:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0341, train/total_loss: 0.0000, train/total_loss/avg: 0.0341, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 150ms, time_since_start: 07h 10m 12s 975ms, eta: 46m 17s 654ms\n",
      "\u001b[32m2021-05-04T05:22:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0339, train/total_loss: 0.0000, train/total_loss/avg: 0.0339, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 078ms, time_since_start: 07h 11m 56s 054ms, eta: 43m 38s 199ms\n",
      "\u001b[32m2021-05-04T05:24:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0338, train/total_loss: 0.0000, train/total_loss/avg: 0.0338, max mem: 10794.0, experiment: run, epoch: 68, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 481ms, time_since_start: 07h 13m 39s 535ms, eta: 42m 03s 290ms\n",
      "\u001b[32m2021-05-04T05:25:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0336, train/total_loss: 0.0000, train/total_loss/avg: 0.0336, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 958ms, time_since_start: 07h 15m 24s 494ms, eta: 40m 52s 673ms\n",
      "\u001b[32m2021-05-04T05:27:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0334, train/total_loss: 0.0000, train/total_loss/avg: 0.0334, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 459ms, time_since_start: 07h 17m 07s 953ms, eta: 38m 32s 522ms\n",
      "\u001b[32m2021-05-04T05:29:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0333, train/total_loss: 0.0000, train/total_loss/avg: 0.0333, max mem: 10794.0, experiment: run, epoch: 69, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 607ms, time_since_start: 07h 18m 51s 561ms, eta: 36m 50s 579ms\n",
      "\u001b[32m2021-05-04T05:31:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T05:31:02 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T05:31:26 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T05:32:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T05:32:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0331, train/total_loss: 0.0000, train/total_loss/avg: 0.0331, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0., ups: 0.60, time: 02m 46s 763ms, time_since_start: 07h 21m 38s 325ms, eta: 56m 28s 639ms\n",
      "\u001b[32m2021-05-04T05:32:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T05:32:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:32:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:32:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:33:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 3.6564, val/total_loss: 3.6564, val/hateful_memes/accuracy: 0.6460, val/hateful_memes/binary_f1: 0.5280, val/hateful_memes/roc_auc: 0.7100, num_updates: 20000, epoch: 70, iterations: 20000, max_updates: 22000, val_time: 01m 02s 707ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[32m2021-05-04T05:34:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0329, train/total_loss: 0.0000, train/total_loss/avg: 0.0329, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 283ms, time_since_start: 07h 24m 27s 325ms, eta: 34m 11s 691ms\n",
      "\u001b[32m2021-05-04T05:36:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0328, train/total_loss: 0.0000, train/total_loss/avg: 0.0328, max mem: 10794.0, experiment: run, epoch: 70, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 941ms, time_since_start: 07h 26m 04s 267ms, eta: 29m 32s 869ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:37:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:37:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:38:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0326, train/total_loss: 0.0000, train/total_loss/avg: 0.0326, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 767ms, time_since_start: 07h 27m 49s 035ms, eta: 30m 09s 545ms\n",
      "\u001b[32m2021-05-04T05:39:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0325, train/total_loss: 0.0000, train/total_loss/avg: 0.0325, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 471ms, time_since_start: 07h 29m 32s 506ms, eta: 28m 02s 031ms\n",
      "\u001b[32m2021-05-04T05:41:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0323, train/total_loss: 0.0000, train/total_loss/avg: 0.0323, max mem: 10794.0, experiment: run, epoch: 71, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 863ms, time_since_start: 07h 31m 16s 370ms, eta: 26m 22s 876ms\n",
      "\u001b[32m2021-05-04T05:43:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0322, train/total_loss: 0.0000, train/total_loss/avg: 0.0322, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 339ms, time_since_start: 07h 33m 709ms, eta: 24m 44s 121ms\n",
      "\u001b[32m2021-05-04T05:45:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0320, train/total_loss: 0.0000, train/total_loss/avg: 0.0320, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 242ms, time_since_start: 07h 34m 43s 951ms, eta: 22m 43s 624ms\n",
      "\u001b[32m2021-05-04T05:46:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0318, train/total_loss: 0.0000, train/total_loss/avg: 0.0318, max mem: 10794.0, experiment: run, epoch: 72, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 041ms, time_since_start: 07h 36m 27s 992ms, eta: 21m 08s 470ms\n",
      "\u001b[32m2021-05-04T05:48:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0317, train/total_loss: 0.0000, train/total_loss/avg: 0.0317, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 324ms, time_since_start: 07h 38m 13s 317ms, eta: 19m 37s 104ms\n",
      "\u001b[32m2021-05-04T05:50:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T05:50:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T05:50:45 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T05:51:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T05:51:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0315, train/total_loss: 0.0000, train/total_loss/avg: 0.0315, max mem: 10794.0, experiment: run, epoch: 73, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 0.61, time: 02m 44s 203ms, time_since_start: 07h 40m 57s 520ms, eta: 27m 48s 309ms\n",
      "\u001b[32m2021-05-04T05:51:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T05:51:23 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:51:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:51:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:52:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 3.5540, val/total_loss: 3.5540, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5252, val/hateful_memes/roc_auc: 0.7128, num_updates: 21000, epoch: 73, iterations: 21000, max_updates: 22000, val_time: 01m 08s 055ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:54:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T05:54:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T05:54:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0314, train/total_loss: 0.0000, train/total_loss/avg: 0.0314, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 0.96, time: 01m 44s 068ms, time_since_start: 07h 43m 49s 647ms, eta: 15m 51s 598ms\n",
      "\u001b[32m2021-05-04T05:55:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0312, train/total_loss: 0.0000, train/total_loss/avg: 0.0312, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 062ms, time_since_start: 07h 45m 30s 710ms, eta: 13m 41s 439ms\n",
      "\u001b[32m2021-05-04T05:57:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0311, train/total_loss: 0.0000, train/total_loss/avg: 0.0311, max mem: 10794.0, experiment: run, epoch: 74, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 962ms, time_since_start: 07h 47m 14s 673ms, eta: 12m 19s 382ms\n",
      "\u001b[32m2021-05-04T05:59:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0309, train/total_loss: 0.0000, train/total_loss/avg: 0.0309, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 0.95, time: 01m 45s 716ms, time_since_start: 07h 49m 389ms, eta: 10m 44s 445ms\n",
      "\u001b[32m2021-05-04T06:01:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0308, train/total_loss: 0.0000, train/total_loss/avg: 0.0308, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 672ms, time_since_start: 07h 50m 42s 062ms, eta: 08m 36s 496ms\n",
      "\u001b[32m2021-05-04T06:02:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0307, train/total_loss: 0.0000, train/total_loss/avg: 0.0307, max mem: 10794.0, experiment: run, epoch: 75, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 935ms, time_since_start: 07h 52m 25s 998ms, eta: 07m 02s 395ms\n",
      "\u001b[32m2021-05-04T06:04:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0305, train/total_loss: 0.0000, train/total_loss/avg: 0.0305, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 208ms, time_since_start: 07h 54m 12s 206ms, eta: 05m 23s 724ms\n",
      "\u001b[32m2021-05-04T06:06:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0304, train/total_loss: 0.0000, train/total_loss/avg: 0.0304, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 185ms, time_since_start: 07h 55m 55s 392ms, eta: 03m 29s 672ms\n",
      "\u001b[32m2021-05-04T06:08:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0302, train/total_loss: 0.0000, train/total_loss/avg: 0.0302, max mem: 10794.0, experiment: run, epoch: 76, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 0.97, time: 01m 43s 417ms, time_since_start: 07h 57m 38s 809ms, eta: 01m 45s 071ms\n",
      "\u001b[32m2021-05-04T06:09:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-05-04T06:09:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-05-04T06:10:14 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-05-04T06:10:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-05-04T06:10:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0301, train/total_loss: 0.0000, train/total_loss/avg: 0.0301, max mem: 10794.0, experiment: run, epoch: 77, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 0.60, time: 02m 47s 125ms, time_since_start: 08h 25s 935ms, eta: 0ms\n",
      "\u001b[32m2021-05-04T06:10:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-05-04T06:10:51 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:10:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:10:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T06:12:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 3.6191, val/total_loss: 3.6191, val/hateful_memes/accuracy: 0.6420, val/hateful_memes/binary_f1: 0.5227, val/hateful_memes/roc_auc: 0.7123, num_updates: 22000, epoch: 77, iterations: 22000, max_updates: 22000, val_time: 01m 09s 668ms, best_update: 11000, best_iteration: 11000, best_val/hateful_memes/roc_auc: 0.732233\n",
      "\u001b[32m2021-05-04T06:12:02 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2021-05-04T06:12:02 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2021-05-04T06:12:02 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2021-05-04T06:12:28 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-04T06:12:28 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 11000\n",
      "\u001b[32m2021-05-04T06:12:28 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 11000\n",
      "\u001b[32m2021-05-04T06:12:28 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 39\n",
      "\u001b[32m2021-05-04T06:12:39 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-04T06:12:39 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:12:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:12:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "100% 16/16 [00:16<00:00,  1.01s/it]\n",
      "\u001b[32m2021-05-04T06:12:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.9383, val/total_loss: 2.9383, val/hateful_memes/accuracy: 0.6540, val/hateful_memes/binary_f1: 0.5435, val/hateful_memes/roc_auc: 0.7322\n",
      "\u001b[32m2021-05-04T06:12:56 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 08h 02m 30s 247ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/vilbert/from_cc.yaml \\\n",
    "model=vilbert \\\n",
    "dataset=hateful_memes \\\n",
    "run_type=train_val \\\n",
    "training.batch_size=32 \\\n",
    "env.save_dir=/content/gdrive/MyDrive/colab/finetuned_vilbertcc_election_memes/ \\\n",
    "checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original \\\n",
    "checkpoint.resume_pretrained=True \\\n",
    "dataset_config.hateful_memes.annotations.train[0]=/content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl \\\n",
    "dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n",
    "dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl \\\n",
    "dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246355,
     "status": "ok",
     "timestamp": 1620108949814,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "T0P5v8HfiFuR",
    "outputId": "9637bff6-be22-48bb-a0d3-f1ca21a3c0a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-04 06:13:18.387827: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-05-04T06:14:28 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/from_cc.yaml\n",
      "\u001b[32m2021-05-04T06:14:28 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
      "\u001b[32m2021-05-04T06:14:28 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-05-04T06:14:28 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2021-05-04T06:14:28 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.from_cc_original\n",
      "\u001b[32m2021-05-04T06:14:28 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
      "\u001b[32m2021-05-04T06:14:28 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2021-05-04T06:14:29 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2021-05-04T06:14:29 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original', 'checkpoint.resume_pretrained=False', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl'])\n",
      "\u001b[32m2021-05-04T06:14:29 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-05-04T06:14:29 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-05-04T06:14:29 | mmf_cli.run: \u001b[0mUsing seed 28874742\n",
      "\u001b[32m2021-05-04T06:14:29 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:14:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:14:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-05-04T06:14:31 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T06:14:31 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T06:14:31 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-05-04T06:14:31 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-05-04T06:14:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-05-04T06:14:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:14:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:14:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-05-04T06:14:48 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:15:03 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:15:03 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:15:03 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:15:03 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:15:04 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
      "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
      "Unexpected keys if any: []\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:15:05 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:15:05 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:15:05 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-05-04T06:15:05 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2021-05-04T06:15:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-05-04T06:15:05 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-05-04T06:15:05 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-05-04T06:15:05 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-05-04T06:15:05 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-05-04T06:15:05 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
      "  (model): ViLBERTForClassification(\n",
      "    (bert): ViLBERTBase(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (v_embeddings): BertImageFeatureEmbeddings(\n",
      "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (v_layer): ModuleList(\n",
      "          (0): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertImageLayer(\n",
      "            (attention): BertImageAttention(\n",
      "              (self): BertImageSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertImageSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (c_layer): ModuleList(\n",
      "          (0): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertConnectionLayer(\n",
      "            (biattention): BertBiAttention(\n",
      "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (biOutput): BertBiOutput(\n",
      "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
      "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout2): Dropout(p=0.1, inplace=False)\n",
      "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (v_intermediate): BertImageIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (v_output): BertImageOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (t_intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (t_output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (t_pooler): BertTextPooler(\n",
      "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (v_pooler): BertImagePooler(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-05-04T06:15:05 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
      "\u001b[32m2021-05-04T06:15:05 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-05-04T06:15:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100% 16/16 [00:42<00:00,  2.66s/it]\n",
      "\u001b[32m2021-05-04T06:15:48 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 3.9571, val/total_loss: 3.9571, val/hateful_memes/accuracy: 0.6080, val/hateful_memes/binary_f1: 0.4061, val/hateful_memes/roc_auc: 0.6851\n",
      "\u001b[32m2021-05-04T06:15:48 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 59s 399ms\n"
     ]
    }
   ],
   "source": [
    "# Baseline on dev_seen.jsonl\n",
    "!mmf_run config=projects/hateful_memes/configs/vilbert/from_cc.yaml \\\n",
    "  model=vilbert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=val \\\n",
    "  checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original \\\n",
    "  checkpoint.resume_pretrained=False \\\n",
    "  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iLMnGqXfwRHV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO1KO1WSfxcVA/wFYM0JOXa",
   "collapsed_sections": [],
   "name": "Pretrained + FineTuned ViLBERT CC(Hateful and Election Memes).ipynb",
   "provenance": [
    {
     "file_id": "1zJ_PgMxUEzdqlZoJPeoOv0T0SYkbYsM7",
     "timestamp": 1620005734396
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
