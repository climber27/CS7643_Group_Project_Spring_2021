{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49759,
     "status": "ok",
     "timestamp": 1619590844017,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "M8S4hnnOGDEi",
    "outputId": "d007dd4a-596a-4421-d1bb-aaad0f0eb3cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49754,
     "status": "ok",
     "timestamp": 1619590844017,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "RaY2MFytGYDJ",
    "outputId": "b776fcc8-2a34-40b5-fd4a-89b332dde599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/colab\n"
     ]
    }
   ],
   "source": [
    "%cd gdrive/MyDrive/colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50012,
     "status": "ok",
     "timestamp": 1619590844279,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "WafTvAelCJ0M",
    "outputId": "98736f88-b6a0-4cfa-8a48-8356fc0b9ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mhateful_and_election_memes_detectron.lmdb\u001b[0m/  \u001b[01;34msaved_models\u001b[0m/\n",
      "\u001b[01;36mmmf\u001b[0m@                                        train_hateful_and_election.jsonl\n",
      "\u001b[01;34msave\u001b[0m/                                       XjiOc5ycDBRRNwbhRlgH.zip\n"
     ]
    }
   ],
   "source": [
    "%ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50737,
     "status": "ok",
     "timestamp": 1619590845007,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "JkF5x4FfP8G_",
    "outputId": "c6ca1b6b-e639-47a6-edda-8527c295c75b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbuild\u001b[0m/   MANIFEST.in  \u001b[01;34mmmf.egg-info\u001b[0m/  pyproject.toml    \u001b[01;34msave\u001b[0m/     \u001b[01;34mtools\u001b[0m/\n",
      "\u001b[01;34mdocs\u001b[0m/    \u001b[01;34mmmf\u001b[0m/         NOTICES        README.md         setup.py  \u001b[01;34mwebsite\u001b[0m/\n",
      "LICENSE  \u001b[01;34mmmf_cli\u001b[0m/     \u001b[01;34mprojects\u001b[0m/      requirements.txt  \u001b[01;34mtests\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls ./mmf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50735,
     "status": "ok",
     "timestamp": 1619590845008,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "yeRnaUJWQhym",
    "outputId": "a9dce578-86de-4721-aaf7-5c9abbee0956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf\n"
     ]
    }
   ],
   "source": [
    "%cd mmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51416,
     "status": "ok",
     "timestamp": 1619590845692,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "H0o5kbTsIon2",
    "outputId": "03bd598c-d3dc-498b-d2e5-1e3a062275b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\t MANIFEST.in  mmf.egg-info  pyproject.toml    save\ttools\n",
      "docs\t mmf\t      NOTICES\t    README.md\t      setup.py\twebsite\n",
      "LICENSE  mmf_cli      projects\t    requirements.txt  tests\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 301209,
     "status": "ok",
     "timestamp": 1619591095490,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "M3TkkPwsQkHq",
    "outputId": "16152741-898c-49a4-99ee-10bb8eced9d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting torchvision<=0.9.1,>=0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/8a/82062a33b5eb7f696bf23f8ccf04bf6fc81d1a4972740fb21c2569ada0a6/torchvision-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (17.4MB)\n",
      "\u001b[K     |████████████████████████████████| 17.4MB 317kB/s \n",
      "\u001b[?25hCollecting iopath==0.1.7\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/d5/1c70fea7632640e8a9fb5a176676e555238119b3e7ee8b6dc49980ec5769/iopath-0.1.7-py3-none-any.whl\n",
      "Collecting omegaconf==2.0.6\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
      "Collecting matplotlib==3.3.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/3d/db9a6b3c83c9511301152dbb64a029c3a4313c86eaef12c237b13ecf91d6/matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6MB 55.0MB/s \n",
      "\u001b[?25hCollecting lmdb==0.98\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/5c/d56dbc2532ecf14fa004c543927500c0f645eaca8bd7ec39420c7546396a/lmdb-0.98.tar.gz (869kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 50.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.23.0)\n",
      "Collecting GitPython==3.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 48.5MB/s \n",
      "\u001b[?25hCollecting transformers==3.4.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 51.0MB/s \n",
      "\u001b[?25hCollecting ftfy==5.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: pycocotools==2.0.2 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (2.0.2)\n",
      "Collecting pytorch-lightning==1.2.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/13/fb401b8f9d9c5e2aa08769d230bb401bf11dee0bc93e069d7337a4201ec8/pytorch_lightning-1.2.7-py3-none-any.whl (830kB)\n",
      "\u001b[K     |████████████████████████████████| 839kB 44.3MB/s \n",
      "\u001b[?25hCollecting torchtext==0.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 2.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.19.5)\n",
      "Collecting demjson==2.2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/67/6db789e2533158963d4af689f961b644ddd9200615b8ce92d6cad695c65a/demjson-2.2.4.tar.gz (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 55.2MB/s \n",
      "\u001b[?25hCollecting torch<=1.8.1,>=1.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/74/6fc9dee50f7c93d6b7d9644554bdc9692f3023fa5d1de779666e6bf8ae76/torch-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (804.1MB)\n",
      "\u001b[K     |████████████████████████████████| 804.1MB 23kB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (1.1.0)\n",
      "Requirement already satisfied: editdistance==0.5.3 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.5.3)\n",
      "Collecting datasets==1.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 55.6MB/s \n",
      "\u001b[?25hCollecting fasttext==0.9.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from mmf==1.0.0rc12) (0.0)\n",
      "Collecting nltk==3.4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 45.1MB/s \n",
      "\u001b[?25hCollecting tqdm<4.50.0,>=4.43.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<=0.9.1,>=0.7.0->mmf==1.0.0rc12) (7.1.2)\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/68/33/cb524f4de298509927b90aa5ee34767b9a2b93e663cf354b2a3efa2b4acd/portalocker-2.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf==2.0.6->mmf==1.0.0rc12) (3.7.4.3)\n",
      "Collecting PyYAML>=5.1.*\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 49.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->mmf==1.0.0rc12) (2.8.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0->mmf==1.0.0rc12) (2020.12.5)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.9.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/e7/edf655ae34925aeaefb7b7fcc3dd0887d2a1203ee6b0df4d1170d1a19d4f/tokenizers-0.9.2-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 48.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (20.9)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.12.4)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001b[K     |████████████████████████████████| 901kB 51.6MB/s \n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 50.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.0.12)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy==5.8->mmf==1.0.0rc12) (0.2.5)\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (56.0.0)\n",
      "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0.2->mmf==1.0.0rc12) (0.29.22)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.2.7->mmf==1.0.0rc12) (2.4.1)\n",
      "Collecting fsspec[http]>=0.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 45.4MB/s \n",
      "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/99/dc59248df9a50349d537ffb3403c1bdc1fa69077109d46feaa0843488001/torchmetrics-0.3.1-py3-none-any.whl (271kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 54.9MB/s \n",
      "\u001b[?25hCollecting future>=0.17.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "\u001b[K     |████████████████████████████████| 829kB 50.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.5.0->mmf==1.0.0rc12) (1.15.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.70.11.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (1.1.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (3.10.1)\n",
      "Collecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 52.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->mmf==1.0.0rc12) (0.3.3)\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (2.6.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->mmf==1.0.0rc12) (0.22.2.post1)\n",
      "Collecting smmap<5,>=3.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (1.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (7.1.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.0.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.32.0)\n",
      "Collecting aiohttp; extra == \"http\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 48.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.2.1->mmf==1.0.0rc12) (2018.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.2.1->mmf==1.0.0rc12) (3.4.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (4.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (1.3.0)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (20.3.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 58.7MB/s \n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 55.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.2.7->mmf==1.0.0rc12) (3.1.0)\n",
      "Building wheels for collected packages: lmdb, ftfy, demjson, fasttext, nltk, future\n",
      "  Building wheel for lmdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=219687 sha256=1ef868f698ef63a93f709be6e40a0c58863863cae3abde10256a64e0d0961107\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/97/8c/7721e4b6b0ac723c6cc45ecca60599a80f75e2367330647390\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-5.8-cp37-none-any.whl size=45613 sha256=da1b6b23135d0e1efe66d1b1ca97897509cbdc2ff1b747411c918fd4bba5e1e2\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
      "  Building wheel for demjson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for demjson: filename=demjson-2.2.4-cp37-none-any.whl size=73546 sha256=10616d68a8d509577bd4406e4a273bc78b104e031f945335740104e23cd5f85c\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/d2/ab/a54fb5ea53ac3badba098160e8452fa126a51febda80440ded\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fasttext: filename=fasttext-0.9.1-cp37-cp37m-linux_x86_64.whl size=2466840 sha256=c3965a33580d2042ead8101369ce31aa932432af89a6c39818a73cb276b5d39f\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449906 sha256=00620cbf96aa60762b4b5bc35a5603c852d5fc01d4cc9bd71ec19cef323d047e\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=0dc29bc1df37bc52816213d4b80a155ee65a063a23053a0fed643efddce4f2da\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built lmdb ftfy demjson fasttext nltk future\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pytorch-lightning 1.2.7 has requirement PyYAML!=5.4.*,>=5.1, but you'll have pyyaml 5.4.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: torch, torchvision, portalocker, tqdm, iopath, PyYAML, omegaconf, matplotlib, lmdb, smmap, gitdb, GitPython, tokenizers, sacremoses, sentencepiece, transformers, ftfy, async-timeout, multidict, yarl, aiohttp, fsspec, torchmetrics, future, pytorch-lightning, torchtext, demjson, xxhash, datasets, fasttext, nltk, mmf\n",
      "  Found existing installation: torch 1.8.1+cu101\n",
      "    Uninstalling torch-1.8.1+cu101:\n",
      "      Successfully uninstalled torch-1.8.1+cu101\n",
      "  Found existing installation: torchvision 0.9.1+cu101\n",
      "    Uninstalling torchvision-0.9.1+cu101:\n",
      "      Successfully uninstalled torchvision-0.9.1+cu101\n",
      "  Found existing installation: tqdm 4.41.1\n",
      "    Uninstalling tqdm-4.41.1:\n",
      "      Successfully uninstalled tqdm-4.41.1\n",
      "  Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "  Found existing installation: matplotlib 3.2.2\n",
      "    Uninstalling matplotlib-3.2.2:\n",
      "      Successfully uninstalled matplotlib-3.2.2\n",
      "  Found existing installation: lmdb 0.99\n",
      "    Uninstalling lmdb-0.99:\n",
      "      Successfully uninstalled lmdb-0.99\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Found existing installation: torchtext 0.9.1\n",
      "    Uninstalling torchtext-0.9.1:\n",
      "      Successfully uninstalled torchtext-0.9.1\n",
      "  Found existing installation: nltk 3.2.5\n",
      "    Uninstalling nltk-3.2.5:\n",
      "      Successfully uninstalled nltk-3.2.5\n",
      "  Running setup.py develop for mmf\n",
      "Successfully installed GitPython-3.1.0 PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.2.1 demjson-2.2.4 fasttext-0.9.1 fsspec-2021.4.0 ftfy-5.8 future-0.18.2 gitdb-4.0.7 iopath-0.1.7 lmdb-0.98 matplotlib-3.3.4 mmf multidict-5.1.0 nltk-3.4.5 omegaconf-2.0.6 portalocker-2.3.0 pytorch-lightning-1.2.7 sacremoses-0.0.45 sentencepiece-0.1.95 smmap-4.0.0 tokenizers-0.9.2 torch-1.8.1 torchmetrics-0.3.1 torchtext-0.5.0 torchvision-0.9.1 tqdm-4.49.0 transformers-3.4.0 xxhash-2.0.2 yarl-1.6.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "matplotlib",
         "mpl_toolkits"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --editable ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 301207,
     "status": "ok",
     "timestamp": 1619591095491,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "w3vQmWX9Fqot",
    "outputId": "d8600f8c-0879-450a-e791-acfa5583a38f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 28 06:24:55 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 307368,
     "status": "ok",
     "timestamp": 1619591101655,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "TyhTfHEcEzy8",
    "outputId": "da315ba5-d292-46d1-969b-2826f627b30c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyYAML==5.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 5.3MB/s \n",
      "\u001b[?25hCollecting imgaug==0.2.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
      "\u001b[K     |████████████████████████████████| 634kB 15.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.4.1)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (0.16.2)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.19.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug==0.2.6) (1.15.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (1.1.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (3.3.4)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.4.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (7.1.2)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.6) (2.5.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug==0.2.6) (2.8.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.6) (4.4.2)\n",
      "Building wheels for collected packages: PyYAML, imgaug\n",
      "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=5d581e03dac6fcfb314e86b2805708353194e6f9853e1622b1bcd9ec058ee5eb\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
      "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for imgaug: filename=imgaug-0.2.6-cp37-none-any.whl size=654019 sha256=14d5685a88dd9341c8eeebbb2b94cab215cfd16c576d134273d30d522b68cedb\n",
      "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
      "Successfully built PyYAML imgaug\n",
      "Installing collected packages: PyYAML, imgaug\n",
      "  Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Found existing installation: imgaug 0.2.9\n",
      "    Uninstalling imgaug-0.2.9:\n",
      "      Successfully uninstalled imgaug-0.2.9\n",
      "Successfully installed PyYAML-5.3.1 imgaug-0.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install PyYAML==5.3.1 imgaug==0.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1619591317757,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "vklppW3ZHsgm",
    "outputId": "e863d313-cade-4395-bfad-4d433fa3a061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/colab\n"
     ]
    }
   ],
   "source": [
    "%cd /content/gdrive/MyDrive/colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206029,
     "status": "ok",
     "timestamp": 1619591526133,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "emeGDbsMKIV-",
    "outputId": "4935538c-a4ec-42f4-a6b4-aca38b669f5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 06:28:42.230472: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Data folder is /root/.cache/torch/mmf/data\n",
      "Zip path is ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Starting checksum for XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Checksum successful\n",
      "Copying ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Unzipping ./XjiOc5ycDBRRNwbhRlgH.zip\n",
      "Extracting the zip can take time. Sit back and relax.\n",
      "Moving train.jsonl\n",
      "Moving dev_seen.jsonl\n",
      "Moving test_seen.jsonl\n",
      "Moving dev_unseen.jsonl\n",
      "Moving test_unseen.jsonl\n",
      "Moving img\n"
     ]
    }
   ],
   "source": [
    "!mmf_convert_hm --zip_file=\"./XjiOc5ycDBRRNwbhRlgH.zip\" --password=REDACTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1464107,
     "status": "ok",
     "timestamp": 1619592807932,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "PaL_WafB7tOc",
    "outputId": "5b95279d-be4c-4619-c2eb-a9ffc6e03d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 06:32:10.331678: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-04-28T06:32:16 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
      "\u001b[32m2021-04-28T06:32:16 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2021-04-28T06:32:16 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-04-28T06:32:16 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2021-04-28T06:32:16 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
      "\u001b[32m2021-04-28T06:32:16 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
      "\u001b[32m2021-04-28T06:32:16 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2021-04-28T06:32:17 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco', 'checkpoint.resume_pretrained=False'])\n",
      "\u001b[32m2021-04-28T06:32:17 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-04-28T06:32:17 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-04-28T06:32:17 | mmf_cli.run: \u001b[0mUsing seed 16168690\n",
      "\u001b[32m2021-04-28T06:32:17 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/features/features_2020_10_01.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/features/features.tar.gz ]\n",
      "Downloading features.tar.gz: 100% 10.3G/10.3G [13:34<00:00, 12.6MB/s]\n",
      "[ Starting checksum for features.tar.gz]\n",
      "[ Checksum successful for features.tar.gz]\n",
      "Unpacking features.tar.gz\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n",
      "Downloading extras.tar.gz: 100% 211k/211k [00:01<00:00, 166kB/s] \n",
      "[ Starting checksum for extras.tar.gz]\n",
      "[ Checksum successful for extras.tar.gz]\n",
      "Unpacking extras.tar.gz\n",
      "\u001b[32m2021-04-28T06:51:48 | filelock: \u001b[0mLock 140701911996688 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "Downloading: 100% 433/433 [00:00<00:00, 352kB/s]\n",
      "\u001b[32m2021-04-28T06:51:49 | filelock: \u001b[0mLock 140701911996688 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
      "\u001b[32m2021-04-28T06:51:49 | filelock: \u001b[0mLock 140702130020816 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 324kB/s]\n",
      "\u001b[32m2021-04-28T06:51:51 | filelock: \u001b[0mLock 140702130020816 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:51:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:51:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T06:51:51 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:51:51 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:51:51 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:51:51 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "\u001b[32m2021-04-28T06:51:51 | filelock: \u001b[0mLock 140701905443024 acquired on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Downloading: 100% 440M/440M [00:06<00:00, 67.2MB/s]\n",
      "\u001b[32m2021-04-28T06:51:58 | filelock: \u001b[0mLock 140701905443024 released on /root/.cache/torch/mmf/distributed_-1/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-04-28T06:52:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-04-28T06:52:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:52:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:52:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-04-28T06:52:08 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.finetuned.hateful_memes_from_coco.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.finetuned.hateful_memes.from_coco/visual_bert.finetuned.hateful_memes_from_coco.tar.gz ]\n",
      "Downloading visual_bert.finetuned.hateful_memes_from_coco.tar.gz: 100% 414M/414M [00:34<00:00, 11.9MB/s]\n",
      "[ Starting checksum for visual_bert.finetuned.hateful_memes_from_coco.tar.gz]\n",
      "[ Checksum successful for visual_bert.finetuned.hateful_memes_from_coco.tar.gz]\n",
      "Unpacking visual_bert.finetuned.hateful_memes_from_coco.tar.gz\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:52:51 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:52:51 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:52:51 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:52:51 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:52:51 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
      "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
      "Unexpected keys if any: []\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:52:52 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:52:52 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:304: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:52:52 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:52:52 | py.warnings: \u001b[0m/content/gdrive/.shortcut-targets-by-id/11TjgvwNkpvsWJ3BXB8-IAoEIg_91lJZ4/mmf/mmf/utils/checkpoint.py:347: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2021-04-28T06:52:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-04-28T06:52:52 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-04-28T06:52:52 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-04-28T06:52:52 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-04-28T06:52:52 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-04-28T06:52:52 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-04-28T06:52:52 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2021-04-28T06:52:52 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-04-28T06:52:52 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100% 9/9 [00:33<00:00,  3.76s/it]\n",
      "\u001b[32m2021-04-28T06:53:26 | mmf.trainers.callbacks.logistics: \u001b[0mval/hateful_memes/cross_entropy: 1.4743, val/total_loss: 1.4743, val/hateful_memes/accuracy: 0.6704, val/hateful_memes/binary_f1: 0.4183, val/hateful_memes/roc_auc: 0.6996\n",
      "\u001b[32m2021-04-28T06:53:26 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01m 17s 725ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/from_coco.yaml model=visual_bert dataset=hateful_memes run_type=val checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco  checkpoint.resume_pretrained=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 1430720,
     "status": "ok",
     "timestamp": 1619592808963,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "e4HQ9P0RaiTS"
   },
   "outputs": [],
   "source": [
    "!cp /content/gdrive/MyDrive/colab/train_hateful_and_election.jsonl /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/annotations/train_hateful_and_election.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krvHhPN6r7-6"
   },
   "source": [
    "Using already fine tuned Visual Bert COCO model on Hateful memes. Fine-tuning again on all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24096777,
     "status": "ok",
     "timestamp": 1619615479000,
     "user": {
      "displayName": "Cathy Wang",
      "photoUrl": "",
      "userId": "09711870337723513287"
     },
     "user_tz": 240
    },
    "id": "0swUuu2jJ5sl",
    "outputId": "03319c33-1f74-435c-8042-190fa39e364e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 06:53:32.540154: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to /content/gdrive/MyDrive/colab/finetuned_visualbertcoco_election_memes/\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.train[0] to hateful_memes/defaults/annotations/train_hateful_and_election.jsonl\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_seen.jsonl\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.train[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.val[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.features.test[0] to /content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf: \u001b[0mLogging to: /content/gdrive/MyDrive/colab/finetuned_visualbertcoco_election_memes/train.log\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=train_val', 'training.batch_size=32', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco', 'checkpoint.resume_pretrained=True', 'env.save_dir=/content/gdrive/MyDrive/colab/finetuned_visualbertcoco_election_memes/', 'dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_hateful_and_election.jsonl', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl', 'dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb', 'dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb'])\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf_cli.run: \u001b[0mTorch version: 1.8.1+cu102\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf_cli.run: \u001b[0mUsing seed 38526689\n",
      "\u001b[32m2021-04-28T06:53:38 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:53:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:53:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T06:53:41 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:53:41 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:53:41 | mmf.datasets.multi_datamodule: \u001b[0mMultitasking disabled by default for single dataset training\n",
      "\u001b[32m2021-04-28T06:53:41 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2021-04-28T06:53:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2021-04-28T06:53:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:53:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:53:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/basecontainer.py:232: UserWarning: cfg.pretty() is deprecated and will be removed in a future version.\n",
      "Use OmegaConf.to_yaml(cfg)\n",
      "\n",
      "  category=UserWarning,\n",
      "\n",
      "\u001b[32m2021-04-28T06:53:47 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:53:49 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:53:49 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:53:49 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T06:53:49 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
      "  (model): VisualBERTForClassification(\n",
      "    (bert): VisualBERTBase(\n",
      "      (embeddings): BertVisioLinguisticEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (token_type_embeddings_visual): Embedding(2, 768)\n",
      "        (position_embeddings_visual): Embedding(512, 768)\n",
      "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder): BertEncoderJit(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayerJit(\n",
      "            (attention): BertAttentionJit(\n",
      "              (self): BertSelfAttentionJit(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (classifier): Sequential(\n",
      "      (0): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses(\n",
      "    (losses): ModuleList(\n",
      "      (0): MMFLoss(\n",
      "        (loss_criterion): CrossEntropyLoss(\n",
      "          (loss_fn): CrossEntropyLoss()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
      "\u001b[32m2021-04-28T06:53:49 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[32m2021-04-28T06:59:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/hateful_memes/cross_entropy: 0.5287, train/hateful_memes/cross_entropy/avg: 0.5287, train/total_loss: 0.5287, train/total_loss/avg: 0.5287, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 0.30, time: 05m 28s 754ms, time_since_start: 05m 30s 464ms, eta: 20h 19m 09s 305ms\n",
      "\u001b[32m2021-04-28T07:00:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/hateful_memes/cross_entropy: 0.4047, train/hateful_memes/cross_entropy/avg: 0.4667, train/total_loss: 0.4047, train/total_loss/avg: 0.4667, max mem: 9172.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 149ms, time_since_start: 06m 59s 613ms, eta: 05h 29m 05s 516ms\n",
      "\u001b[32m2021-04-28T07:02:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/hateful_memes/cross_entropy: 0.4047, train/hateful_memes/cross_entropy/avg: 0.4413, train/total_loss: 0.4047, train/total_loss/avg: 0.4413, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0.00001, ups: 1.09, time: 01m 32s 315ms, time_since_start: 08m 31s 929ms, eta: 05h 39m 13s 083ms\n",
      "\u001b[32m2021-04-28T07:03:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/hateful_memes/cross_entropy: 0.3906, train/hateful_memes/cross_entropy/avg: 0.4119, train/total_loss: 0.3906, train/total_loss/avg: 0.4119, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 017ms, time_since_start: 09m 59s 947ms, eta: 05h 21m 55s 967ms\n",
      "\u001b[32m2021-04-28T07:05:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/hateful_memes/cross_entropy: 0.3906, train/hateful_memes/cross_entropy/avg: 0.3873, train/total_loss: 0.3906, train/total_loss/avg: 0.3873, max mem: 9172.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 863ms, time_since_start: 11m 28s 810ms, eta: 05h 23m 31s 265ms\n",
      "\u001b[32m2021-04-28T07:06:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/hateful_memes/cross_entropy: 0.3235, train/hateful_memes/cross_entropy/avg: 0.3386, train/total_loss: 0.3235, train/total_loss/avg: 0.3386, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 121ms, time_since_start: 13m 932ms, eta: 05h 33m 49s 466ms\n",
      "\u001b[32m2021-04-28T07:08:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/hateful_memes/cross_entropy: 0.3235, train/hateful_memes/cross_entropy/avg: 0.3283, train/total_loss: 0.3235, train/total_loss/avg: 0.3283, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0.00002, ups: 1.15, time: 01m 27s 532ms, time_since_start: 14m 28s 465ms, eta: 05h 15m 42s 817ms\n",
      "\u001b[32m2021-04-28T07:09:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/hateful_memes/cross_entropy: 0.2889, train/hateful_memes/cross_entropy/avg: 0.3174, train/total_loss: 0.2889, train/total_loss/avg: 0.3174, max mem: 9172.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 389ms, time_since_start: 15m 57s 854ms, eta: 05h 20m 53s 821ms\n",
      "\u001b[32m2021-04-28T07:11:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/hateful_memes/cross_entropy: 0.2889, train/hateful_memes/cross_entropy/avg: 0.3003, train/total_loss: 0.2889, train/total_loss/avg: 0.3003, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 734ms, time_since_start: 17m 29s 589ms, eta: 05h 27m 45s 601ms\n",
      "\u001b[32m2021-04-28T07:12:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T07:12:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:20:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:20:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:20:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/hateful_memes/cross_entropy: 0.2665, train/hateful_memes/cross_entropy/avg: 0.2946, train/total_loss: 0.2665, train/total_loss/avg: 0.2946, max mem: 9172.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00003, ups: 0.18, time: 09m 29s 400ms, time_since_start: 26m 58s 989ms, eta: 33h 44m 47s 221ms\n",
      "\u001b[32m2021-04-28T07:20:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T07:20:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:20:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:20:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:21:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:21:20 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-04-28T07:21:32 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:21:51 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:21:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/hateful_memes/cross_entropy: 1.1964, val/total_loss: 1.1964, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5778, val/hateful_memes/roc_auc: 0.7019, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 01m 04s 849ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.701941\n",
      "\u001b[32m2021-04-28T07:23:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/hateful_memes/cross_entropy: 0.2665, train/hateful_memes/cross_entropy/avg: 0.2746, train/total_loss: 0.2665, train/total_loss/avg: 0.2746, max mem: 9226.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00003, ups: 1.06, time: 01m 34s 995ms, time_since_start: 29m 38s 838ms, eta: 05h 36m 11s 767ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:24:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:24:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:24:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/hateful_memes/cross_entropy: 0.2433, train/hateful_memes/cross_entropy/avg: 0.2618, train/total_loss: 0.2433, train/total_loss/avg: 0.2618, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 268ms, time_since_start: 31m 10s 107ms, eta: 05h 21m 27s 650ms\n",
      "\u001b[32m2021-04-28T07:26:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/hateful_memes/cross_entropy: 0.2433, train/hateful_memes/cross_entropy/avg: 0.2456, train/total_loss: 0.2433, train/total_loss/avg: 0.2456, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 143ms, time_since_start: 32m 38s 251ms, eta: 05h 08m 57s 709ms\n",
      "\u001b[32m2021-04-28T07:27:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/hateful_memes/cross_entropy: 0.2406, train/hateful_memes/cross_entropy/avg: 0.2367, train/total_loss: 0.2406, train/total_loss/avg: 0.2367, max mem: 9226.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 372ms, time_since_start: 34m 06s 625ms, eta: 05h 08m 16s 200ms\n",
      "\u001b[32m2021-04-28T07:29:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/hateful_memes/cross_entropy: 0.2406, train/hateful_memes/cross_entropy/avg: 0.2331, train/total_loss: 0.2406, train/total_loss/avg: 0.2331, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 714ms, time_since_start: 35m 38s 339ms, eta: 05h 18m 22s 200ms\n",
      "\u001b[32m2021-04-28T07:30:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/hateful_memes/cross_entropy: 0.1837, train/hateful_memes/cross_entropy/avg: 0.2240, train/total_loss: 0.1837, train/total_loss/avg: 0.2240, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 073ms, time_since_start: 37m 07s 413ms, eta: 05h 07m 41s 830ms\n",
      "\u001b[32m2021-04-28T07:32:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/hateful_memes/cross_entropy: 0.1837, train/hateful_memes/cross_entropy/avg: 0.2137, train/total_loss: 0.1837, train/total_loss/avg: 0.2137, max mem: 9226.0, experiment: run, epoch: 6, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 950ms, time_since_start: 38m 36s 363ms, eta: 05h 05m 45s 780ms\n",
      "\u001b[32m2021-04-28T07:33:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/hateful_memes/cross_entropy: 0.1640, train/hateful_memes/cross_entropy/avg: 0.2031, train/total_loss: 0.1640, train/total_loss/avg: 0.2031, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 944ms, time_since_start: 40m 07s 307ms, eta: 05h 11m 04s 644ms\n",
      "\u001b[32m2021-04-28T07:35:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/hateful_memes/cross_entropy: 0.1640, train/hateful_memes/cross_entropy/avg: 0.2007, train/total_loss: 0.1640, train/total_loss/avg: 0.2007, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 865ms, time_since_start: 41m 36s 172ms, eta: 05h 02m 27s 696ms\n",
      "\u001b[32m2021-04-28T07:36:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T07:36:52 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:37:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:37:13 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:37:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/hateful_memes/cross_entropy: 0.1582, train/hateful_memes/cross_entropy/avg: 0.1915, train/total_loss: 0.1582, train/total_loss/avg: 0.1915, max mem: 9226.0, experiment: run, epoch: 7, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00005, ups: 0.92, time: 01m 49s 159ms, time_since_start: 43m 25s 332ms, eta: 06h 09m 41s 257ms\n",
      "\u001b[32m2021-04-28T07:37:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T07:37:13 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:37:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:37:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:37:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:37:54 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-04-28T07:38:05 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:38:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:38:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/hateful_memes/cross_entropy: 1.5703, val/total_loss: 1.5703, val/hateful_memes/accuracy: 0.6540, val/hateful_memes/binary_f1: 0.5929, val/hateful_memes/roc_auc: 0.7138, num_updates: 2000, epoch: 7, iterations: 2000, max_updates: 22000, val_time: 01m 08s 225ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.713823\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:39:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:39:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:40:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/hateful_memes/cross_entropy: 0.1216, train/hateful_memes/cross_entropy/avg: 0.1880, train/total_loss: 0.1216, train/total_loss/avg: 0.1880, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00005, ups: 0.76, time: 02m 11s 153ms, time_since_start: 46m 44s 715ms, eta: 07h 21m 57s 106ms\n",
      "\u001b[32m2021-04-28T07:42:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/hateful_memes/cross_entropy: 0.1208, train/hateful_memes/cross_entropy/avg: 0.1811, train/total_loss: 0.1208, train/total_loss/avg: 0.1811, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 293ms, time_since_start: 48m 13s 008ms, eta: 04h 56m 01s 757ms\n",
      "\u001b[32m2021-04-28T07:43:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/hateful_memes/cross_entropy: 0.1166, train/hateful_memes/cross_entropy/avg: 0.1748, train/total_loss: 0.1166, train/total_loss/avg: 0.1748, max mem: 9226.0, experiment: run, epoch: 8, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 192ms, time_since_start: 49m 41s 200ms, eta: 04h 54m 11s 869ms\n",
      "\u001b[32m2021-04-28T07:44:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/hateful_memes/cross_entropy: 0.0954, train/hateful_memes/cross_entropy/avg: 0.1685, train/total_loss: 0.0954, train/total_loss/avg: 0.1685, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 086ms, time_since_start: 51m 11s 286ms, eta: 04h 58m 59s 410ms\n",
      "\u001b[32m2021-04-28T07:46:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/hateful_memes/cross_entropy: 0.0954, train/hateful_memes/cross_entropy/avg: 0.1668, train/total_loss: 0.0954, train/total_loss/avg: 0.1668, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00005, ups: 1.15, time: 01m 27s 457ms, time_since_start: 52m 38s 744ms, eta: 04h 48m 47s 165ms\n",
      "\u001b[32m2021-04-28T07:47:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/hateful_memes/cross_entropy: 0.1115, train/hateful_memes/cross_entropy/avg: 0.1646, train/total_loss: 0.1115, train/total_loss/avg: 0.1646, max mem: 9226.0, experiment: run, epoch: 9, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00005, ups: 1.15, time: 01m 27s 979ms, time_since_start: 54m 06s 723ms, eta: 04h 49m 01s 028ms\n",
      "\u001b[32m2021-04-28T07:49:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/hateful_memes/cross_entropy: 0.0862, train/hateful_memes/cross_entropy/avg: 0.1598, train/total_loss: 0.0862, train/total_loss/avg: 0.1598, max mem: 9226.0, experiment: run, epoch: 10, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 432ms, time_since_start: 55m 37s 156ms, eta: 04h 55m 32s 720ms\n",
      "\u001b[32m2021-04-28T07:50:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/hateful_memes/cross_entropy: 0.0743, train/hateful_memes/cross_entropy/avg: 0.1542, train/total_loss: 0.0743, train/total_loss/avg: 0.1542, max mem: 9226.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00005, ups: 1.15, time: 01m 27s 988ms, time_since_start: 57m 05s 145ms, eta: 04h 46m 04s 176ms\n",
      "\u001b[32m2021-04-28T07:52:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/hateful_memes/cross_entropy: 0.0743, train/hateful_memes/cross_entropy/avg: 0.1548, train/total_loss: 0.0743, train/total_loss/avg: 0.1548, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 989ms, time_since_start: 58m 36s 135ms, eta: 04h 54m 17s 134ms\n",
      "\u001b[32m2021-04-28T07:53:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T07:53:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:54:02 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:54:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:54:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/hateful_memes/cross_entropy: 0.0504, train/hateful_memes/cross_entropy/avg: 0.1498, train/total_loss: 0.0504, train/total_loss/avg: 0.1498, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00005, ups: 0.91, time: 01m 50s 437ms, time_since_start: 01h 26s 573ms, eta: 05h 55m 18s 925ms\n",
      "\u001b[32m2021-04-28T07:54:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T07:54:14 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:54:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:54:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:54:40 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T07:54:49 | mmf.utils.checkpoint: \u001b[0mSaving best checkpoint\n",
      "\u001b[32m2021-04-28T07:54:59 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T07:55:16 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T07:55:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 2.1685, val/total_loss: 2.1685, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5649, val/hateful_memes/roc_auc: 0.7513, num_updates: 3000, epoch: 11, iterations: 3000, max_updates: 22000, val_time: 01m 02s 322ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T07:57:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/hateful_memes/cross_entropy: 0.0489, train/hateful_memes/cross_entropy/avg: 0.1455, train/total_loss: 0.0489, train/total_loss/avg: 0.1455, max mem: 9226.0, experiment: run, epoch: 11, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00005, ups: 0.78, time: 02m 09s 780ms, time_since_start: 01h 03m 38s 680ms, eta: 06h 55m 20s 892ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:58:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T07:58:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T07:58:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/hateful_memes/cross_entropy: 0.0372, train/hateful_memes/cross_entropy/avg: 0.1410, train/total_loss: 0.0372, train/total_loss/avg: 0.1410, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00005, ups: 1.10, time: 01m 31s 534ms, time_since_start: 01h 05m 10s 215ms, eta: 04h 51m 23s 840ms\n",
      "\u001b[32m2021-04-28T08:00:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/hateful_memes/cross_entropy: 0.0370, train/hateful_memes/cross_entropy/avg: 0.1368, train/total_loss: 0.0370, train/total_loss/avg: 0.1368, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00005, ups: 1.16, time: 01m 26s 707ms, time_since_start: 01h 06m 36s 923ms, eta: 04h 34m 33s 822ms\n",
      "\u001b[32m2021-04-28T08:01:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/hateful_memes/cross_entropy: 0.0370, train/hateful_memes/cross_entropy/avg: 0.1359, train/total_loss: 0.0370, train/total_loss/avg: 0.1359, max mem: 9226.0, experiment: run, epoch: 12, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 186ms, time_since_start: 01h 08m 05s 110ms, eta: 04h 37m 45s 215ms\n",
      "\u001b[32m2021-04-28T08:03:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/hateful_memes/cross_entropy: 0.0337, train/hateful_memes/cross_entropy/avg: 0.1321, train/total_loss: 0.0337, train/total_loss/avg: 0.1321, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 753ms, time_since_start: 01h 09m 35s 863ms, eta: 04h 44m 17s 968ms\n",
      "\u001b[32m2021-04-28T08:04:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/hateful_memes/cross_entropy: 0.0232, train/hateful_memes/cross_entropy/avg: 0.1286, train/total_loss: 0.0232, train/total_loss/avg: 0.1286, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00005, ups: 1.15, time: 01m 27s 886ms, time_since_start: 01h 11m 03s 750ms, eta: 04h 33m 49s 932ms\n",
      "\u001b[32m2021-04-28T08:06:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/hateful_memes/cross_entropy: 0.0232, train/hateful_memes/cross_entropy/avg: 0.1313, train/total_loss: 0.0232, train/total_loss/avg: 0.1313, max mem: 9226.0, experiment: run, epoch: 13, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00005, ups: 1.15, time: 01m 27s 699ms, time_since_start: 01h 12m 31s 450ms, eta: 04h 31m 45s 867ms\n",
      "\u001b[32m2021-04-28T08:07:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/hateful_memes/cross_entropy: 0.0337, train/hateful_memes/cross_entropy/avg: 0.1293, train/total_loss: 0.0337, train/total_loss/avg: 0.1293, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00005, ups: 1.11, time: 01m 30s 485ms, time_since_start: 01h 14m 01s 935ms, eta: 04h 38m 51s 786ms\n",
      "\u001b[32m2021-04-28T08:09:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/hateful_memes/cross_entropy: 0.0231, train/hateful_memes/cross_entropy/avg: 0.1260, train/total_loss: 0.0231, train/total_loss/avg: 0.1260, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00005, ups: 1.14, time: 01m 28s 280ms, time_since_start: 01h 15m 30s 216ms, eta: 04h 30m 34s 517ms\n",
      "\u001b[32m2021-04-28T08:10:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T08:10:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T08:10:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T08:11:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T08:11:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/hateful_memes/cross_entropy: 0.0231, train/hateful_memes/cross_entropy/avg: 0.1232, train/total_loss: 0.0231, train/total_loss/avg: 0.1232, max mem: 9226.0, experiment: run, epoch: 14, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00005, ups: 0.93, time: 01m 48s 897ms, time_since_start: 01h 17m 19s 114ms, eta: 05h 31m 55s 203ms\n",
      "\u001b[32m2021-04-28T08:11:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T08:11:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:11:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:11:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:11:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T08:11:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T08:12:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T08:12:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/hateful_memes/cross_entropy: 2.2607, val/total_loss: 2.2607, val/hateful_memes/accuracy: 0.6400, val/hateful_memes/binary_f1: 0.5408, val/hateful_memes/roc_auc: 0.7293, num_updates: 4000, epoch: 14, iterations: 4000, max_updates: 22000, val_time: 54s 286ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:12:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:12:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:13:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/hateful_memes/cross_entropy: 0.0178, train/hateful_memes/cross_entropy/avg: 0.1202, train/total_loss: 0.0178, train/total_loss/avg: 0.1202, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00004, ups: 0.91, time: 01m 50s 315ms, time_since_start: 01h 20m 03s 719ms, eta: 05h 34m 22s 346ms\n",
      "\u001b[32m2021-04-28T08:15:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/hateful_memes/cross_entropy: 0.0178, train/hateful_memes/cross_entropy/avg: 0.1179, train/total_loss: 0.0178, train/total_loss/avg: 0.1179, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 060ms, time_since_start: 01h 21m 31s 779ms, eta: 04h 25m 25s 496ms\n",
      "\u001b[32m2021-04-28T08:16:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/hateful_memes/cross_entropy: 0.0109, train/hateful_memes/cross_entropy/avg: 0.1154, train/total_loss: 0.0109, train/total_loss/avg: 0.1154, max mem: 9226.0, experiment: run, epoch: 15, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00004, ups: 1.15, time: 01m 27s 838ms, time_since_start: 01h 22m 59s 618ms, eta: 04h 23m 16s 240ms\n",
      "\u001b[32m2021-04-28T08:18:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1128, train/total_loss: 0.0073, train/total_loss/avg: 0.1128, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 706ms, time_since_start: 01h 24m 29s 324ms, eta: 04h 27m 20s 939ms\n",
      "\u001b[32m2021-04-28T08:19:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1121, train/total_loss: 0.0073, train/total_loss/avg: 0.1121, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00004, ups: 1.15, time: 01m 27s 026ms, time_since_start: 01h 25m 56s 351ms, eta: 04h 17m 53s 289ms\n",
      "\u001b[32m2021-04-28T08:21:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/hateful_memes/cross_entropy: 0.0072, train/hateful_memes/cross_entropy/avg: 0.1097, train/total_loss: 0.0072, train/total_loss/avg: 0.1097, max mem: 9226.0, experiment: run, epoch: 16, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00004, ups: 1.15, time: 01m 27s 952ms, time_since_start: 01h 27m 24s 303ms, eta: 04h 19m 08s 587ms\n",
      "\u001b[32m2021-04-28T08:22:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/hateful_memes/cross_entropy: 0.0043, train/hateful_memes/cross_entropy/avg: 0.1074, train/total_loss: 0.0043, train/total_loss/avg: 0.1074, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 403ms, time_since_start: 01h 28m 53s 707ms, eta: 04h 21m 54s 331ms\n",
      "\u001b[32m2021-04-28T08:24:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/hateful_memes/cross_entropy: 0.0072, train/hateful_memes/cross_entropy/avg: 0.1057, train/total_loss: 0.0072, train/total_loss/avg: 0.1057, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 009ms, time_since_start: 01h 30m 21s 717ms, eta: 04h 16m 19s 908ms\n",
      "\u001b[32m2021-04-28T08:25:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/hateful_memes/cross_entropy: 0.0072, train/hateful_memes/cross_entropy/avg: 0.1054, train/total_loss: 0.0072, train/total_loss/avg: 0.1054, max mem: 9226.0, experiment: run, epoch: 17, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00004, ups: 1.15, time: 01m 27s 630ms, time_since_start: 01h 31m 49s 348ms, eta: 04h 13m 44s 645ms\n",
      "\u001b[32m2021-04-28T08:27:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T08:27:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T08:27:18 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T08:27:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T08:27:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1055, train/total_loss: 0.0073, train/total_loss/avg: 0.1055, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00004, ups: 0.89, time: 01m 52s 730ms, time_since_start: 01h 33m 42s 078ms, eta: 05h 24m 30s 818ms\n",
      "\u001b[32m2021-04-28T08:27:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T08:27:29 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:27:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:27:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:27:59 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T08:28:10 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T08:28:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T08:28:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/hateful_memes/cross_entropy: 1.9787, val/total_loss: 1.9787, val/hateful_memes/accuracy: 0.6540, val/hateful_memes/binary_f1: 0.5620, val/hateful_memes/roc_auc: 0.7383, num_updates: 5000, epoch: 18, iterations: 5000, max_updates: 22000, val_time: 49s 708ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T08:30:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1037, train/total_loss: 0.0073, train/total_loss/avg: 0.1037, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00004, ups: 0.92, time: 01m 49s 424ms, time_since_start: 01h 36m 21s 217ms, eta: 05h 13m 08s 710ms\n",
      "\u001b[32m2021-04-28T08:31:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.1018, train/total_loss: 0.0073, train/total_loss/avg: 0.1018, max mem: 9226.0, experiment: run, epoch: 18, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 736ms, time_since_start: 01h 37m 49s 954ms, eta: 04h 12m 26s 296ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:31:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:31:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:33:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/hateful_memes/cross_entropy: 0.0097, train/hateful_memes/cross_entropy/avg: 0.1002, train/total_loss: 0.0097, train/total_loss/avg: 0.1002, max mem: 9226.0, experiment: run, epoch: 19, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 501ms, time_since_start: 01h 39m 21s 455ms, eta: 04h 18m 45s 207ms\n",
      "\u001b[32m2021-04-28T08:34:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.0985, train/total_loss: 0.0073, train/total_loss/avg: 0.0985, max mem: 9226.0, experiment: run, epoch: 19, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 931ms, time_since_start: 01h 40m 50s 387ms, eta: 04h 09m 58s 863ms\n",
      "\u001b[32m2021-04-28T08:36:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.0968, train/total_loss: 0.0073, train/total_loss/avg: 0.0968, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00004, ups: 1.09, time: 01m 32s 358ms, time_since_start: 01h 42m 22s 746ms, eta: 04h 18m 03s 024ms\n",
      "\u001b[32m2021-04-28T08:37:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/hateful_memes/cross_entropy: 0.0097, train/hateful_memes/cross_entropy/avg: 0.0981, train/total_loss: 0.0097, train/total_loss/avg: 0.0981, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 003ms, time_since_start: 01h 43m 50s 749ms, eta: 04h 04m 23s 479ms\n",
      "\u001b[32m2021-04-28T08:39:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/hateful_memes/cross_entropy: 0.0097, train/hateful_memes/cross_entropy/avg: 0.0989, train/total_loss: 0.0097, train/total_loss/avg: 0.0989, max mem: 9226.0, experiment: run, epoch: 20, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 270ms, time_since_start: 01h 45m 19s 019ms, eta: 04h 03m 38s 222ms\n",
      "\u001b[32m2021-04-28T08:40:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.0972, train/total_loss: 0.0073, train/total_loss/avg: 0.0972, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 055ms, time_since_start: 01h 46m 50s 075ms, eta: 04h 09m 47s 096ms\n",
      "\u001b[32m2021-04-28T08:42:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/hateful_memes/cross_entropy: 0.0097, train/hateful_memes/cross_entropy/avg: 0.0958, train/total_loss: 0.0097, train/total_loss/avg: 0.0958, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00004, ups: 1.15, time: 01m 27s 972ms, time_since_start: 01h 48m 18s 048ms, eta: 03h 59m 50s 164ms\n",
      "\u001b[32m2021-04-28T08:43:35 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T08:43:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T08:43:44 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T08:43:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T08:43:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.0942, train/total_loss: 0.0073, train/total_loss/avg: 0.0942, max mem: 9226.0, experiment: run, epoch: 21, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00004, ups: 0.91, time: 01m 50s 963ms, time_since_start: 01h 50m 09s 011ms, eta: 05h 38s 247ms\n",
      "\u001b[32m2021-04-28T08:43:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T08:43:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:43:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:43:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:44:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T08:44:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T08:44:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T08:44:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/hateful_memes/cross_entropy: 1.7981, val/total_loss: 1.7981, val/hateful_memes/accuracy: 0.6560, val/hateful_memes/binary_f1: 0.5635, val/hateful_memes/roc_auc: 0.7222, num_updates: 6000, epoch: 21, iterations: 6000, max_updates: 22000, val_time: 57s 982ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:46:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T08:46:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T08:46:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/hateful_memes/cross_entropy: 0.0097, train/hateful_memes/cross_entropy/avg: 0.0940, train/total_loss: 0.0097, train/total_loss/avg: 0.0940, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00004, ups: 0.85, time: 01m 58s 863ms, time_since_start: 01h 53m 05s 859ms, eta: 05h 20m 01s 699ms\n",
      "\u001b[32m2021-04-28T08:48:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.0925, train/total_loss: 0.0073, train/total_loss/avg: 0.0925, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 422ms, time_since_start: 01h 54m 34s 282ms, eta: 03h 56m 34s 240ms\n",
      "\u001b[32m2021-04-28T08:49:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.0911, train/total_loss: 0.0073, train/total_loss/avg: 0.0911, max mem: 9226.0, experiment: run, epoch: 22, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00004, ups: 1.12, time: 01m 29s 132ms, time_since_start: 01h 56m 03s 415ms, eta: 03h 56m 57s 758ms\n",
      "\u001b[32m2021-04-28T08:51:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/hateful_memes/cross_entropy: 0.0073, train/hateful_memes/cross_entropy/avg: 0.0896, train/total_loss: 0.0073, train/total_loss/avg: 0.0896, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 164ms, time_since_start: 01h 57m 34s 580ms, eta: 04h 49s 278ms\n",
      "\u001b[32m2021-04-28T08:52:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/hateful_memes/cross_entropy: 0.0063, train/hateful_memes/cross_entropy/avg: 0.0883, train/total_loss: 0.0063, train/total_loss/avg: 0.0883, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 532ms, time_since_start: 01h 59m 03s 112ms, eta: 03h 52m 22s 082ms\n",
      "\u001b[32m2021-04-28T08:54:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/hateful_memes/cross_entropy: 0.0063, train/hateful_memes/cross_entropy/avg: 0.0871, train/total_loss: 0.0063, train/total_loss/avg: 0.0871, max mem: 9226.0, experiment: run, epoch: 23, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 734ms, time_since_start: 02h 31s 847ms, eta: 03h 51m 23s 796ms\n",
      "\u001b[32m2021-04-28T08:55:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/hateful_memes/cross_entropy: 0.0063, train/hateful_memes/cross_entropy/avg: 0.0858, train/total_loss: 0.0063, train/total_loss/avg: 0.0858, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 082ms, time_since_start: 02h 02m 02s 929ms, eta: 03h 55m 58s 532ms\n",
      "\u001b[32m2021-04-28T08:57:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/hateful_memes/cross_entropy: 0.0063, train/hateful_memes/cross_entropy/avg: 0.0845, train/total_loss: 0.0063, train/total_loss/avg: 0.0845, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 504ms, time_since_start: 02h 03m 31s 434ms, eta: 03h 47m 47s 966ms\n",
      "\u001b[32m2021-04-28T08:58:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/hateful_memes/cross_entropy: 0.0034, train/hateful_memes/cross_entropy/avg: 0.0833, train/total_loss: 0.0034, train/total_loss/avg: 0.0833, max mem: 9226.0, experiment: run, epoch: 24, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 198ms, time_since_start: 02h 04m 59s 632ms, eta: 03h 45m 30s 994ms\n",
      "\u001b[32m2021-04-28T09:00:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T09:00:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T09:00:29 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T09:00:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T09:00:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/hateful_memes/cross_entropy: 0.0034, train/hateful_memes/cross_entropy/avg: 0.0825, train/total_loss: 0.0034, train/total_loss/avg: 0.0825, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00004, ups: 0.88, time: 01m 53s 835ms, time_since_start: 02h 06m 53s 467ms, eta: 04h 49m 08s 472ms\n",
      "\u001b[32m2021-04-28T09:00:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T09:00:41 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:00:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:00:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:01:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T09:01:22 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T09:01:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T09:01:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/hateful_memes/cross_entropy: 2.6055, val/total_loss: 2.6055, val/hateful_memes/accuracy: 0.6680, val/hateful_memes/binary_f1: 0.5911, val/hateful_memes/roc_auc: 0.7219, num_updates: 7000, epoch: 25, iterations: 7000, max_updates: 22000, val_time: 52s 015ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T09:03:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/hateful_memes/cross_entropy: 0.0034, train/hateful_memes/cross_entropy/avg: 0.0839, train/total_loss: 0.0034, train/total_loss/avg: 0.0839, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00004, ups: 1.08, time: 01m 33s 454ms, time_since_start: 02h 09m 18s 944ms, eta: 03h 55m 47s 502ms\n",
      "\u001b[32m2021-04-28T09:04:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/hateful_memes/cross_entropy: 0.0027, train/hateful_memes/cross_entropy/avg: 0.0827, train/total_loss: 0.0027, train/total_loss/avg: 0.0827, max mem: 9226.0, experiment: run, epoch: 25, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 752ms, time_since_start: 02h 10m 47s 697ms, eta: 03h 42m 25s 548ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:04:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:04:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:06:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.0816, train/total_loss: 0.0019, train/total_loss/avg: 0.0816, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00004, ups: 1.11, time: 01m 30s 924ms, time_since_start: 02h 12m 18s 621ms, eta: 03h 46m 19s 736ms\n",
      "\u001b[32m2021-04-28T09:07:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.0814, train/total_loss: 0.0019, train/total_loss/avg: 0.0814, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 255ms, time_since_start: 02h 13m 46s 877ms, eta: 03h 38m 11s 527ms\n",
      "\u001b[32m2021-04-28T09:09:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0803, train/total_loss: 0.0013, train/total_loss/avg: 0.0803, max mem: 9226.0, experiment: run, epoch: 26, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 727ms, time_since_start: 02h 15m 15s 604ms, eta: 03h 37m 51s 271ms\n",
      "\u001b[32m2021-04-28T09:10:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0794, train/total_loss: 0.0013, train/total_loss/avg: 0.0794, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00004, ups: 1.10, time: 01m 31s 627ms, time_since_start: 02h 16m 47s 231ms, eta: 03h 43m 25s 404ms\n",
      "\u001b[32m2021-04-28T09:12:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.0786, train/total_loss: 0.0013, train/total_loss/avg: 0.0786, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 610ms, time_since_start: 02h 18m 15s 842ms, eta: 03h 34m 34s 060ms\n",
      "\u001b[32m2021-04-28T09:13:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.0776, train/total_loss: 0.0019, train/total_loss/avg: 0.0776, max mem: 9226.0, experiment: run, epoch: 27, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00004, ups: 1.14, time: 01m 28s 669ms, time_since_start: 02h 19m 44s 511ms, eta: 03h 33m 12s 469ms\n",
      "\u001b[32m2021-04-28T09:15:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.0767, train/total_loss: 0.0019, train/total_loss/avg: 0.0767, max mem: 9226.0, experiment: run, epoch: 28, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00004, ups: 1.11, time: 01m 30s 697ms, time_since_start: 02h 21m 15s 209ms, eta: 03h 36m 32s 992ms\n",
      "\u001b[32m2021-04-28T09:16:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T09:16:31 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T09:16:42 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T09:16:55 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T09:16:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.0757, train/total_loss: 0.0014, train/total_loss/avg: 0.0757, max mem: 9226.0, experiment: run, epoch: 28, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00003, ups: 0.89, time: 01m 52s 037ms, time_since_start: 02h 23m 07s 246ms, eta: 04h 25m 36s 213ms\n",
      "\u001b[32m2021-04-28T09:16:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T09:16:55 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:16:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:16:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:17:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/hateful_memes/cross_entropy: 2.0218, val/total_loss: 2.0218, val/hateful_memes/accuracy: 0.6540, val/hateful_memes/binary_f1: 0.5459, val/hateful_memes/roc_auc: 0.7219, num_updates: 8000, epoch: 28, iterations: 8000, max_updates: 22000, val_time: 30s 145ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:18:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:18:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:19:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.0748, train/total_loss: 0.0014, train/total_loss/avg: 0.0748, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00003, ups: 0.95, time: 01m 45s 018ms, time_since_start: 02h 25m 22s 418ms, eta: 04h 07m 11s 185ms\n",
      "\u001b[32m2021-04-28T09:20:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.0739, train/total_loss: 0.0014, train/total_loss/avg: 0.0739, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 189ms, time_since_start: 02h 26m 50s 607ms, eta: 03h 26m 04s 853ms\n",
      "\u001b[32m2021-04-28T09:22:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.0731, train/total_loss: 0.0014, train/total_loss/avg: 0.0731, max mem: 9226.0, experiment: run, epoch: 29, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 829ms, time_since_start: 02h 28m 19s 437ms, eta: 03h 26m 04s 387ms\n",
      "\u001b[32m2021-04-28T09:23:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.0722, train/total_loss: 0.0014, train/total_loss/avg: 0.0722, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 896ms, time_since_start: 02h 29m 51s 334ms, eta: 03h 31m 37s 936ms\n",
      "\u001b[32m2021-04-28T09:25:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.0713, train/total_loss: 0.0010, train/total_loss/avg: 0.0713, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00003, ups: 1.15, time: 01m 27s 456ms, time_since_start: 02h 31m 18s 790ms, eta: 03h 19m 55s 485ms\n",
      "\u001b[32m2021-04-28T09:26:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0705, train/total_loss: 0.0009, train/total_loss/avg: 0.0705, max mem: 9226.0, experiment: run, epoch: 30, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00003, ups: 1.15, time: 01m 27s 940ms, time_since_start: 02h 32m 46s 730ms, eta: 03h 19m 32s 602ms\n",
      "\u001b[32m2021-04-28T09:28:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0698, train/total_loss: 0.0009, train/total_loss/avg: 0.0698, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 837ms, time_since_start: 02h 34m 17s 568ms, eta: 03h 24m 34s 762ms\n",
      "\u001b[32m2021-04-28T09:29:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0690, train/total_loss: 0.0009, train/total_loss/avg: 0.0690, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00003, ups: 1.15, time: 01m 27s 974ms, time_since_start: 02h 35m 45s 543ms, eta: 03h 16m 38s 380ms\n",
      "\u001b[32m2021-04-28T09:31:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.0682, train/total_loss: 0.0009, train/total_loss/avg: 0.0682, max mem: 9226.0, experiment: run, epoch: 31, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 011ms, time_since_start: 02h 37m 13s 554ms, eta: 03h 15m 13s 937ms\n",
      "\u001b[32m2021-04-28T09:32:32 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T09:32:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T09:32:43 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T09:32:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T09:32:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0675, train/total_loss: 0.0006, train/total_loss/avg: 0.0675, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00003, ups: 0.87, time: 01m 55s 246ms, time_since_start: 02h 39m 08s 801ms, eta: 04h 13m 41s 791ms\n",
      "\u001b[32m2021-04-28T09:32:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T09:32:56 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:32:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:32:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:33:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/hateful_memes/cross_entropy: 2.6762, val/total_loss: 2.6762, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5604, val/hateful_memes/roc_auc: 0.7205, num_updates: 9000, epoch: 32, iterations: 9000, max_updates: 22000, val_time: 31s 593ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T09:34:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0667, train/total_loss: 0.0005, train/total_loss/avg: 0.0667, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 338ms, time_since_start: 02h 41m 09s 735ms, eta: 03h 15m 09s 067ms\n",
      "\u001b[32m2021-04-28T09:36:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0663, train/total_loss: 0.0005, train/total_loss/avg: 0.0663, max mem: 9226.0, experiment: run, epoch: 32, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 498ms, time_since_start: 02h 42m 38s 233ms, eta: 03h 11m 49s 081ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:37:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:37:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:37:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0656, train/total_loss: 0.0006, train/total_loss/avg: 0.0656, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 219ms, time_since_start: 02h 44m 08s 453ms, eta: 03h 14m 01s 259ms\n",
      "\u001b[32m2021-04-28T09:39:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0649, train/total_loss: 0.0005, train/total_loss/avg: 0.0649, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 311ms, time_since_start: 02h 45m 37s 764ms, eta: 03h 10m 33s 247ms\n",
      "\u001b[32m2021-04-28T09:40:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0643, train/total_loss: 0.0006, train/total_loss/avg: 0.0643, max mem: 9226.0, experiment: run, epoch: 33, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 867ms, time_since_start: 02h 47m 06s 631ms, eta: 03h 08m 06s 119ms\n",
      "\u001b[32m2021-04-28T09:42:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0643, train/total_loss: 0.0006, train/total_loss/avg: 0.0643, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 705ms, time_since_start: 02h 48m 37s 337ms, eta: 03h 10m 27s 391ms\n",
      "\u001b[32m2021-04-28T09:43:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0636, train/total_loss: 0.0005, train/total_loss/avg: 0.0636, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 985ms, time_since_start: 02h 50m 06s 322ms, eta: 03h 05m 20s 382ms\n",
      "\u001b[32m2021-04-28T09:45:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0630, train/total_loss: 0.0005, train/total_loss/avg: 0.0630, max mem: 9226.0, experiment: run, epoch: 34, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 202ms, time_since_start: 02h 51m 34s 525ms, eta: 03h 02m 12s 917ms\n",
      "\u001b[32m2021-04-28T09:46:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0623, train/total_loss: 0.0004, train/total_loss/avg: 0.0623, max mem: 9226.0, experiment: run, epoch: 35, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 538ms, time_since_start: 02h 53m 06s 064ms, eta: 03h 07m 33s 383ms\n",
      "\u001b[32m2021-04-28T09:48:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T09:48:22 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T09:48:33 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T09:48:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T09:48:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0617, train/total_loss: 0.0003, train/total_loss/avg: 0.0617, max mem: 9226.0, experiment: run, epoch: 35, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00003, ups: 0.89, time: 01m 52s 154ms, time_since_start: 02h 54m 58s 219ms, eta: 03h 47m 53s 887ms\n",
      "\u001b[32m2021-04-28T09:48:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T09:48:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:48:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:48:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:49:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/hateful_memes/cross_entropy: 2.0655, val/total_loss: 2.0655, val/hateful_memes/accuracy: 0.6680, val/hateful_memes/binary_f1: 0.5990, val/hateful_memes/roc_auc: 0.7352, num_updates: 10000, epoch: 35, iterations: 10000, max_updates: 22000, val_time: 29s 451ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T09:50:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0611, train/total_loss: 0.0003, train/total_loss/avg: 0.0611, max mem: 9226.0, experiment: run, epoch: 35, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00003, ups: 0.99, time: 01m 41s 493ms, time_since_start: 02h 57m 09s 165ms, eta: 03h 24m 30s 957ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:51:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T09:51:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T09:52:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0605, train/total_loss: 0.0003, train/total_loss/avg: 0.0605, max mem: 9226.0, experiment: run, epoch: 36, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00003, ups: 1.10, time: 01m 31s 407ms, time_since_start: 02h 58m 40s 573ms, eta: 03h 02m 38s 634ms\n",
      "\u001b[32m2021-04-28T09:53:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0599, train/total_loss: 0.0003, train/total_loss/avg: 0.0599, max mem: 9226.0, experiment: run, epoch: 36, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 227ms, time_since_start: 03h 08s 801ms, eta: 02h 54m 47s 838ms\n",
      "\u001b[32m2021-04-28T09:55:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0594, train/total_loss: 0.0003, train/total_loss/avg: 0.0594, max mem: 9226.0, experiment: run, epoch: 36, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 077ms, time_since_start: 03h 01m 36s 878ms, eta: 02h 53m 478ms\n",
      "\u001b[32m2021-04-28T09:56:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0588, train/total_loss: 0.0003, train/total_loss/avg: 0.0588, max mem: 9226.0, experiment: run, epoch: 37, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 874ms, time_since_start: 03h 03m 07s 753ms, eta: 02h 56m 57s 788ms\n",
      "\u001b[32m2021-04-28T09:58:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0583, train/total_loss: 0.0004, train/total_loss/avg: 0.0583, max mem: 9226.0, experiment: run, epoch: 37, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 231ms, time_since_start: 03h 04m 35s 984ms, eta: 02h 50m 19s 298ms\n",
      "\u001b[32m2021-04-28T09:59:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0578, train/total_loss: 0.0004, train/total_loss/avg: 0.0578, max mem: 9226.0, experiment: run, epoch: 38, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 428ms, time_since_start: 03h 06m 08s 413ms, eta: 02h 56m 51s 569ms\n",
      "\u001b[32m2021-04-28T10:01:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0572, train/total_loss: 0.0005, train/total_loss/avg: 0.0572, max mem: 9226.0, experiment: run, epoch: 38, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00003, ups: 1.15, time: 01m 27s 538ms, time_since_start: 03h 07m 35s 952ms, eta: 02h 46m 01s 207ms\n",
      "\u001b[32m2021-04-28T10:02:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0567, train/total_loss: 0.0005, train/total_loss/avg: 0.0567, max mem: 9226.0, experiment: run, epoch: 38, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00003, ups: 1.15, time: 01m 27s 760ms, time_since_start: 03h 09m 03s 712ms, eta: 02h 44m 57s 256ms\n",
      "\u001b[32m2021-04-28T10:04:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T10:04:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T10:04:34 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T10:04:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T10:04:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0562, train/total_loss: 0.0005, train/total_loss/avg: 0.0562, max mem: 9226.0, experiment: run, epoch: 39, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00003, ups: 0.88, time: 01m 54s 520ms, time_since_start: 03h 10m 58s 233ms, eta: 03h 33m 18s 852ms\n",
      "\u001b[32m2021-04-28T10:04:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T10:04:46 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:04:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:04:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:05:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/hateful_memes/cross_entropy: 2.5277, val/total_loss: 2.5277, val/hateful_memes/accuracy: 0.6720, val/hateful_memes/binary_f1: 0.6058, val/hateful_memes/roc_auc: 0.7193, num_updates: 11000, epoch: 39, iterations: 11000, max_updates: 22000, val_time: 30s 695ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T10:06:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0557, train/total_loss: 0.0005, train/total_loss/avg: 0.0557, max mem: 9226.0, experiment: run, epoch: 39, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00003, ups: 1.12, time: 01m 29s 280ms, time_since_start: 03h 12m 58s 214ms, eta: 02h 44m 47s 260ms\n",
      "\u001b[32m2021-04-28T10:08:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0552, train/total_loss: 0.0005, train/total_loss/avg: 0.0552, max mem: 9226.0, experiment: run, epoch: 39, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 978ms, time_since_start: 03h 14m 27s 192ms, eta: 02h 42m 43s 418ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:09:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:09:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:09:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0547, train/total_loss: 0.0004, train/total_loss/avg: 0.0547, max mem: 9226.0, experiment: run, epoch: 40, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00003, ups: 1.09, time: 01m 32s 270ms, time_since_start: 03h 15m 59s 463ms, eta: 02h 47m 10s 927ms\n",
      "\u001b[32m2021-04-28T10:11:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0542, train/total_loss: 0.0004, train/total_loss/avg: 0.0542, max mem: 9226.0, experiment: run, epoch: 40, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 252ms, time_since_start: 03h 17m 27s 716ms, eta: 02h 38m 24s 467ms\n",
      "\u001b[32m2021-04-28T10:12:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0538, train/total_loss: 0.0004, train/total_loss/avg: 0.0538, max mem: 9226.0, experiment: run, epoch: 40, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00003, ups: 1.14, time: 01m 28s 795ms, time_since_start: 03h 18m 56s 511ms, eta: 02h 37m 52s 716ms\n",
      "\u001b[32m2021-04-28T10:14:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0535, train/total_loss: 0.0004, train/total_loss/avg: 0.0535, max mem: 9226.0, experiment: run, epoch: 41, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 557ms, time_since_start: 03h 20m 27s 069ms, eta: 02h 39m 28s 718ms\n",
      "\u001b[32m2021-04-28T10:15:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0530, train/total_loss: 0.0004, train/total_loss/avg: 0.0530, max mem: 9226.0, experiment: run, epoch: 41, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00003, ups: 1.15, time: 01m 27s 852ms, time_since_start: 03h 21m 54s 922ms, eta: 02h 33m 13s 636ms\n",
      "\u001b[32m2021-04-28T10:17:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0526, train/total_loss: 0.0004, train/total_loss/avg: 0.0526, max mem: 9226.0, experiment: run, epoch: 41, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00003, ups: 1.15, time: 01m 27s 986ms, time_since_start: 03h 23m 22s 908ms, eta: 02h 31m 58s 184ms\n",
      "\u001b[32m2021-04-28T10:18:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0522, train/total_loss: 0.0004, train/total_loss/avg: 0.0522, max mem: 9226.0, experiment: run, epoch: 42, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00003, ups: 1.11, time: 01m 30s 577ms, time_since_start: 03h 24m 53s 486ms, eta: 02h 34m 54s 671ms\n",
      "\u001b[32m2021-04-28T10:20:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T10:20:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T10:20:21 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T10:20:35 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T10:20:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0517, train/total_loss: 0.0004, train/total_loss/avg: 0.0517, max mem: 9226.0, experiment: run, epoch: 42, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00003, ups: 0.88, time: 01m 54s 497ms, time_since_start: 03h 26m 47s 983ms, eta: 03h 13m 52s 926ms\n",
      "\u001b[32m2021-04-28T10:20:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T10:20:35 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:20:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:20:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:21:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/hateful_memes/cross_entropy: 2.4106, val/total_loss: 2.4106, val/hateful_memes/accuracy: 0.6560, val/hateful_memes/binary_f1: 0.5743, val/hateful_memes/roc_auc: 0.7379, num_updates: 12000, epoch: 42, iterations: 12000, max_updates: 22000, val_time: 27s 676ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T10:22:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0513, train/total_loss: 0.0004, train/total_loss/avg: 0.0513, max mem: 9226.0, experiment: run, epoch: 42, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 897ms, time_since_start: 03h 28m 44s 562ms, eta: 02h 29m 01s 630ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:23:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:23:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:24:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0509, train/total_loss: 0.0004, train/total_loss/avg: 0.0509, max mem: 9226.0, experiment: run, epoch: 43, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 045ms, time_since_start: 03h 30m 15s 607ms, eta: 02h 31m 05s 190ms\n",
      "\u001b[32m2021-04-28T10:25:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0505, train/total_loss: 0.0004, train/total_loss/avg: 0.0505, max mem: 9226.0, experiment: run, epoch: 43, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 460ms, time_since_start: 03h 31m 44s 067ms, eta: 02h 25m 17s 917ms\n",
      "\u001b[32m2021-04-28T10:27:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0501, train/total_loss: 0.0004, train/total_loss/avg: 0.0501, max mem: 9226.0, experiment: run, epoch: 43, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 588ms, time_since_start: 03h 33m 12s 656ms, eta: 02h 24m 583ms\n",
      "\u001b[32m2021-04-28T10:28:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0497, train/total_loss: 0.0004, train/total_loss/avg: 0.0497, max mem: 9226.0, experiment: run, epoch: 44, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 093ms, time_since_start: 03h 34m 42s 750ms, eta: 02h 24m 55s 842ms\n",
      "\u001b[32m2021-04-28T10:29:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0493, train/total_loss: 0.0004, train/total_loss/avg: 0.0493, max mem: 9226.0, experiment: run, epoch: 44, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 189ms, time_since_start: 03h 36m 10s 939ms, eta: 02h 20m 22s 428ms\n",
      "\u001b[32m2021-04-28T10:31:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0489, train/total_loss: 0.0003, train/total_loss/avg: 0.0489, max mem: 9226.0, experiment: run, epoch: 44, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 534ms, time_since_start: 03h 37m 39s 474ms, eta: 02h 19m 25s 452ms\n",
      "\u001b[32m2021-04-28T10:32:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0485, train/total_loss: 0.0003, train/total_loss/avg: 0.0485, max mem: 9226.0, experiment: run, epoch: 45, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 067ms, time_since_start: 03h 39m 09s 541ms, eta: 02h 20m 18s 794ms\n",
      "\u001b[32m2021-04-28T10:34:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0482, train/total_loss: 0.0003, train/total_loss/avg: 0.0482, max mem: 9226.0, experiment: run, epoch: 45, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 677ms, time_since_start: 03h 40m 38s 219ms, eta: 02h 16m 38s 787ms\n",
      "\u001b[32m2021-04-28T10:35:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T10:35:53 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T10:36:03 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T10:36:15 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T10:36:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0478, train/total_loss: 0.0003, train/total_loss/avg: 0.0478, max mem: 9226.0, experiment: run, epoch: 45, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0.00002, ups: 0.92, time: 01m 49s 654ms, time_since_start: 03h 42m 27s 873ms, eta: 02h 47m 06s 771ms\n",
      "\u001b[32m2021-04-28T10:36:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T10:36:15 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:36:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:36:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:36:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/hateful_memes/cross_entropy: 1.9470, val/total_loss: 1.9470, val/hateful_memes/accuracy: 0.6500, val/hateful_memes/binary_f1: 0.5902, val/hateful_memes/roc_auc: 0.7275, num_updates: 13000, epoch: 45, iterations: 13000, max_updates: 22000, val_time: 28s 409ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:36:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:36:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:38:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0474, train/total_loss: 0.0003, train/total_loss/avg: 0.0474, max mem: 9226.0, experiment: run, epoch: 46, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0.00002, ups: 1.08, time: 01m 33s 706ms, time_since_start: 03h 44m 29s 994ms, eta: 02h 21m 13s 287ms\n",
      "\u001b[32m2021-04-28T10:39:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0471, train/total_loss: 0.0003, train/total_loss/avg: 0.0471, max mem: 9226.0, experiment: run, epoch: 46, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 963ms, time_since_start: 03h 45m 58s 957ms, eta: 02h 12m 34s 062ms\n",
      "\u001b[32m2021-04-28T10:41:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0467, train/total_loss: 0.0003, train/total_loss/avg: 0.0467, max mem: 9226.0, experiment: run, epoch: 47, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 017ms, time_since_start: 03h 47m 30s 974ms, eta: 02h 15m 33s 581ms\n",
      "\u001b[32m2021-04-28T10:42:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0464, train/total_loss: 0.0003, train/total_loss/avg: 0.0464, max mem: 9226.0, experiment: run, epoch: 47, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0.00002, ups: 1.15, time: 01m 27s 445ms, time_since_start: 03h 48m 58s 420ms, eta: 02h 07m 20s 620ms\n",
      "\u001b[32m2021-04-28T10:44:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0461, train/total_loss: 0.0003, train/total_loss/avg: 0.0461, max mem: 9226.0, experiment: run, epoch: 47, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 239ms, time_since_start: 03h 50m 26s 660ms, eta: 02h 07m 387ms\n",
      "\u001b[32m2021-04-28T10:45:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0457, train/total_loss: 0.0003, train/total_loss/avg: 0.0457, max mem: 9226.0, experiment: run, epoch: 48, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 068ms, time_since_start: 03h 51m 57s 728ms, eta: 02h 09m 32s 117ms\n",
      "\u001b[32m2021-04-28T10:47:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0454, train/total_loss: 0.0003, train/total_loss/avg: 0.0454, max mem: 9226.0, experiment: run, epoch: 48, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0.00002, ups: 1.15, time: 01m 27s 731ms, time_since_start: 03h 53m 25s 460ms, eta: 02h 03m 18s 244ms\n",
      "\u001b[32m2021-04-28T10:48:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0451, train/total_loss: 0.0002, train/total_loss/avg: 0.0451, max mem: 9226.0, experiment: run, epoch: 48, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 690ms, time_since_start: 03h 54m 54s 150ms, eta: 02h 03m 09s 002ms\n",
      "\u001b[32m2021-04-28T10:50:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0447, train/total_loss: 0.0002, train/total_loss/avg: 0.0447, max mem: 9226.0, experiment: run, epoch: 49, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 076ms, time_since_start: 03h 56m 25s 227ms, eta: 02h 04m 55s 267ms\n",
      "\u001b[32m2021-04-28T10:51:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T10:51:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T10:51:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T10:52:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T10:52:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0444, train/total_loss: 0.0002, train/total_loss/avg: 0.0444, max mem: 9226.0, experiment: run, epoch: 49, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0.00002, ups: 0.88, time: 01m 53s 632ms, time_since_start: 03h 58m 18s 860ms, eta: 02h 33m 56s 037ms\n",
      "\u001b[32m2021-04-28T10:52:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T10:52:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:52:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:52:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:52:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/hateful_memes/cross_entropy: 2.2103, val/total_loss: 2.2103, val/hateful_memes/accuracy: 0.6620, val/hateful_memes/binary_f1: 0.5908, val/hateful_memes/roc_auc: 0.7277, num_updates: 14000, epoch: 49, iterations: 14000, max_updates: 22000, val_time: 30s 788ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T10:54:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0441, train/total_loss: 0.0002, train/total_loss/avg: 0.0441, max mem: 9226.0, experiment: run, epoch: 49, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0.00002, ups: 1.01, time: 01m 39s 925ms, time_since_start: 04h 29s 577ms, eta: 02h 13m 40s 390ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:55:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T10:55:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T10:55:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0438, train/total_loss: 0.0002, train/total_loss/avg: 0.0438, max mem: 9226.0, experiment: run, epoch: 50, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 787ms, time_since_start: 04h 02m 365ms, eta: 01h 59m 54s 765ms\n",
      "\u001b[32m2021-04-28T10:57:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0435, train/total_loss: 0.0002, train/total_loss/avg: 0.0435, max mem: 9226.0, experiment: run, epoch: 50, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 706ms, time_since_start: 04h 03m 29s 071ms, eta: 01h 55m 39s 703ms\n",
      "\u001b[32m2021-04-28T10:58:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0432, train/total_loss: 0.0002, train/total_loss/avg: 0.0432, max mem: 9226.0, experiment: run, epoch: 50, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0.00002, ups: 1.15, time: 01m 27s 999ms, time_since_start: 04h 04m 57s 071ms, eta: 01h 53m 14s 976ms\n",
      "\u001b[32m2021-04-28T11:00:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0429, train/total_loss: 0.0002, train/total_loss/avg: 0.0429, max mem: 9226.0, experiment: run, epoch: 51, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 627ms, time_since_start: 04h 06m 27s 699ms, eta: 01h 55m 05s 831ms\n",
      "\u001b[32m2021-04-28T11:01:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0426, train/total_loss: 0.0002, train/total_loss/avg: 0.0426, max mem: 9226.0, experiment: run, epoch: 51, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 556ms, time_since_start: 04h 07m 56s 255ms, eta: 01h 50m 58s 005ms\n",
      "\u001b[32m2021-04-28T11:03:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0423, train/total_loss: 0.0001, train/total_loss/avg: 0.0423, max mem: 9226.0, experiment: run, epoch: 51, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0.00002, ups: 1.15, time: 01m 27s 616ms, time_since_start: 04h 09m 23s 871ms, eta: 01h 48m 18s 327ms\n",
      "\u001b[32m2021-04-28T11:04:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0420, train/total_loss: 0.0001, train/total_loss/avg: 0.0420, max mem: 9226.0, experiment: run, epoch: 52, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 992ms, time_since_start: 04h 10m 54s 864ms, eta: 01h 50m 56s 295ms\n",
      "\u001b[32m2021-04-28T11:06:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0417, train/total_loss: 0.0001, train/total_loss/avg: 0.0417, max mem: 9226.0, experiment: run, epoch: 52, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 356ms, time_since_start: 04h 12m 23s 221ms, eta: 01h 46m 13s 700ms\n",
      "\u001b[32m2021-04-28T11:07:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T11:07:39 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T11:07:51 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T11:08:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T11:08:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0415, train/total_loss: 0.0001, train/total_loss/avg: 0.0415, max mem: 9226.0, experiment: run, epoch: 52, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0.00002, ups: 0.88, time: 01m 54s 809ms, time_since_start: 04h 14m 18s 030ms, eta: 02h 16m 05s 251ms\n",
      "\u001b[32m2021-04-28T11:08:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T11:08:05 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:08:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:08:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:08:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/hateful_memes/cross_entropy: 3.1084, val/total_loss: 3.1084, val/hateful_memes/accuracy: 0.6520, val/hateful_memes/binary_f1: 0.5469, val/hateful_memes/roc_auc: 0.7172, num_updates: 15000, epoch: 52, iterations: 15000, max_updates: 22000, val_time: 31s 033ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:09:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:09:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:10:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0417, train/total_loss: 0.0001, train/total_loss/avg: 0.0417, max mem: 9226.0, experiment: run, epoch: 53, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0.00002, ups: 0.98, time: 01m 42s 569ms, time_since_start: 04h 16m 31s 641ms, eta: 01h 59m 50s 518ms\n",
      "\u001b[32m2021-04-28T11:11:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0414, train/total_loss: 0.0001, train/total_loss/avg: 0.0414, max mem: 9226.0, experiment: run, epoch: 53, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 286ms, time_since_start: 04h 17m 59s 927ms, eta: 01h 41m 39s 519ms\n",
      "\u001b[32m2021-04-28T11:13:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0411, train/total_loss: 0.0001, train/total_loss/avg: 0.0411, max mem: 9226.0, experiment: run, epoch: 53, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 672ms, time_since_start: 04h 19m 28s 600ms, eta: 01h 40m 36s 137ms\n",
      "\u001b[32m2021-04-28T11:14:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0409, train/total_loss: 0.0001, train/total_loss/avg: 0.0409, max mem: 9226.0, experiment: run, epoch: 54, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0.00002, ups: 1.11, time: 01m 30s 475ms, time_since_start: 04h 20m 59s 075ms, eta: 01h 41m 06s 923ms\n",
      "\u001b[32m2021-04-28T11:16:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0406, train/total_loss: 0.0001, train/total_loss/avg: 0.0406, max mem: 9226.0, experiment: run, epoch: 54, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 335ms, time_since_start: 04h 22m 27s 411ms, eta: 01h 37m 13s 686ms\n",
      "\u001b[32m2021-04-28T11:17:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0404, train/total_loss: 0.0001, train/total_loss/avg: 0.0404, max mem: 9226.0, experiment: run, epoch: 54, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0.00002, ups: 1.14, time: 01m 28s 482ms, time_since_start: 04h 23m 55s 894ms, eta: 01h 35m 53s 480ms\n",
      "\u001b[32m2021-04-28T11:19:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0401, train/total_loss: 0.0001, train/total_loss/avg: 0.0401, max mem: 9226.0, experiment: run, epoch: 55, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0.00002, ups: 1.10, time: 01m 31s 873ms, time_since_start: 04h 25m 27s 767ms, eta: 01h 38m 634ms\n",
      "\u001b[32m2021-04-28T11:20:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0398, train/total_loss: 0.0001, train/total_loss/avg: 0.0398, max mem: 9226.0, experiment: run, epoch: 55, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0.00002, ups: 1.12, time: 01m 29s 288ms, time_since_start: 04h 26m 57s 056ms, eta: 01h 33m 44s 485ms\n",
      "\u001b[32m2021-04-28T11:22:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0396, train/total_loss: 0.0001, train/total_loss/avg: 0.0396, max mem: 9226.0, experiment: run, epoch: 56, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0.00002, ups: 1.09, time: 01m 32s 244ms, time_since_start: 04h 28m 29s 300ms, eta: 01h 35m 16s 934ms\n",
      "\u001b[32m2021-04-28T11:23:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T11:23:44 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T11:23:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T11:24:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T11:24:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0393, train/total_loss: 0.0001, train/total_loss/avg: 0.0393, max mem: 9226.0, experiment: run, epoch: 56, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0.00002, ups: 0.91, time: 01m 50s 178ms, time_since_start: 04h 30m 19s 479ms, eta: 01h 51m 56s 501ms\n",
      "\u001b[32m2021-04-28T11:24:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T11:24:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:24:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:24:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:24:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/hateful_memes/cross_entropy: 2.6492, val/total_loss: 2.6492, val/hateful_memes/accuracy: 0.6600, val/hateful_memes/binary_f1: 0.5854, val/hateful_memes/roc_auc: 0.7133, num_updates: 16000, epoch: 56, iterations: 16000, max_updates: 22000, val_time: 27s 131ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T11:26:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0391, train/total_loss: 0.0001, train/total_loss/avg: 0.0391, max mem: 9226.0, experiment: run, epoch: 56, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0.00001, ups: 0.96, time: 01m 44s 125ms, time_since_start: 04h 32m 30s 740ms, eta: 01h 44m 01s 695ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:27:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:27:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:27:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0389, train/total_loss: 0.0001, train/total_loss/avg: 0.0389, max mem: 9226.0, experiment: run, epoch: 57, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 242ms, time_since_start: 04h 34m 01s 982ms, eta: 01h 29m 36s 736ms\n",
      "\u001b[32m2021-04-28T11:29:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0386, train/total_loss: 0.0001, train/total_loss/avg: 0.0386, max mem: 9226.0, experiment: run, epoch: 57, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 769ms, time_since_start: 04h 35m 29s 752ms, eta: 01h 24m 42s 915ms\n",
      "\u001b[32m2021-04-28T11:30:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0384, train/total_loss: 0.0001, train/total_loss/avg: 0.0384, max mem: 9226.0, experiment: run, epoch: 57, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 214ms, time_since_start: 04h 36m 57s 967ms, eta: 01h 23m 39s 059ms\n",
      "\u001b[32m2021-04-28T11:32:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0382, train/total_loss: 0.0001, train/total_loss/avg: 0.0382, max mem: 9226.0, experiment: run, epoch: 58, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 312ms, time_since_start: 04h 38m 29s 280ms, eta: 01h 25m 02s 555ms\n",
      "\u001b[32m2021-04-28T11:33:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0379, train/total_loss: 0.0001, train/total_loss/avg: 0.0379, max mem: 9226.0, experiment: run, epoch: 58, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 049ms, time_since_start: 04h 39m 56s 330ms, eta: 01h 19m 35s 910ms\n",
      "\u001b[32m2021-04-28T11:35:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0377, train/total_loss: 0.0001, train/total_loss/avg: 0.0377, max mem: 9226.0, experiment: run, epoch: 58, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 943ms, time_since_start: 04h 41m 25s 273ms, eta: 01h 19m 49s 443ms\n",
      "\u001b[32m2021-04-28T11:36:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0375, train/total_loss: 0.0001, train/total_loss/avg: 0.0375, max mem: 9226.0, experiment: run, epoch: 59, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 621ms, time_since_start: 04h 42m 55s 895ms, eta: 01h 19m 47s 702ms\n",
      "\u001b[32m2021-04-28T11:38:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0373, train/total_loss: 0.0001, train/total_loss/avg: 0.0373, max mem: 9226.0, experiment: run, epoch: 59, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 509ms, time_since_start: 04h 44m 24s 404ms, eta: 01h 16m 26s 218ms\n",
      "\u001b[32m2021-04-28T11:39:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T11:39:41 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T11:39:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T11:40:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T11:40:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0370, train/total_loss: 0.0002, train/total_loss/avg: 0.0370, max mem: 9226.0, experiment: run, epoch: 59, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0.00001, ups: 0.88, time: 01m 54s 323ms, time_since_start: 04h 46m 18s 728ms, eta: 01h 36m 47s 643ms\n",
      "\u001b[32m2021-04-28T11:40:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T11:40:06 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:40:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:40:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:40:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/hateful_memes/cross_entropy: 3.0960, val/total_loss: 3.0960, val/hateful_memes/accuracy: 0.6540, val/hateful_memes/binary_f1: 0.5483, val/hateful_memes/roc_auc: 0.7193, num_updates: 17000, epoch: 59, iterations: 17000, max_updates: 22000, val_time: 24s 519ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:41:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T11:41:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T11:42:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0368, train/total_loss: 0.0001, train/total_loss/avg: 0.0368, max mem: 9226.0, experiment: run, epoch: 60, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 658ms, time_since_start: 04h 48m 14s 909ms, eta: 01h 16m 03s 121ms\n",
      "\u001b[32m2021-04-28T11:43:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0366, train/total_loss: 0.0001, train/total_loss/avg: 0.0366, max mem: 9226.0, experiment: run, epoch: 60, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 299ms, time_since_start: 04h 49m 44s 208ms, eta: 01h 12m 34s 956ms\n",
      "\u001b[32m2021-04-28T11:45:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0364, train/total_loss: 0.0001, train/total_loss/avg: 0.0364, max mem: 9226.0, experiment: run, epoch: 60, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 140ms, time_since_start: 04h 51m 13s 349ms, eta: 01h 10m 56s 656ms\n",
      "\u001b[32m2021-04-28T11:46:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0362, train/total_loss: 0.0001, train/total_loss/avg: 0.0362, max mem: 9226.0, experiment: run, epoch: 61, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 010ms, time_since_start: 04h 52m 43s 359ms, eta: 01h 10m 06s 718ms\n",
      "\u001b[32m2021-04-28T11:47:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0360, train/total_loss: 0.0001, train/total_loss/avg: 0.0360, max mem: 9226.0, experiment: run, epoch: 61, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 112ms, time_since_start: 04h 54m 11s 472ms, eta: 01h 07m 08s 526ms\n",
      "\u001b[32m2021-04-28T11:49:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0358, train/total_loss: 0.0001, train/total_loss/avg: 0.0358, max mem: 9226.0, experiment: run, epoch: 61, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 789ms, time_since_start: 04h 55m 41s 262ms, eta: 01h 06m 53s 967ms\n",
      "\u001b[32m2021-04-28T11:51:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0356, train/total_loss: 0.0001, train/total_loss/avg: 0.0356, max mem: 9226.0, experiment: run, epoch: 62, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 367ms, time_since_start: 04h 57m 12s 630ms, eta: 01h 06m 31s 686ms\n",
      "\u001b[32m2021-04-28T11:52:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0354, train/total_loss: 0.0001, train/total_loss/avg: 0.0354, max mem: 9226.0, experiment: run, epoch: 62, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 450ms, time_since_start: 04h 58m 40s 081ms, eta: 01h 02m 11s 695ms\n",
      "\u001b[32m2021-04-28T11:53:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0352, train/total_loss: 0.0001, train/total_loss/avg: 0.0352, max mem: 9226.0, experiment: run, epoch: 62, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 942ms, time_since_start: 05h 08s 023ms, eta: 01h 01m 03s 326ms\n",
      "\u001b[32m2021-04-28T11:59:20 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T11:59:20 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T12:05:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T12:06:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T12:06:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0350, train/total_loss: 0.0001, train/total_loss/avg: 0.0350, max mem: 9226.0, experiment: run, epoch: 63, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0.00001, ups: 0.13, time: 12m 31s 003ms, time_since_start: 05h 12m 39s 027ms, eta: 08h 28m 40s 799ms\n",
      "\u001b[32m2021-04-28T12:06:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T12:06:26 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:06:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:06:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:06:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/hateful_memes/cross_entropy: 2.6700, val/total_loss: 2.6700, val/hateful_memes/accuracy: 0.6480, val/hateful_memes/binary_f1: 0.5368, val/hateful_memes/roc_auc: 0.7197, num_updates: 18000, epoch: 63, iterations: 18000, max_updates: 22000, val_time: 15s 550ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T12:08:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0348, train/total_loss: 0.0001, train/total_loss/avg: 0.0348, max mem: 9226.0, experiment: run, epoch: 63, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 340ms, time_since_start: 05h 14m 22s 929ms, eta: 58m 20s 393ms\n",
      "\u001b[32m2021-04-28T12:09:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0346, train/total_loss: 0.0001, train/total_loss/avg: 0.0346, max mem: 9226.0, experiment: run, epoch: 63, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 151ms, time_since_start: 05h 15m 51s 081ms, eta: 56m 43s 369ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:09:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:09:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:11:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/hateful_memes/cross_entropy: 0.0001, train/hateful_memes/cross_entropy/avg: 0.0344, train/total_loss: 0.0001, train/total_loss/avg: 0.0344, max mem: 9226.0, experiment: run, epoch: 64, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0.00001, ups: 1.11, time: 01m 30s 961ms, time_since_start: 05h 17m 22s 042ms, eta: 56m 59s 428ms\n",
      "\u001b[32m2021-04-28T12:12:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0342, train/total_loss: 0.0000, train/total_loss/avg: 0.0342, max mem: 9226.0, experiment: run, epoch: 64, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 860ms, time_since_start: 05h 18m 50s 903ms, eta: 54m 10s 163ms\n",
      "\u001b[32m2021-04-28T12:14:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0341, train/total_loss: 0.0000, train/total_loss/avg: 0.0341, max mem: 9226.0, experiment: run, epoch: 65, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0.00001, ups: 1.09, time: 01m 32s 391ms, time_since_start: 05h 20m 23s 294ms, eta: 54m 45s 447ms\n",
      "\u001b[32m2021-04-28T12:15:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0339, train/total_loss: 0.0000, train/total_loss/avg: 0.0339, max mem: 9226.0, experiment: run, epoch: 65, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0.00001, ups: 1.15, time: 01m 27s 103ms, time_since_start: 05h 21m 50s 398ms, eta: 50m 08s 907ms\n",
      "\u001b[32m2021-04-28T12:17:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0337, train/total_loss: 0.0000, train/total_loss/avg: 0.0337, max mem: 9226.0, experiment: run, epoch: 65, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 775ms, time_since_start: 05h 23m 19s 173ms, eta: 49m 36s 458ms\n",
      "\u001b[32m2021-04-28T12:18:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0335, train/total_loss: 0.0000, train/total_loss/avg: 0.0335, max mem: 9226.0, experiment: run, epoch: 66, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0.00001, ups: 1.09, time: 01m 32s 693ms, time_since_start: 05h 24m 51s 867ms, eta: 50m 13s 668ms\n",
      "\u001b[32m2021-04-28T12:20:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0333, train/total_loss: 0.0000, train/total_loss/avg: 0.0333, max mem: 9226.0, experiment: run, epoch: 66, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 329ms, time_since_start: 05h 26m 20s 197ms, eta: 46m 22s 038ms\n",
      "\u001b[32m2021-04-28T12:21:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T12:21:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T12:21:49 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T12:22:00 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T12:22:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0332, train/total_loss: 0.0000, train/total_loss/avg: 0.0332, max mem: 9226.0, experiment: run, epoch: 66, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0.00001, ups: 0.89, time: 01m 52s 412ms, time_since_start: 05h 28m 12s 610ms, eta: 57m 06s 322ms\n",
      "\u001b[32m2021-04-28T12:22:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T12:22:00 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:22:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:22:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:22:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/hateful_memes/cross_entropy: 3.1131, val/total_loss: 3.1131, val/hateful_memes/accuracy: 0.6640, val/hateful_memes/binary_f1: 0.5800, val/hateful_memes/roc_auc: 0.7230, num_updates: 19000, epoch: 66, iterations: 19000, max_updates: 22000, val_time: 30s 287ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:23:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:23:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:24:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0330, train/total_loss: 0.0000, train/total_loss/avg: 0.0330, max mem: 9226.0, experiment: run, epoch: 67, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 434ms, time_since_start: 05h 30m 25s 334ms, eta: 50m 18s 135ms\n",
      "\u001b[32m2021-04-28T12:25:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0328, train/total_loss: 0.0000, train/total_loss/avg: 0.0328, max mem: 9226.0, experiment: run, epoch: 67, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0.00001, ups: 1.14, time: 01m 28s 935ms, time_since_start: 05h 31m 54s 270ms, eta: 42m 10s 041ms\n",
      "\u001b[32m2021-04-28T12:27:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0326, train/total_loss: 0.0000, train/total_loss/avg: 0.0326, max mem: 9226.0, experiment: run, epoch: 67, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 377ms, time_since_start: 05h 33m 23s 648ms, eta: 40m 51s 815ms\n",
      "\u001b[32m2021-04-28T12:28:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0325, train/total_loss: 0.0000, train/total_loss/avg: 0.0325, max mem: 9226.0, experiment: run, epoch: 68, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 466ms, time_since_start: 05h 34m 55s 114ms, eta: 40m 16s 172ms\n",
      "\u001b[32m2021-04-28T12:30:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0323, train/total_loss: 0.0000, train/total_loss/avg: 0.0323, max mem: 9226.0, experiment: run, epoch: 68, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 241ms, time_since_start: 05h 36m 24s 355ms, eta: 37m 46s 728ms\n",
      "\u001b[32m2021-04-28T12:31:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0321, train/total_loss: 0.0000, train/total_loss/avg: 0.0321, max mem: 9226.0, experiment: run, epoch: 68, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 240ms, time_since_start: 05h 37m 53s 596ms, eta: 36m 16s 051ms\n",
      "\u001b[32m2021-04-28T12:33:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0320, train/total_loss: 0.0000, train/total_loss/avg: 0.0320, max mem: 9226.0, experiment: run, epoch: 69, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0.00001, ups: 1.10, time: 01m 31s 668ms, time_since_start: 05h 39m 25s 265ms, eta: 35m 42s 122ms\n",
      "\u001b[32m2021-04-28T12:34:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0318, train/total_loss: 0.0000, train/total_loss/avg: 0.0318, max mem: 9226.0, experiment: run, epoch: 69, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 032ms, time_since_start: 05h 40m 54s 297ms, eta: 33m 10s 044ms\n",
      "\u001b[32m2021-04-28T12:36:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0317, train/total_loss: 0.0000, train/total_loss/avg: 0.0317, max mem: 9226.0, experiment: run, epoch: 69, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0.00001, ups: 1.12, time: 01m 29s 625ms, time_since_start: 05h 42m 23s 922ms, eta: 31m 52s 240ms\n",
      "\u001b[32m2021-04-28T12:37:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T12:37:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T12:37:55 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T12:38:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T12:38:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0315, train/total_loss: 0.0000, train/total_loss/avg: 0.0315, max mem: 9226.0, experiment: run, epoch: 70, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0.00001, ups: 0.87, time: 01m 55s 557ms, time_since_start: 05h 44m 19s 480ms, eta: 39m 08s 135ms\n",
      "\u001b[32m2021-04-28T12:38:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T12:38:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:38:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:38:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:38:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/hateful_memes/cross_entropy: 3.2299, val/total_loss: 3.2299, val/hateful_memes/accuracy: 0.6640, val/hateful_memes/binary_f1: 0.5800, val/hateful_memes/roc_auc: 0.7236, num_updates: 20000, epoch: 70, iterations: 20000, max_updates: 22000, val_time: 30s 976ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T12:40:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0313, train/total_loss: 0.0000, train/total_loss/avg: 0.0313, max mem: 9226.0, experiment: run, epoch: 70, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 769ms, time_since_start: 05h 46m 27s 229ms, eta: 31m 08s 030ms\n",
      "\u001b[32m2021-04-28T12:41:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0312, train/total_loss: 0.0000, train/total_loss/avg: 0.0312, max mem: 9226.0, experiment: run, epoch: 70, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 163ms, time_since_start: 05h 47m 56s 393ms, eta: 27m 10s 631ms\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:42:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:42:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:43:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0310, train/total_loss: 0.0000, train/total_loss/avg: 0.0310, max mem: 9226.0, experiment: run, epoch: 71, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.10, time: 01m 31s 570ms, time_since_start: 05h 49m 27s 964ms, eta: 26m 21s 613ms\n",
      "\u001b[32m2021-04-28T12:44:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0309, train/total_loss: 0.0000, train/total_loss/avg: 0.0309, max mem: 9226.0, experiment: run, epoch: 71, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 686ms, time_since_start: 05h 50m 56s 651ms, eta: 24m 01s 686ms\n",
      "\u001b[32m2021-04-28T12:46:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0307, train/total_loss: 0.0000, train/total_loss/avg: 0.0307, max mem: 9226.0, experiment: run, epoch: 71, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 610ms, time_since_start: 05h 52m 25s 261ms, eta: 22m 30s 429ms\n",
      "\u001b[32m2021-04-28T12:47:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0306, train/total_loss: 0.0000, train/total_loss/avg: 0.0306, max mem: 9226.0, experiment: run, epoch: 72, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.10, time: 01m 31s 580ms, time_since_start: 05h 53m 56s 842ms, eta: 21m 42s 637ms\n",
      "\u001b[32m2021-04-28T12:49:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0304, train/total_loss: 0.0000, train/total_loss/avg: 0.0304, max mem: 9226.0, experiment: run, epoch: 72, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 186ms, time_since_start: 05h 55m 26s 028ms, eta: 19m 37s 972ms\n",
      "\u001b[32m2021-04-28T12:50:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0303, train/total_loss: 0.0000, train/total_loss/avg: 0.0303, max mem: 9226.0, experiment: run, epoch: 72, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.12, time: 01m 29s 151ms, time_since_start: 05h 56m 55s 179ms, eta: 18m 06s 930ms\n",
      "\u001b[32m2021-04-28T12:52:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0301, train/total_loss: 0.0000, train/total_loss/avg: 0.0301, max mem: 9226.0, experiment: run, epoch: 73, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.11, time: 01m 30s 678ms, time_since_start: 05h 58m 25s 858ms, eta: 16m 53s 424ms\n",
      "\u001b[32m2021-04-28T12:53:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T12:53:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T12:53:53 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T12:54:06 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T12:54:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0300, train/total_loss: 0.0000, train/total_loss/avg: 0.0300, max mem: 9226.0, experiment: run, epoch: 73, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 0.88, time: 01m 53s 311ms, time_since_start: 06h 19s 169ms, eta: 19m 11s 241ms\n",
      "\u001b[32m2021-04-28T12:54:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T12:54:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:54:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:54:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:54:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/hateful_memes/cross_entropy: 3.3458, val/total_loss: 3.3458, val/hateful_memes/accuracy: 0.6680, val/hateful_memes/binary_f1: 0.5808, val/hateful_memes/roc_auc: 0.7272, num_updates: 21000, epoch: 73, iterations: 21000, max_updates: 22000, val_time: 26s 609ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:56:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T12:56:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T12:56:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0299, train/total_loss: 0.0000, train/total_loss/avg: 0.0299, max mem: 9226.0, experiment: run, epoch: 74, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 0.94, time: 01m 46s 039ms, time_since_start: 06h 02m 31s 820ms, eta: 16m 09s 628ms\n",
      "\u001b[32m2021-04-28T12:57:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0297, train/total_loss: 0.0000, train/total_loss/avg: 0.0297, max mem: 9226.0, experiment: run, epoch: 74, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.15, time: 01m 27s 743ms, time_since_start: 06h 03m 59s 564ms, eta: 11m 53s 182ms\n",
      "\u001b[32m2021-04-28T12:59:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0296, train/total_loss: 0.0000, train/total_loss/avg: 0.0296, max mem: 9226.0, experiment: run, epoch: 74, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 671ms, time_since_start: 06h 05m 28s 236ms, eta: 10m 30s 634ms\n",
      "\u001b[32m2021-04-28T13:00:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0294, train/total_loss: 0.0000, train/total_loss/avg: 0.0294, max mem: 9226.0, experiment: run, epoch: 75, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.10, time: 01m 31s 974ms, time_since_start: 06h 07m 210ms, eta: 09m 20s 674ms\n",
      "\u001b[32m2021-04-28T13:02:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0293, train/total_loss: 0.0000, train/total_loss/avg: 0.0293, max mem: 9226.0, experiment: run, epoch: 75, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.15, time: 01m 27s 467ms, time_since_start: 06h 08m 27s 678ms, eta: 07m 24s 336ms\n",
      "\u001b[32m2021-04-28T13:03:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0292, train/total_loss: 0.0000, train/total_loss/avg: 0.0292, max mem: 9226.0, experiment: run, epoch: 75, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 828ms, time_since_start: 06h 09m 56s 506ms, eta: 06m 997ms\n",
      "\u001b[32m2021-04-28T13:05:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0290, train/total_loss: 0.0000, train/total_loss/avg: 0.0290, max mem: 9226.0, experiment: run, epoch: 76, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.10, time: 01m 31s 797ms, time_since_start: 06h 11m 28s 304ms, eta: 04m 39s 798ms\n",
      "\u001b[32m2021-04-28T13:06:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0289, train/total_loss: 0.0000, train/total_loss/avg: 0.0289, max mem: 9226.0, experiment: run, epoch: 76, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.15, time: 01m 27s 443ms, time_since_start: 06h 12m 55s 748ms, eta: 02m 57s 686ms\n",
      "\u001b[32m2021-04-28T13:08:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0288, train/total_loss: 0.0000, train/total_loss/avg: 0.0288, max mem: 9226.0, experiment: run, epoch: 76, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 899ms, time_since_start: 06h 14m 24s 647ms, eta: 01m 30s 321ms\n",
      "\u001b[32m2021-04-28T13:09:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[32m2021-04-28T13:09:43 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation started!\n",
      "\u001b[32m2021-04-28T13:09:52 | mmf.utils.checkpoint: \u001b[0mSaving current checkpoint\n",
      "\u001b[32m2021-04-28T13:10:04 | mmf.utils.checkpoint: \u001b[0mCheckpoint save operation finished!\n",
      "\u001b[32m2021-04-28T13:10:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/hateful_memes/cross_entropy: 0.0000, train/hateful_memes/cross_entropy/avg: 0.0286, train/total_loss: 0.0000, train/total_loss/avg: 0.0286, max mem: 9226.0, experiment: run, epoch: 77, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 0.89, time: 01m 52s 299ms, time_since_start: 06h 16m 16s 946ms, eta: 0ms\n",
      "\u001b[32m2021-04-28T13:10:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2021-04-28T13:10:04 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T13:10:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T13:10:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[32m2021-04-28T13:10:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/hateful_memes/cross_entropy: 3.3961, val/total_loss: 3.3961, val/hateful_memes/accuracy: 0.6700, val/hateful_memes/binary_f1: 0.5823, val/hateful_memes/roc_auc: 0.7271, num_updates: 22000, epoch: 77, iterations: 22000, max_updates: 22000, val_time: 34s 786ms, best_update: 3000, best_iteration: 3000, best_val/hateful_memes/roc_auc: 0.751308\n",
      "\u001b[32m2021-04-28T13:10:40 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2021-04-28T13:10:40 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2021-04-28T13:10:40 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[32m2021-04-28T13:10:56 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2021-04-28T13:10:56 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 3000\n",
      "\u001b[32m2021-04-28T13:10:56 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 3000\n",
      "\u001b[32m2021-04-28T13:10:56 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 11\n",
      "\u001b[32m2021-04-28T13:10:58 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
      "\u001b[32m2021-04-28T13:10:58 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T13:10:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2021-04-28T13:10:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "\n",
      "100% 16/16 [00:16<00:00,  1.03s/it]\n",
      "\u001b[32m2021-04-28T13:11:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/hateful_memes/cross_entropy: 2.1685, val/total_loss: 2.1685, val/hateful_memes/accuracy: 0.6580, val/hateful_memes/binary_f1: 0.5649, val/hateful_memes/roc_auc: 0.7513\n",
      "\u001b[32m2021-04-28T13:11:15 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 06h 17m 27s 252ms\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config=projects/hateful_memes/configs/visual_bert/from_coco.yaml \\\n",
    "  model=visual_bert \\\n",
    "  dataset=hateful_memes \\\n",
    "  run_type=train_val \\\n",
    "  training.batch_size=32 \\\n",
    "  checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco \\\n",
    "  checkpoint.resume_pretrained=True \\\n",
    "  env.save_dir=/content/gdrive/MyDrive/colab/finetuned_visualbertcoco_election_memes/ \\\n",
    "  dataset_config.hateful_memes.annotations.train[0]=hateful_memes/defaults/annotations/train_hateful_and_election.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl \\\n",
    "  dataset_config.hateful_memes.features.train[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.val[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb \\\n",
    "  dataset_config.hateful_memes.features.test[0]=/content/gdrive/MyDrive/colab/hateful_and_election_memes_detectron.lmdb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Finetuned VisualBert COCO (Hateful and Election Memes).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
